{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Notebook: Labeling Utils\n",
    "\n",
    "This notebook tests the `labeling_utils` module for extracting frame-wise posteriors from CTC models.\n",
    "\n",
    "**Features tested:**\n",
    "1. Model loading (HuggingFace MMS, Wav2Vec2)\n",
    "2. Emission extraction (single audio)\n",
    "3. Batched emission extraction\n",
    "4. Vocabulary information\n",
    "5. TorchAudio Pipeline Backend (MMS_FA)\n",
    "6. Integration with audio_frontend\n",
    "\n",
    "**Architecture:**\n",
    "The module uses a plugin-style backend system:\n",
    "- `labeling_utils.base`: Core abstractions (CTCModelBackend, VocabInfo, BackendConfig)\n",
    "- `labeling_utils.registry`: Backend registration and discovery\n",
    "- `labeling_utils.backends/`: Individual backend implementations\n",
    "  - `huggingface.py`: HuggingFace Transformers (MMS, Wav2Vec2)\n",
    "  - `torchaudio_backend.py`: TorchAudio pipelines (MMS_FA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q transformers torch torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating repository (branch: dev)...\n",
      "Repository updated\n",
      "\n",
      "============================================================\n",
      "Labeling Utils imported successfully!\n",
      "============================================================\n",
      "Available backends: ['huggingface', 'torchaudio']\n",
      "Device: cuda\n",
      "\n",
      "Available presets:\n",
      "  mms: facebook/mms-1b-all (huggingface, 1100+ languages)\n",
      "  mms-1b-all: facebook/mms-1b-all (huggingface, 1100+ languages)\n",
      "  mms-1b-fl102: facebook/mms-1b-fl102 (huggingface, 102 languages)\n",
      "  mms-300m: facebook/mms-300m (huggingface, Multiple languages)\n",
      "  mms-fa: MMS_FA (torchaudio, 1130+ languages)\n",
      "  mms-fa-torchaudio: MMS_FA (torchaudio, 1130+ languages)\n",
      "  mms-fa-hf: MahmoudAshraf/mms-300m-1130-forced-aligner (huggingface, 1130+ languages)\n",
      "  wav2vec2-base: facebook/wav2vec2-base-960h (huggingface, English languages)\n",
      "  wav2vec2-large: facebook/wav2vec2-large-960h-lv60-self (huggingface, English languages)\n",
      "  wav2vec2-large-lv60: facebook/wav2vec2-large-960h-lv60-self (huggingface, English languages)\n",
      "  wav2vec2-xlsr: facebook/wav2vec2-large-xlsr-53 (huggingface, 53 languages)\n",
      "  wav2vec2-base-ta: WAV2VEC2_ASR_BASE_960H (torchaudio, English languages)\n",
      "  wav2vec2-large-ta: WAV2VEC2_ASR_LARGE_960H (torchaudio, English languages)\n",
      "  wav2vec2-large-lv60k-ta: WAV2VEC2_ASR_LARGE_LV60K_960H (torchaudio, English languages)\n",
      "  hubert-large: HUBERT_ASR_LARGE (torchaudio, English languages)\n",
      "  hubert-xlarge: HUBERT_ASR_XLARGE (torchaudio, English languages)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Setup: Clone Repository and Configure Imports\n",
    "# =============================================================================\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# ===== CONFIGURATION =====\n",
    "GITHUB_REPO = \"https://github.com/huangruizhe/torchaudio_aligner.git\"\n",
    "BRANCH = \"dev\"  # Use 'dev' for testing, 'main' for stable\n",
    "# =========================\n",
    "\n",
    "def setup_imports():\n",
    "    \"\"\"Setup Python path for imports based on environment.\"\"\"\n",
    "    \n",
    "    IN_COLAB = 'google.colab' in sys.modules\n",
    "    \n",
    "    if IN_COLAB:\n",
    "        repo_path = '/content/torchaudio_aligner'\n",
    "        src_path = f'{repo_path}/src'\n",
    "        \n",
    "        if not os.path.exists(repo_path):\n",
    "            print(f\"Cloning repository (branch: {BRANCH})...\")\n",
    "            os.system(f'git clone -b {BRANCH} {GITHUB_REPO} {repo_path}')\n",
    "            print(\"Repository cloned\")\n",
    "        else:\n",
    "            print(f\"Updating repository (branch: {BRANCH})...\")\n",
    "            os.system(f'cd {repo_path} && git fetch origin && git checkout {BRANCH} && git pull origin {BRANCH}')\n",
    "            print(\"Repository updated\")\n",
    "    else:\n",
    "        possible_paths = [\n",
    "            Path(\".\").absolute().parent / \"src\",\n",
    "            Path(\".\").absolute() / \"src\",\n",
    "        ]\n",
    "        \n",
    "        src_path = None\n",
    "        for p in possible_paths:\n",
    "            if p.exists() and (p / \"labeling_utils\").exists():\n",
    "                src_path = str(p.absolute())\n",
    "                break\n",
    "        \n",
    "        if src_path is None:\n",
    "            raise FileNotFoundError(\"src directory not found\")\n",
    "        \n",
    "        print(f\"Running locally from: {src_path}\")\n",
    "    \n",
    "    if src_path not in sys.path:\n",
    "        sys.path.insert(0, src_path)\n",
    "    \n",
    "    return src_path\n",
    "\n",
    "src_path = setup_imports()\n",
    "\n",
    "# Import labeling_utils - new modular structure\n",
    "from labeling_utils import (\n",
    "    # High-level API\n",
    "    load_model,\n",
    "    get_emissions,\n",
    "    get_emissions_batched,\n",
    "    EmissionResult,\n",
    "    # Model configuration\n",
    "    ModelConfig,\n",
    "    list_presets,\n",
    "    get_model_info,\n",
    "    # Backend system\n",
    "    list_backends,\n",
    "    get_backend,\n",
    "    is_backend_available,\n",
    "    # Core classes\n",
    "    CTCModelBackend,\n",
    "    VocabInfo,\n",
    "    BackendConfig,\n",
    ")\n",
    "\n",
    "import torch\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"Labeling Utils imported successfully!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Available backends: {list_backends()}\")\n",
    "print(f\"Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")\n",
    "print()\n",
    "print(\"Available presets:\")\n",
    "for preset in list_presets():\n",
    "    info = get_model_info(preset)\n",
    "    print(f\"  {preset}: {info['model_name']} ({info['backend']}, {info['languages']} languages)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: Load MMS Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Test 1: Load MMS Model (HuggingFace Backend)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5938392151b84047beda85cb3795c99e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/254 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6234248a2ab4b3788b83ea9531c5ccd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/397 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4ba6daa3f484eaea82fc93919e779d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d65bafd7e0f04737aa7daf941d90dfc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/96.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1994b6b9687e4919a96f5a485b83d5a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "148af0c5e2d340969d46c709d2b23ab3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7462c69c7b440f69b19b47e6dad41c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter.eng.safetensors:   0%|          | 0.00/9.43M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: HuggingFaceCTCBackend(model='facebook/mms-1b-all', loaded=True)\n",
      "Is loaded: True\n",
      "Frame duration: 0.02s\n",
      "Sample rate: 16000Hz\n",
      "\n",
      "Vocabulary:\n",
      "  Size: 154\n",
      "  Blank ID: 0 ('<pad>')\n",
      "  UNK ID: 3 ('<unk>')\n",
      "  Sample labels: ['<pad>', '<s>', '</s>', '<unk>', '|', 'e', 't', 'a', 'o', 'i']...\n",
      "\n",
      "Test 1 PASSED - MMS model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Test 1: Load MMS Model (HuggingFace Backend)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    # Load MMS model for English\n",
    "    backend = load_model(\n",
    "        \"facebook/mms-1b-all\",\n",
    "        language=\"eng\",\n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    )\n",
    "    \n",
    "    print(f\"Model loaded: {backend}\")\n",
    "    print(f\"Is loaded: {backend.is_loaded}\")\n",
    "    print(f\"Frame duration: {backend.frame_duration}s\")\n",
    "    print(f\"Sample rate: {backend.sample_rate}Hz\")\n",
    "    \n",
    "    # Get vocab info\n",
    "    vocab = backend.get_vocab_info()\n",
    "    print(f\"\\nVocabulary:\")\n",
    "    print(f\"  Size: {len(vocab.labels)}\")\n",
    "    print(f\"  Blank ID: {vocab.blank_id} ('{vocab.blank_token}')\")\n",
    "    print(f\"  UNK ID: {vocab.unk_id} ('{vocab.unk_token}')\")\n",
    "    print(f\"  Sample labels: {vocab.labels[:10]}...\")\n",
    "    \n",
    "    print(\"\\nTest 1 PASSED - MMS model loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nTest 1 FAILED: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Test 2: Extract Emissions from VOiCES Sample Audio\n\nUsing the Lab41 VOiCES dataset sample: \"I had that curiosity beside me at this moment\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\" * 60)\nprint(\"Test 2: Extract Emissions from VOiCES Sample Audio\")\nprint(\"=\" * 60)\n\ntry:\n    import torchaudio\n    import urllib.request\n    import os\n    \n    # VOiCES sample audio from repository\n    # Transcript: \"I had that curiosity beside me at this moment\"\n    SAMPLE_AUDIO = \"Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav\"\n    SAMPLE_TEXT = \"I had that curiosity beside me at this moment\"\n    \n    # Determine path based on environment\n    IN_COLAB = 'google.colab' in sys.modules\n    if IN_COLAB:\n        sample_path = f\"/content/torchaudio_aligner/examples/{SAMPLE_AUDIO}\"\n    else:\n        sample_path = str(Path(src_path).parent / \"examples\" / SAMPLE_AUDIO)\n    \n    if not os.path.exists(sample_path):\n        # Download from GitHub if not found locally\n        url = f\"https://raw.githubusercontent.com/huangruizhe/torchaudio_aligner/dev/examples/{SAMPLE_AUDIO}\"\n        print(f\"Downloading sample audio...\")\n        urllib.request.urlretrieve(url, SAMPLE_AUDIO)\n        sample_path = SAMPLE_AUDIO\n        print(f\"Downloaded: {sample_path}\")\n    \n    # Load audio\n    waveform, sample_rate = torchaudio.load(sample_path)\n    print(f\"Loaded: {sample_path}\")\n    print(f\"  Transcript: \\\"{SAMPLE_TEXT}\\\"\")\n    print(f\"  Waveform shape: {waveform.shape}\")\n    print(f\"  Sample rate: {sample_rate}Hz\")\n    print(f\"  Duration: {waveform.shape[1] / sample_rate:.2f}s\")\n    \n    # Resample if needed\n    if sample_rate != 16000:\n        waveform = torchaudio.functional.resample(waveform, sample_rate, 16000)\n        sample_rate = 16000\n        print(f\"  Resampled to: {sample_rate}Hz\")\n    \n    # Extract emissions\n    result = get_emissions(backend, waveform, sample_rate=sample_rate)\n    \n    print(f\"\\nEmission result:\")\n    print(f\"  Emissions shape: {result.emissions.shape}\")\n    print(f\"  Num frames: {result.num_frames}\")\n    print(f\"  Vocab size: {result.vocab_size}\")\n    print(f\"  Duration: {result.duration:.2f}s\")\n    \n    # Verify log probabilities (should sum to ~1 after exp)\n    probs = torch.exp(result.emissions[0])\n    prob_sum = probs.sum().item()\n    print(f\"  Prob sum at frame 0: {prob_sum:.4f} (should be ~1.0)\")\n    \n    # Show top predictions for a few frames\n    print(f\"\\nTop predictions (frames 10-15):\")\n    vocab = result.vocab_info\n    for i in range(10, min(15, result.num_frames)):\n        top_idx = result.emissions[i].argmax().item()\n        top_prob = torch.exp(result.emissions[i, top_idx]).item()\n        label = vocab.id_to_label.get(top_idx, \"?\")\n        print(f\"  Frame {i}: '{label}' (prob={top_prob:.3f})\")\n    \n    print(\"\\nTest 2 PASSED - Emissions extracted from VOiCES sample\")\nexcept Exception as e:\n    print(f\"\\nTest 2 FAILED: {e}\")\n    import traceback\n    traceback.print_exc()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Test 3: Greedy Decoding from Emissions\n\nDecode the emissions to verify the model recognizes the speech content."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\" * 60)\nprint(\"Test 3: Greedy Decoding from Emissions\")\nprint(\"=\" * 60)\n\ntry:\n    # Use the emissions from Test 2\n    # Greedy decode: take argmax at each frame, collapse repeats, remove blanks\n    \n    def greedy_decode(emissions: torch.Tensor, vocab_info: VocabInfo) -> str:\n        \"\"\"Simple greedy CTC decoding.\"\"\"\n        # Get most likely token at each frame\n        indices = emissions.argmax(dim=-1).tolist()\n        \n        # Collapse consecutive duplicates\n        collapsed = []\n        prev = None\n        for idx in indices:\n            if idx != prev:\n                collapsed.append(idx)\n                prev = idx\n        \n        # Remove blanks and convert to characters\n        tokens = []\n        for idx in collapsed:\n            if idx == vocab_info.blank_id:\n                continue\n            label = vocab_info.id_to_label.get(idx, \"\")\n            # Handle word boundary token\n            if label == \"|\":\n                tokens.append(\" \")\n            else:\n                tokens.append(label)\n        \n        return \"\".join(tokens).strip()\n    \n    decoded = greedy_decode(result.emissions, result.vocab_info)\n    \n    print(f\"Ground truth: \\\"{SAMPLE_TEXT}\\\"\")\n    print(f\"Decoded:      \\\"{decoded}\\\"\")\n    \n    # Check if decoding roughly matches\n    gt_normalized = SAMPLE_TEXT.lower().replace(\"'\", \"\")\n    decoded_normalized = decoded.lower().replace(\"'\", \"\")\n    \n    # Simple word overlap check\n    gt_words = set(gt_normalized.split())\n    decoded_words = set(decoded_normalized.split())\n    overlap = len(gt_words & decoded_words)\n    total = len(gt_words)\n    \n    print(f\"\\nWord overlap: {overlap}/{total} ({100*overlap/total:.0f}%)\")\n    \n    if overlap >= total // 2:\n        print(\"\\nTest 3 PASSED - Greedy decoding produces reasonable output\")\n    else:\n        print(\"\\nTest 3 WARNING - Low word overlap (model may need tuning)\")\n        \nexcept Exception as e:\n    print(f\"\\nTest 3 FAILED: {e}\")\n    import traceback\n    traceback.print_exc()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Test 4: Batched Emission Extraction\n\nTest batch processing using variations of the VOiCES sample (different segments)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\" * 60)\nprint(\"Test 4: Batched Emission Extraction\")\nprint(\"=\" * 60)\n\ntry:\n    # Create multiple waveforms from the VOiCES sample (different segments)\n    # Use the waveform loaded in Test 2\n    full_wav = waveform.squeeze(0)  # Remove channel dim\n    total_samples = full_wav.shape[0]\n    \n    # Create 3 segments of different lengths\n    waveforms = [\n        full_wav[:total_samples // 3],           # First third\n        full_wav[total_samples // 4:],            # Last 3/4\n        full_wav[total_samples // 3:2*total_samples // 3],  # Middle third\n    ]\n    \n    print(f\"Input: {len(waveforms)} waveforms from VOiCES sample\")\n    for i, w in enumerate(waveforms):\n        print(f\"  [{i}] shape={w.shape}, duration={len(w)/16000:.2f}s\")\n    \n    # Extract emissions in batch\n    results = get_emissions_batched(\n        backend,\n        waveforms,\n        sample_rate=16000,\n        batch_size=2,\n    )\n    \n    print(f\"\\nOutput: {len(results)} EmissionResults\")\n    for i, res in enumerate(results):\n        decoded = greedy_decode(res.emissions, res.vocab_info)\n        print(f\"  [{i}] frames={res.num_frames}, duration={res.duration:.2f}s\")\n        print(f\"       decoded: \\\"{decoded[:50]}{'...' if len(decoded) > 50 else ''}\\\"\")\n    \n    assert len(results) == len(waveforms), \"Output count mismatch\"\n    \n    print(\"\\nTest 4 PASSED - Batched extraction works\")\nexcept Exception as e:\n    print(f\"\\nTest 4 FAILED: {e}\")\n    import traceback\n    traceback.print_exc()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 5: Different Languages (MMS Multilingual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Test 5: Load MMS for Different Languages\n",
      "============================================================\n",
      "\n",
      "Loading MMS for French (fra)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30a2a04c9c3c4557a3b485675d0ee0e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter.fra.safetensors:   0%|          | 0.00/10.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/mms-1b-all and are newly initialized because the shapes did not match:\n",
      "- lm_head.bias: found shape torch.Size([154]) in the checkpoint and torch.Size([314]) in the model instantiated\n",
      "- lm_head.weight: found shape torch.Size([154, 1280]) in the checkpoint and torch.Size([314, 1280]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loaded! Vocab size: 314\n",
      "  Emissions shape: torch.Size([49, 314])\n",
      "  PASSED\n",
      "\n",
      "Loading MMS for Mandarin Chinese (cmn)...\n",
      "  FAILED: 'cmn'\n",
      "\n",
      "Loading MMS for Japanese (jpn)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac6c1ae62ea449e1a18059f785d49dc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter.jpn.safetensors:   0%|          | 0.00/20.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/mms-1b-all and are newly initialized because the shapes did not match:\n",
      "- lm_head.bias: found shape torch.Size([154]) in the checkpoint and torch.Size([2268]) in the model instantiated\n",
      "- lm_head.weight: found shape torch.Size([154, 1280]) in the checkpoint and torch.Size([2268, 1280]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loaded! Vocab size: 2268\n",
      "  Emissions shape: torch.Size([49, 2268])\n",
      "  PASSED\n",
      "\n",
      "Test 5 Complete\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Test 5: Load MMS for Different Languages\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test a few languages\n",
    "languages = [\n",
    "    (\"fra\", \"French\"),\n",
    "    (\"cmn\", \"Mandarin Chinese\"),\n",
    "    (\"jpn\", \"Japanese\"),\n",
    "]\n",
    "\n",
    "for lang_code, lang_name in languages:\n",
    "    print(f\"\\nLoading MMS for {lang_name} ({lang_code})...\")\n",
    "    try:\n",
    "        lang_backend = load_model(\n",
    "            \"facebook/mms-1b-all\",\n",
    "            language=lang_code,\n",
    "            device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        )\n",
    "        \n",
    "        vocab = lang_backend.get_vocab_info()\n",
    "        print(f\"  Loaded! Vocab size: {len(vocab.labels)}\")\n",
    "        \n",
    "        # Quick emission test\n",
    "        test_wav = torch.randn(16000)  # 1 second\n",
    "        result = get_emissions(lang_backend, test_wav)\n",
    "        print(f\"  Emissions shape: {result.emissions.shape}\")\n",
    "        print(f\"  PASSED\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  FAILED: {e}\")\n",
    "\n",
    "print(\"\\nTest 5 Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 6: TorchAudio Pipeline Backend (MMS_FA)\n",
    "\n",
    "Note: This test uses the TorchAudio pipeline API which has a different interface than HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\" * 60)\nprint(\"Test 6: TorchAudio Pipeline Backend (MMS_FA)\")\nprint(\"=\" * 60)\n\ntry:\n    # Check if torchaudio backend is available\n    if not is_backend_available(\"torchaudio\"):\n        print(\"TorchAudio backend not available (missing dependencies)\")\n        print(\"Skipping test...\")\n    else:\n        # Load MMS_FA using the preset\n        ta_backend = load_model(\"mms-fa\")  # Uses TorchAudio backend automatically\n        \n        print(f\"Model loaded: {ta_backend}\")\n        print(f\"Is loaded: {ta_backend.is_loaded}\")\n        print(f\"Frame duration: {ta_backend.frame_duration}s\")\n        print(f\"Sample rate: {ta_backend.sample_rate}Hz\")\n        \n        # Get vocab info\n        vocab = ta_backend.get_vocab_info()\n        print(f\"\\nVocabulary:\")\n        print(f\"  Size: {len(vocab.labels)}\")\n        print(f\"  Labels: {vocab.labels}\") \n        print(f\"  Blank ID: {vocab.blank_id} ('{vocab.blank_token}')\")\n        print(f\"  UNK ID: {vocab.unk_id} ('{vocab.unk_token}')\")\n        \n        # Test with VOiCES sample\n        print(f\"\\nTesting with VOiCES sample:\")\n        print(f\"  Transcript: \\\"{SAMPLE_TEXT}\\\"\")\n        \n        ta_result = get_emissions(ta_backend, waveform, sample_rate=16000)\n        \n        print(f\"\\nEmission result:\")\n        print(f\"  Emissions shape: {ta_result.emissions.shape}\")\n        print(f\"  Num frames: {ta_result.num_frames}\")\n        print(f\"  Vocab size: {ta_result.vocab_size}\")\n        \n        # MMS_FA uses romanized phonemes, so decode differently\n        ta_decoded = greedy_decode(ta_result.emissions, ta_result.vocab_info)\n        print(f\"\\n  Decoded (romanized): \\\"{ta_decoded}\\\"\")\n        \n        print(\"\\nTest 6 PASSED - TorchAudio Pipeline backend works with real audio\")\nexcept Exception as e:\n    print(f\"\\nTest 6 FAILED: {e}\")\n    print(\"Note: This test requires torchaudio with MMS_FA pipeline.\")\n    import traceback\n    traceback.print_exc()"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "print(\"=\" * 60)\nprint(\"Test 7: Integration with Audio Frontend\")\nprint(\"=\" * 60)\n\ntry:\n    from audio_frontend import segment_audio\n    \n    # Use VOiCES sample - segment into smaller chunks\n    print(f\"Original audio: {waveform.shape[1]/16000:.2f}s\")\n    \n    # Segment into overlapping chunks\n    seg_result = segment_audio(\n        waveform.squeeze(0), \n        sample_rate=16000, \n        segment_size=1.5,  # 1.5 second segments\n        overlap=0.3\n    )\n    print(f\"Segmented into {len(seg_result.segments)} segments\")\n    \n    # Extract emissions for each segment\n    all_emissions = []\n    for i, segment in enumerate(seg_result.segments):\n        seg_emission = get_emissions(backend, segment.waveform)\n        all_emissions.append(seg_emission)\n        decoded = greedy_decode(seg_emission.emissions, seg_emission.vocab_info)\n        print(f\"  Segment {i}: frames={seg_emission.num_frames}, decoded=\\\"{decoded}\\\"\")\n    \n    print(f\"\\nTotal emissions extracted: {len(all_emissions)}\")\n    print(f\"Total frames: {sum(e.num_frames for e in all_emissions)}\")\n    \n    print(\"\\nTest 7 PASSED - Audio frontend + labeling utils integration works\")\nexcept ImportError:\n    print(\"audio_frontend not available - skipping integration test\")\n    print(\"This is expected if running labeling_utils tests only\")\nexcept Exception as e:\n    print(f\"\\nTest 7 FAILED: {e}\")\n    import traceback\n    traceback.print_exc()"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LABELING UTILS TEST SUMMARY\n",
      "============================================================\n",
      "\n",
      "The labeling_utils module provides a plugin-style architecture for\n",
      "extracting frame-wise posteriors from CTC acoustic models.\n",
      "\n",
      "ARCHITECTURE:\n",
      "├── base.py          - Core abstractions (CTCModelBackend, VocabInfo, BackendConfig)\n",
      "├── registry.py      - Backend registration and discovery\n",
      "├── emissions.py     - get_emissions(), EmissionResult\n",
      "├── models.py        - load_model(), model presets\n",
      "└── backends/        - Individual backend implementations\n",
      "    ├── huggingface.py      - HuggingFace Transformers\n",
      "    └── torchaudio_backend.py - TorchAudio pipelines\n",
      "\n",
      "HIGH-LEVEL API:\n",
      "1. load_model() - Load CTC models with automatic backend detection\n",
      "   - Presets: \"mms\", \"mms-fa\", \"wav2vec2-base\", etc.\n",
      "   - Direct model IDs: \"facebook/mms-1b-all\"\n",
      "\n",
      "2. get_emissions() - Extract frame-wise log posteriors\n",
      "   - Returns EmissionResult with emissions, lengths, vocab_info\n",
      "\n",
      "3. get_emissions_batched() - Efficient batch processing\n",
      "\n",
      "SUPPORTED BACKENDS:\n",
      "- huggingface (hf): MMS (1100+ languages), Wav2Vec2, XLSR\n",
      "- torchaudio (ta): MMS_FA, WAV2VEC2_ASR_*, HUBERT_ASR_*\n",
      "\n",
      "EXTENDING:\n",
      "    from labeling_utils import CTCModelBackend, register_backend\n",
      "\n",
      "    class MyBackend(CTCModelBackend):\n",
      "        def load(self): ...\n",
      "        def get_emissions(self, waveform, lengths=None): ...\n",
      "        def get_vocab_info(self): ...\n",
      "\n",
      "    register_backend(\"mybackend\", MyBackend, aliases=[\"mb\"])\n",
      "\n",
      "NEXT STEPS:\n",
      "- Add NeMo backend (Conformer-CTC, QuartzNet)\n",
      "- Add ESPnet backend\n",
      "- Add OmniASR backend\n",
      "- Integrate with k2 WFST for alignment\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"LABELING UTILS TEST SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "The labeling_utils module provides a plugin-style architecture for\n",
    "extracting frame-wise posteriors from CTC acoustic models.\n",
    "\n",
    "ARCHITECTURE:\n",
    "├── base.py          - Core abstractions (CTCModelBackend, VocabInfo, BackendConfig)\n",
    "├── registry.py      - Backend registration and discovery\n",
    "├── emissions.py     - get_emissions(), EmissionResult\n",
    "├── models.py        - load_model(), model presets\n",
    "└── backends/        - Individual backend implementations\n",
    "    ├── huggingface.py      - HuggingFace Transformers\n",
    "    └── torchaudio_backend.py - TorchAudio pipelines\n",
    "\n",
    "HIGH-LEVEL API:\n",
    "1. load_model() - Load CTC models with automatic backend detection\n",
    "   - Presets: \"mms\", \"mms-fa\", \"wav2vec2-base\", etc.\n",
    "   - Direct model IDs: \"facebook/mms-1b-all\"\n",
    "\n",
    "2. get_emissions() - Extract frame-wise log posteriors\n",
    "   - Returns EmissionResult with emissions, lengths, vocab_info\n",
    "\n",
    "3. get_emissions_batched() - Efficient batch processing\n",
    "\n",
    "SUPPORTED BACKENDS:\n",
    "- huggingface (hf): MMS (1100+ languages), Wav2Vec2, XLSR\n",
    "- torchaudio (ta): MMS_FA, WAV2VEC2_ASR_*, HUBERT_ASR_*\n",
    "\n",
    "EXTENDING:\n",
    "    from labeling_utils import CTCModelBackend, register_backend\n",
    "\n",
    "    class MyBackend(CTCModelBackend):\n",
    "        def load(self): ...\n",
    "        def get_emissions(self, waveform, lengths=None): ...\n",
    "        def get_vocab_info(self): ...\n",
    "\n",
    "    register_backend(\"mybackend\", MyBackend, aliases=[\"mb\"])\n",
    "\n",
    "NEXT STEPS:\n",
    "- Add NeMo backend (Conformer-CTC, QuartzNet)\n",
    "- Add ESPnet backend\n",
    "- Add OmniASR backend\n",
    "- Integrate with k2 WFST for alignment\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}