{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Notebook: Labeling Utils\n",
    "\n",
    "This notebook tests the `labeling_utils` module for extracting frame-wise posteriors from CTC models.\n",
    "\n",
    "**Features tested:**\n",
    "1. Model loading (HuggingFace MMS, Wav2Vec2)\n",
    "2. Emission extraction (single audio)\n",
    "3. Batched emission extraction\n",
    "4. Vocabulary information\n",
    "5. Integration with audio_frontend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q transformers torch torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Setup: Clone Repository and Configure Imports\n",
    "# =============================================================================\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# ===== CONFIGURATION =====\n",
    "GITHUB_REPO = \"https://github.com/huangruizhe/torchaudio_aligner.git\"\n",
    "BRANCH = \"dev\"  # Use 'dev' for testing, 'main' for stable\n",
    "# =========================\n",
    "\n",
    "def setup_imports():\n",
    "    \"\"\"Setup Python path for imports based on environment.\"\"\"\n",
    "    \n",
    "    IN_COLAB = 'google.colab' in sys.modules\n",
    "    \n",
    "    if IN_COLAB:\n",
    "        repo_path = '/content/torchaudio_aligner'\n",
    "        src_path = f'{repo_path}/src'\n",
    "        \n",
    "        if not os.path.exists(repo_path):\n",
    "            print(f\"Cloning repository (branch: {BRANCH})...\")\n",
    "            os.system(f'git clone -b {BRANCH} {GITHUB_REPO} {repo_path}')\n",
    "            print(\"Repository cloned\")\n",
    "        else:\n",
    "            print(f\"Updating repository (branch: {BRANCH})...\")\n",
    "            os.system(f'cd {repo_path} && git fetch origin && git checkout {BRANCH} && git pull origin {BRANCH}')\n",
    "            print(\"Repository updated\")\n",
    "    else:\n",
    "        possible_paths = [\n",
    "            Path(\".\").absolute().parent / \"src\",\n",
    "            Path(\".\").absolute() / \"src\",\n",
    "        ]\n",
    "        \n",
    "        src_path = None\n",
    "        for p in possible_paths:\n",
    "            if p.exists() and (p / \"labeling_utils\").exists():\n",
    "                src_path = str(p.absolute())\n",
    "                break\n",
    "        \n",
    "        if src_path is None:\n",
    "            raise FileNotFoundError(\"src directory not found\")\n",
    "        \n",
    "        print(f\"Running locally from: {src_path}\")\n",
    "    \n",
    "    if src_path not in sys.path:\n",
    "        sys.path.insert(0, src_path)\n",
    "    \n",
    "    return src_path\n",
    "\n",
    "src_path = setup_imports()\n",
    "\n",
    "# Import labeling_utils\n",
    "from labeling_utils import (\n",
    "    load_model,\n",
    "    get_emissions,\n",
    "    get_emissions_batched,\n",
    "    EmissionResult,\n",
    "    ModelConfig,\n",
    "    list_backends,\n",
    ")\n",
    "\n",
    "import torch\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"Labeling Utils imported successfully!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Available backends: {list_backends()}\")\n",
    "print(f\"Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: Load MMS Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Test 1: Load MMS Model (HuggingFace Backend)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    # Load MMS model for English\n",
    "    backend = load_model(\n",
    "        \"facebook/mms-1b-all\",\n",
    "        language=\"eng\",\n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    )\n",
    "    \n",
    "    print(f\"Model loaded: {backend}\")\n",
    "    print(f\"Is loaded: {backend.is_loaded}\")\n",
    "    print(f\"Frame duration: {backend.frame_duration}s\")\n",
    "    print(f\"Sample rate: {backend.sample_rate}Hz\")\n",
    "    \n",
    "    # Get vocab info\n",
    "    vocab = backend.get_vocab_info()\n",
    "    print(f\"\\nVocabulary:\")\n",
    "    print(f\"  Size: {len(vocab.labels)}\")\n",
    "    print(f\"  Blank ID: {vocab.blank_id} ('{vocab.blank_token}')\")\n",
    "    print(f\"  UNK ID: {vocab.unk_id} ('{vocab.unk_token}')\")\n",
    "    print(f\"  Sample labels: {vocab.labels[:10]}...\")\n",
    "    \n",
    "    print(\"\\nTest 1 PASSED - MMS model loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nTest 1 FAILED: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: Extract Emissions from Sample Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Test 2: Extract Emissions from Sample Audio\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    import torchaudio\n",
    "    \n",
    "    # Download sample audio\n",
    "    sample_url = \"https://pytorch.org/audio/stable/_static/audio.wav\"\n",
    "    \n",
    "    # Create a simple test waveform (1 second of random noise - just for shape testing)\n",
    "    # In real usage, load actual audio\n",
    "    sample_rate = 16000\n",
    "    duration = 2.0  # seconds\n",
    "    waveform = torch.randn(1, int(sample_rate * duration))\n",
    "    \n",
    "    print(f\"Input waveform shape: {waveform.shape}\")\n",
    "    print(f\"Duration: {waveform.shape[1] / sample_rate:.2f}s\")\n",
    "    \n",
    "    # Extract emissions\n",
    "    result = get_emissions(backend, waveform, sample_rate=sample_rate)\n",
    "    \n",
    "    print(f\"\\nEmission result:\")\n",
    "    print(f\"  Emissions shape: {result.emissions.shape}\")\n",
    "    print(f\"  Num frames: {result.num_frames}\")\n",
    "    print(f\"  Vocab size: {result.vocab_size}\")\n",
    "    print(f\"  Duration: {result.duration:.2f}s\")\n",
    "    print(f\"  Frame timestamps (first 10): {result.get_frame_timestamps()[:10].tolist()}\")\n",
    "    \n",
    "    # Verify shape\n",
    "    assert result.emissions.dim() == 2, f\"Expected 2D tensor, got {result.emissions.dim()}D\"\n",
    "    assert result.emissions.shape[-1] == result.vocab_size\n",
    "    \n",
    "    # Verify log probabilities (should sum to ~1 after exp)\n",
    "    probs = torch.exp(result.emissions[0])  # First frame\n",
    "    prob_sum = probs.sum().item()\n",
    "    print(f\"\\n  Prob sum at frame 0: {prob_sum:.4f} (should be ~1.0)\")\n",
    "    \n",
    "    print(\"\\nTest 2 PASSED - Emissions extracted successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nTest 2 FAILED: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3: Real Audio - Meta Earnings Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Test 3: Real Audio - Extract Emissions from Meta Earnings Call\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    import torchaudio\n",
    "    import urllib.request\n",
    "    import os\n",
    "    \n",
    "    # Download a short segment of real audio\n",
    "    audio_url = \"https://static.seekingalpha.com/cdn/s3/transcripts_audio/4780182.mp3\"\n",
    "    audio_file = \"meta_earnings.mp3\"\n",
    "    \n",
    "    if not os.path.exists(audio_file):\n",
    "        print(f\"Downloading audio...\")\n",
    "        urllib.request.urlretrieve(audio_url, audio_file)\n",
    "        print(f\"Downloaded: {audio_file}\")\n",
    "    \n",
    "    # Load first 10 seconds\n",
    "    waveform, sample_rate = torchaudio.load(audio_file, num_frames=10 * 16000)\n",
    "    print(f\"Loaded waveform: shape={waveform.shape}, sample_rate={sample_rate}\")\n",
    "    \n",
    "    # Resample if needed\n",
    "    if sample_rate != 16000:\n",
    "        waveform = torchaudio.functional.resample(waveform, sample_rate, 16000)\n",
    "        sample_rate = 16000\n",
    "    \n",
    "    # Convert to mono\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)\n",
    "    \n",
    "    print(f\"Preprocessed: shape={waveform.shape}\")\n",
    "    \n",
    "    # Extract emissions\n",
    "    result = get_emissions(backend, waveform.squeeze(0), sample_rate=sample_rate)\n",
    "    \n",
    "    print(f\"\\nEmission result:\")\n",
    "    print(f\"  Emissions shape: {result.emissions.shape}\")\n",
    "    print(f\"  Num frames: {result.num_frames}\")\n",
    "    print(f\"  Duration: {result.duration:.2f}s\")\n",
    "    \n",
    "    # Show top predictions for first few frames\n",
    "    print(f\"\\nTop predictions (first 5 frames):\")\n",
    "    vocab = result.vocab_info\n",
    "    for i in range(min(5, result.num_frames)):\n",
    "        top_idx = result.emissions[i].argmax().item()\n",
    "        top_prob = torch.exp(result.emissions[i, top_idx]).item()\n",
    "        label = vocab.id_to_label.get(top_idx, \"?\")\n",
    "        print(f\"  Frame {i}: '{label}' (prob={top_prob:.3f})\")\n",
    "    \n",
    "    print(\"\\nTest 3 PASSED - Real audio emissions extracted successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nTest 3 FAILED: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 4: Batched Emission Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Test 4: Batched Emission Extraction\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    # Create multiple test waveforms of different lengths\n",
    "    sample_rate = 16000\n",
    "    waveforms = [\n",
    "        torch.randn(int(sample_rate * 1.0)),  # 1 second\n",
    "        torch.randn(int(sample_rate * 2.0)),  # 2 seconds\n",
    "        torch.randn(int(sample_rate * 1.5)),  # 1.5 seconds\n",
    "    ]\n",
    "    \n",
    "    print(f\"Input: {len(waveforms)} waveforms\")\n",
    "    for i, w in enumerate(waveforms):\n",
    "        print(f\"  [{i}] shape={w.shape}, duration={len(w)/sample_rate:.2f}s\")\n",
    "    \n",
    "    # Extract emissions in batch\n",
    "    results = get_emissions_batched(\n",
    "        backend,\n",
    "        waveforms,\n",
    "        sample_rate=sample_rate,\n",
    "        batch_size=2,\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nOutput: {len(results)} EmissionResults\")\n",
    "    for i, result in enumerate(results):\n",
    "        print(f\"  [{i}] emissions shape={result.emissions.shape}, duration={result.duration:.2f}s\")\n",
    "    \n",
    "    assert len(results) == len(waveforms), \"Output count mismatch\"\n",
    "    \n",
    "    print(\"\\nTest 4 PASSED - Batched extraction works\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nTest 4 FAILED: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 5: Different Languages (MMS Multilingual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Test 5: Load MMS for Different Languages\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test a few languages\n",
    "languages = [\n",
    "    (\"fra\", \"French\"),\n",
    "    (\"cmn\", \"Mandarin Chinese\"),\n",
    "    (\"jpn\", \"Japanese\"),\n",
    "]\n",
    "\n",
    "for lang_code, lang_name in languages:\n",
    "    print(f\"\\nLoading MMS for {lang_name} ({lang_code})...\")\n",
    "    try:\n",
    "        lang_backend = load_model(\n",
    "            \"facebook/mms-1b-all\",\n",
    "            language=lang_code,\n",
    "            device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        )\n",
    "        \n",
    "        vocab = lang_backend.get_vocab_info()\n",
    "        print(f\"  Loaded! Vocab size: {len(vocab.labels)}\")\n",
    "        \n",
    "        # Quick emission test\n",
    "        test_wav = torch.randn(16000)  # 1 second\n",
    "        result = get_emissions(lang_backend, test_wav)\n",
    "        print(f\"  Emissions shape: {result.emissions.shape}\")\n",
    "        print(f\"  PASSED\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  FAILED: {e}\")\n",
    "\n",
    "print(\"\\nTest 5 Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Test 6: TorchAudio Pipeline Backend (MMS_FA)\n\nNote: This test uses the TorchAudio pipeline API which has a different interface than HuggingFace."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\" * 60)\nprint(\"Test 6: TorchAudio Pipeline Backend (MMS_FA)\")\nprint(\"=\" * 60)\n\ntry:\n    from labeling_utils import TorchAudioPipelineBackend, BackendConfig\n    \n    # Create config for MMS_FA\n    config = BackendConfig(\n        model_name=\"MMS_FA\",\n        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n        with_star=True,\n    )\n    \n    # Create and load backend\n    ta_backend = TorchAudioPipelineBackend(config)\n    ta_backend.load()\n    \n    print(f\"Model loaded: {ta_backend}\")\n    print(f\"Is loaded: {ta_backend.is_loaded}\")\n    print(f\"Frame duration: {ta_backend.frame_duration}s\")\n    print(f\"Sample rate: {ta_backend.sample_rate}Hz\")\n    \n    # Get vocab info\n    vocab = ta_backend.get_vocab_info()\n    print(f\"\\nVocabulary:\")\n    print(f\"  Size: {len(vocab.labels)}\")\n    print(f\"  Labels: {vocab.labels[:15]}...\")\n    print(f\"  Blank ID: {vocab.blank_id} ('{vocab.blank_token}')\")\n    print(f\"  UNK ID: {vocab.unk_id} ('{vocab.unk_token}')\")\n    \n    # Test emission extraction\n    test_wav = torch.randn(16000 * 2)  # 2 seconds\n    result = get_emissions(ta_backend, test_wav)\n    \n    print(f\"\\nEmission result:\")\n    print(f\"  Emissions shape: {result.emissions.shape}\")\n    print(f\"  Num frames: {result.num_frames}\")\n    print(f\"  Vocab size: {result.vocab_size}\")\n    \n    print(\"\\nTest 6 PASSED - TorchAudio Pipeline backend works\")\nexcept Exception as e:\n    print(f\"\\nTest 6 FAILED: {e}\")\n    print(\"Note: This test requires torchaudio with MMS_FA pipeline.\")\n    print(\"If MMS_FA is not available, this is expected.\")\n    import traceback\n    traceback.print_exc()"
  },
  {
   "cell_type": "markdown",
   "source": "## Test 7: Integration with Audio Frontend",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "print(\"=\" * 60)\nprint(\"LABELING UTILS TEST SUMMARY\")\nprint(\"=\" * 60)\nprint(\"\"\"\nThe labeling_utils module provides:\n\n1. load_model() - Load CTC models from HuggingFace or TorchAudio\n   - Supports MMS (1100+ languages)\n   - Supports Wav2Vec2 variants\n   - Automatic language adapter loading\n\n2. get_emissions() - Extract frame-wise log posteriors\n   - Returns EmissionResult with metadata\n   - Automatic resampling if needed\n\n3. get_emissions_batched() - Efficient batch processing\n   - Process multiple audio files at once\n\n4. Extensible backend system:\n   - HuggingFaceCTCBackend: For HuggingFace models (facebook/mms-1b-all, etc.)\n   - TorchAudioPipelineBackend: For TorchAudio pipelines (MMS_FA, etc.)\n   - Easy to add NeMo, ESPnet, etc. via register_backend()\n\nNext steps:\n- Use emissions with k2 WFST for alignment\n- Add more backends (NeMo, ESPnet, OmniASR)\n\"\"\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"LABELING UTILS TEST SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "The labeling_utils module provides:\n",
    "\n",
    "1. load_model() - Load CTC models from HuggingFace\n",
    "   - Supports MMS (1100+ languages)\n",
    "   - Supports Wav2Vec2 variants\n",
    "   - Automatic language adapter loading\n",
    "\n",
    "2. get_emissions() - Extract frame-wise log posteriors\n",
    "   - Returns EmissionResult with metadata\n",
    "   - Automatic resampling if needed\n",
    "\n",
    "3. get_emissions_batched() - Efficient batch processing\n",
    "   - Process multiple audio files at once\n",
    "\n",
    "4. Extensible backend system:\n",
    "   - HuggingFaceCTCBackend (current)\n",
    "   - Easy to add NeMo, ESPnet, etc.\n",
    "\n",
    "Next steps:\n",
    "- Use emissions with k2 WFST for alignment\n",
    "- Add more backends (NeMo, ESPnet, OmniASR)\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}