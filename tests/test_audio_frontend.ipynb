{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Frontend Module Tests\n",
    "\n",
    "This notebook tests the `audio_frontend` module of the TorchAudio Long-Form Aligner.\n",
    "\n",
    "Each test cell will display:\n",
    "- ‚úÖ if the test passes\n",
    "- ‚ùå if the test fails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check PyTorch and TorchAudio versions\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"TorchAudio: {torchaudio.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install torchcodec if using torchaudio >= 2.8\n",
    "import torchaudio\n",
    "version_parts = torchaudio.__version__.split('.')\n",
    "major, minor = int(version_parts[0]), int(version_parts[1].split('+')[0])\n",
    "if (major, minor) >= (2, 8):\n",
    "    print(\"TorchAudio >= 2.8 detected, installing torchcodec...\")\n",
    "    !pip install -q torchcodec\n",
    "else:\n",
    "    print(f\"TorchAudio {torchaudio.__version__} - torchcodec not required\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<cell_type>markdown</cell_type>## Import the Audio Frontend Module\n\nImport from the modular `audio_frontend` package:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# Setup: Clone Repository and Configure Imports\n# =============================================================================\n# This cell sets up the environment for both Colab and local execution.\n#\n# For Colab users:\n#   - Clones the repo from GitHub (dev branch for testing)\n#   - No authentication needed for public repos\n#\n# For local users:\n#   - Automatically finds the src/ directory\n# =============================================================================\n\nimport sys\nimport os\nfrom pathlib import Path\n\n# ===== CONFIGURATION =====\nGITHUB_REPO = \"https://github.com/huangruizhe/torchaudio_aligner.git\"\nBRANCH = \"dev\"  # Use 'dev' for testing, 'main' for stable\n# =========================\n\ndef setup_imports():\n    \"\"\"Setup Python path for imports based on environment.\"\"\"\n    \n    # Check if running in Colab\n    IN_COLAB = 'google.colab' in sys.modules\n    \n    if IN_COLAB:\n        repo_path = '/content/torchaudio_aligner'\n        src_path = f'{repo_path}/src'\n        \n        # Clone or update repo\n        if not os.path.exists(repo_path):\n            print(f\"üì• Cloning repository (branch: {BRANCH})...\")\n            os.system(f'git clone -b {BRANCH} {GITHUB_REPO} {repo_path}')\n            print(\"‚úÖ Repository cloned\")\n        else:\n            # Pull latest changes\n            print(f\"üì• Updating repository (branch: {BRANCH})...\")\n            os.system(f'cd {repo_path} && git fetch origin && git checkout {BRANCH} && git pull origin {BRANCH}')\n            print(\"‚úÖ Repository updated\")\n        \n        # Verify src exists\n        if os.path.exists(src_path):\n            print(f\"‚úÖ Found src at: {src_path}\")\n        else:\n            print(f\"‚ùå src directory NOT found at: {src_path}\")\n            raise FileNotFoundError(f\"src not found at {src_path}\")\n    \n    else:\n        # Local environment - find src directory\n        possible_paths = [\n            Path(\".\").absolute().parent / \"src\",  # Running from tests/\n            Path(\".\").absolute() / \"src\",          # Running from project root\n        ]\n        \n        src_path = None\n        for p in possible_paths:\n            if p.exists() and (p / \"audio_frontend\").exists():\n                src_path = str(p.absolute())\n                break\n        \n        if src_path is None:\n            print(\"‚ùå Could not find src directory locally\")\n            print(f\"   Current directory: {os.getcwd()}\")\n            raise FileNotFoundError(\"src directory not found\")\n        \n        print(f\"‚úÖ Running locally from: {src_path}\")\n    \n    # Add to Python path\n    if src_path not in sys.path:\n        sys.path.insert(0, src_path)\n    \n    return src_path\n\n# Run setup\nsrc_path = setup_imports()\n\n# Now import from the modular audio_frontend package\nfrom audio_frontend import (\n    # Loaders\n    load_audio,\n    get_available_backends,\n    AudioBackend,\n    # Preprocessing\n    resample,\n    to_mono,\n    normalize_peak,\n    preprocess,\n    # Segmentation\n    AudioSegment,\n    SegmentationResult,\n    segment_waveform,\n    # Enhancement\n    AudioEnhancement,\n    EnhancementResult,\n    TimeMappingManager,\n    enhance_audio,\n    denoise_noisereduce,\n    get_available_enhancement_backends,\n    # Frontend\n    AudioFrontend,\n    segment_audio,\n)\n\nimport logging\nlogging.basicConfig(level=logging.INFO)\n\nprint()\nprint(\"=\" * 60)\nprint(\"‚úÖ Audio Frontend imported successfully!\")\nprint(\"=\" * 60)\nprint(\"Modules loaded:\")\nprint(\"  ‚Ä¢ loaders: load_audio, AudioBackend\")\nprint(\"  ‚Ä¢ preprocessing: resample, to_mono, normalize_peak\")\nprint(\"  ‚Ä¢ segmentation: AudioSegment, SegmentationResult\")\nprint(\"  ‚Ä¢ enhancement: AudioEnhancement, TimeMappingManager\")\nprint(\"  ‚Ä¢ frontend: AudioFrontend, segment_audio\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Test Audio\n",
    "\n",
    "We'll use Meta's Q1 2025 earnings call as test audio (same as in Tutorial.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download test audio (Meta Q1 2025 Earnings Call - ~1 hour)\n",
    "!wget -q https://static.seekingalpha.com/cdn/s3/transcripts_audio/4780182.mp3 -O test_audio.mp3\n",
    "!ls -lh test_audio.mp3\n",
    "\n",
    "TEST_AUDIO = \"test_audio.mp3\"\n",
    "print(\"‚úÖ Test audio downloaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: Load Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Test 1: AudioFrontend.load()\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    frontend = AudioFrontend(target_sample_rate=16000)\n",
    "    waveform, sample_rate = frontend.load(TEST_AUDIO)\n",
    "\n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"  Waveform shape: {waveform.shape}\")\n",
    "    print(f\"  Sample rate: {sample_rate} Hz\")\n",
    "    print(f\"  Duration: {waveform.shape[1] / sample_rate:.2f} seconds\")\n",
    "    print(f\"  Duration: {waveform.shape[1] / sample_rate / 60:.2f} minutes\")\n",
    "\n",
    "    assert waveform.dim() == 2, \"Waveform should be 2D\"\n",
    "    assert sample_rate > 0, \"Sample rate should be positive\"\n",
    "    print(\"\\n‚úÖ Test 1 PASSED\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Test 1 FAILED: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: Resample Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Test 2: AudioFrontend.resample()\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    print(f\"\\nOriginal sample rate: {sample_rate} Hz\")\n",
    "    print(f\"Original samples: {waveform.shape[1]}\")\n",
    "\n",
    "    resampled = frontend.resample(waveform, sample_rate, 16000)\n",
    "\n",
    "    expected_samples = int(waveform.shape[1] * 16000 / sample_rate)\n",
    "    print(f\"Resampled samples: {resampled.shape[1]}\")\n",
    "    print(f\"Expected samples (approx): {expected_samples}\")\n",
    "\n",
    "    assert abs(resampled.shape[1] - expected_samples) < 100, \"Resampled length mismatch\"\n",
    "    print(\"\\n‚úÖ Test 2 PASSED\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Test 2 FAILED: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3: Convert to Mono"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Test 3: AudioFrontend.to_mono()\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    print(f\"\\nOriginal channels: {waveform.shape[0]}\")\n",
    "\n",
    "    mono = frontend.to_mono(waveform)\n",
    "\n",
    "    print(f\"Mono channels: {mono.shape[0]}\")\n",
    "    assert mono.shape[0] == 1, \"Should have 1 channel\"\n",
    "    print(\"\\n‚úÖ Test 3 PASSED\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Test 3 FAILED: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 4: Segment Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Test 4: AudioFrontend.segment()\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    frontend = AudioFrontend(target_sample_rate=16000, mono=True)\n",
    "    waveform, orig_sr = frontend.load(TEST_AUDIO)\n",
    "    waveform = frontend.resample(waveform, orig_sr)\n",
    "    waveform = frontend.to_mono(waveform)\n",
    "\n",
    "    result = frontend.segment(\n",
    "        waveform,\n",
    "        sample_rate=16000,\n",
    "        segment_size=15.0,\n",
    "        overlap=2.0,\n",
    "        min_segment_size=0.2,\n",
    "    )\n",
    "\n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"  Original duration: {result.original_duration_seconds:.2f} seconds ({result.original_duration_seconds/60:.2f} min)\")\n",
    "    print(f\"  Number of segments: {result.num_segments}\")\n",
    "    print(f\"  Segment size: {result.segment_size_samples} samples ({result.segment_size_samples/16000:.2f}s)\")\n",
    "    print(f\"  Overlap: {result.overlap_samples} samples ({result.overlap_samples/16000:.2f}s)\")\n",
    "\n",
    "    print(f\"\\nFirst 3 segments:\")\n",
    "    for i, seg in enumerate(result.segments[:3]):\n",
    "        print(f\"  Segment {i}: offset={seg.offset_seconds:.2f}s, duration={seg.duration_seconds:.2f}s, shape={seg.waveform.shape}\")\n",
    "\n",
    "    print(f\"\\nLast segment:\")\n",
    "    last_seg = result.segments[-1]\n",
    "    print(f\"  Segment {last_seg.segment_index}: offset={last_seg.offset_seconds:.2f}s, duration={last_seg.duration_seconds:.2f}s\")\n",
    "\n",
    "    assert result.num_segments > 0, \"Should have at least one segment\"\n",
    "    assert all(seg.sample_rate == 16000 for seg in result.segments), \"All segments should have correct sample rate\"\n",
    "    print(\"\\n‚úÖ Test 4 PASSED\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Test 4 FAILED: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 5: Full Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Test 5: AudioFrontend.process() - Full Pipeline\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    frontend = AudioFrontend(\n",
    "        target_sample_rate=16000,\n",
    "        mono=True,\n",
    "        normalize=False,\n",
    "    )\n",
    "\n",
    "    result = frontend.process(\n",
    "        TEST_AUDIO,\n",
    "        segment_size=15.0,\n",
    "        overlap=2.0,\n",
    "    )\n",
    "\n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"  Original duration: {result.original_duration_seconds:.2f} seconds\")\n",
    "    print(f\"  Number of segments: {result.num_segments}\")\n",
    "\n",
    "    assert isinstance(result, SegmentationResult)\n",
    "    assert result.num_segments > 0\n",
    "    print(\"\\n‚úÖ Test 5 PASSED\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Test 5 FAILED: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 6: Batching for GPU Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Test 6: SegmentationResult.get_waveforms_batched()\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    waveforms, lengths = result.get_waveforms_batched()\n",
    "\n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"  Batched waveforms shape: {waveforms.shape}\")\n",
    "    print(f\"  Lengths shape: {lengths.shape}\")\n",
    "    print(f\"  First 5 lengths: {lengths[:5].tolist()}\")\n",
    "    print(f\"  Last 5 lengths: {lengths[-5:].tolist()}\")\n",
    "\n",
    "    assert waveforms.shape[0] == result.num_segments, \"Batch size mismatch\"\n",
    "    assert lengths.shape[0] == result.num_segments, \"Lengths mismatch\"\n",
    "    assert waveforms.dim() == 2, \"Should be 2D for mono\"\n",
    "    print(\"\\n‚úÖ Test 6 PASSED\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Test 6 FAILED: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 7: Frame Offset Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Test 7: SegmentationResult.get_offsets_in_frames()\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    # MMS model has 20ms frame duration\n",
    "    frame_duration = 0.02\n",
    "    offsets = result.get_offsets_in_frames(frame_duration)\n",
    "\n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"  Frame duration: {frame_duration}s (20ms)\")\n",
    "    print(f\"  Frame offsets shape: {offsets.shape}\")\n",
    "    print(f\"  First 5 offsets (frames): {offsets[:5].tolist()}\")\n",
    "\n",
    "    # Verify monotonically increasing\n",
    "    is_monotonic = all(offsets[i] < offsets[i+1] for i in range(len(offsets)-1))\n",
    "    print(f\"  Monotonically increasing: {is_monotonic}\")\n",
    "\n",
    "    assert is_monotonic, \"Offsets should be monotonically increasing\"\n",
    "    print(\"\\n‚úÖ Test 7 PASSED\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Test 7 FAILED: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 8: Convenience Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Test 8: segment_audio() convenience function\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    result = segment_audio(\n",
    "        TEST_AUDIO,\n",
    "        target_sample_rate=16000,\n",
    "        segment_size=15.0,\n",
    "        overlap=2.0,\n",
    "    )\n",
    "\n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"  Duration: {result.original_duration_seconds:.2f}s\")\n",
    "    print(f\"  Segments: {result.num_segments}\")\n",
    "\n",
    "    assert isinstance(result, SegmentationResult)\n",
    "    print(\"\\n‚úÖ Test 8 PASSED\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Test 8 FAILED: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 9: Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Test 9: Audio Normalization\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    frontend_norm = AudioFrontend(target_sample_rate=16000, mono=True, normalize=True, normalize_db=-3.0)\n",
    "\n",
    "    waveform, sr = frontend_norm.load(TEST_AUDIO)\n",
    "    waveform = frontend_norm.resample(waveform, sr)\n",
    "    waveform = frontend_norm.to_mono(waveform)\n",
    "\n",
    "    original_peak = waveform.abs().max().item()\n",
    "    print(f\"\\nOriginal peak: {original_peak:.4f}\")\n",
    "\n",
    "    normalized = frontend_norm.apply_normalization(waveform.clone())\n",
    "    normalized_peak = normalized.abs().max().item()\n",
    "    print(f\"Normalized peak: {normalized_peak:.4f}\")\n",
    "\n",
    "    expected_peak = 10 ** (-3.0 / 20)  # -3 dB\n",
    "    print(f\"Expected peak (-3dB): {expected_peak:.4f}\")\n",
    "\n",
    "    assert abs(normalized_peak - expected_peak) < 0.01, \"Normalized peak mismatch\"\n",
    "    print(\"\\n‚úÖ Test 9 PASSED\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Test 9 FAILED: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 10: Listen to a Segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Test 10: Listen to a Segment (Visual/Audio Check)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    import IPython.display as ipd\n",
    "\n",
    "    result = segment_audio(TEST_AUDIO, segment_size=15.0, overlap=2.0)\n",
    "\n",
    "    # Play first segment\n",
    "    seg = result.segments[0]\n",
    "    print(f\"\\nPlaying Segment 0:\")\n",
    "    print(f\"  Offset: {seg.offset_seconds:.2f}s\")\n",
    "    print(f\"  Duration: {seg.duration_seconds:.2f}s\")\n",
    "    ipd.display(ipd.Audio(seg.waveform.numpy(), rate=seg.sample_rate))\n",
    "\n",
    "    # Play a middle segment\n",
    "    mid_idx = result.num_segments // 2\n",
    "    seg = result.segments[mid_idx]\n",
    "    print(f\"\\nPlaying Segment {mid_idx} (middle):\")\n",
    "    print(f\"  Offset: {seg.offset_seconds:.2f}s\")\n",
    "    print(f\"  Duration: {seg.duration_seconds:.2f}s\")\n",
    "    ipd.display(ipd.Audio(seg.waveform.numpy(), rate=seg.sample_rate))\n",
    "    \n",
    "    print(\"\\n‚úÖ Test 10 PASSED (verify audio plays correctly)\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Test 10 FAILED: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Test 11-14: Audio Enhancement Module (Demucs + VAD)\n\nThese tests verify the optional audio enhancement features:\n- Demucs source separation (vocal extraction)\n- Silence removal\n- Voice Activity Detection (VAD)\n- Timestamp mapping for alignment recovery\n\n**Note**: These require optional dependencies:\n```\npip install demucs pyloudnorm\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Install optional enhancement dependencies\n!pip install -q demucs pyloudnorm\n\n# Check availability\ntry:\n    import demucs\n    from demucs.pretrained import get_model_from_args\n    from demucs.apply import apply_model\n    DEMUCS_AVAILABLE = True\n    print(\"‚úÖ demucs available\")\nexcept ImportError:\n    DEMUCS_AVAILABLE = False\n    print(\"‚ùå demucs not available\")\n\ntry:\n    import pyloudnorm\n    PYLOUDNORM_AVAILABLE = True\n    print(\"‚úÖ pyloudnorm available\")\nexcept ImportError:\n    PYLOUDNORM_AVAILABLE = False\n    print(\"‚ùå pyloudnorm not available\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "print(\"=\" * 60)\nprint(\"Test 11: TimeMappingManager - Timestamp Recovery\")\nprint(\"=\" * 60)\nprint(\"\"\"\nWhen silence is removed from audio, timestamps change.\nTimeMappingManager tracks these changes for recovery.\n\nExample:\n  Original audio:  [speech][silence][speech][silence][speech]\n                    0-2s    2-5s     5-8s    8-10s    10-15s\n  \n  After removal:   [speech][speech][speech]\n                    0-2s    2-5s    5-10s\n  \n  Mapping: processed_time=3.0 -> original_time=6.0\n\"\"\")\n\n# TimeMappingManager is now imported from audio_frontend.enhancement\n\ntry:\n    # Test case\n    mapper = TimeMappingManager([(0, 1), (3, 5), (6, 8)])\n    \n    # Test mappings\n    test_cases = [\n        (-1, -1),\n        (0, 1),      # After removing 0-1s silence, time 0 maps to 1\n        (0.5, 1.5),\n        (1, 2),\n        (2, 5),      # After removing 3-5s silence, time 2 maps to 5\n        (3, 8),      # After removing 6-8s silence, time 3 maps to 8\n    ]\n    \n    print(\"Testing timestamp mappings:\")\n    all_passed = True\n    for processed, expected_original in test_cases:\n        actual = mapper.map_to_original(processed)\n        passed = abs(actual - expected_original) < 1e-6\n        status = \"‚úÖ\" if passed else \"‚ùå\"\n        print(f\"  {status} map_to_original({processed}) = {actual:.2f} (expected {expected_original})\")\n        all_passed = all_passed and passed\n    \n    assert all_passed, \"Some timestamp mappings failed\"\n    print(\"\\n‚úÖ Test 11 PASSED - TimeMappingManager works correctly\")\nexcept Exception as e:\n    print(f\"\\n‚ùå Test 11 FAILED: {e}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "print(\"=\" * 60)\nprint(\"Test 12: Silence Removal (Energy-based)\")\nprint(\"=\" * 60)\n\ntry:\n    # Create test audio with silence\n    sr = 16000\n    duration = 10.0  # 10 seconds\n    \n    # Create audio: [noise 0-2s][silence 2-4s][noise 4-7s][silence 7-9s][noise 9-10s]\n    samples = int(sr * duration)\n    waveform = torch.zeros(samples)\n    \n    # Add noise to speech segments\n    waveform[0:int(sr*2)] = torch.randn(int(sr*2)) * 0.5       # 0-2s: speech\n    waveform[int(sr*4):int(sr*7)] = torch.randn(int(sr*3)) * 0.5  # 4-7s: speech  \n    waveform[int(sr*9):int(sr*10)] = torch.randn(int(sr*1)) * 0.5 # 9-10s: speech\n    \n    print(f\"üìÑ Created test audio: {duration}s with 3 speech segments\")\n    print(f\"   Speech: 0-2s, 4-7s, 9-10s (total 6s)\")\n    print(f\"   Silence: 2-4s, 7-9s (total 4s)\")\n    \n    # Simple silence removal\n    def remove_silence_simple(waveform, sr, threshold_db=-50, min_dur=0.2):\n        threshold = 10 ** (threshold_db / 20)\n        frame_size = int(sr * 0.02)\n        hop_size = int(sr * 0.01)\n        \n        silence_intervals = []\n        in_silence = False\n        silence_start = 0\n        \n        for i in range(0, waveform.shape[0] - frame_size, hop_size):\n            frame = waveform[i:i + frame_size]\n            energy = frame.abs().max().item()\n            \n            if energy < threshold:\n                if not in_silence:\n                    in_silence = True\n                    silence_start = i / sr\n            else:\n                if in_silence:\n                    in_silence = False\n                    silence_end = i / sr\n                    if silence_end - silence_start >= min_dur:\n                        silence_intervals.append((silence_start, silence_end))\n        \n        if in_silence:\n            silence_end = waveform.shape[0] / sr\n            if silence_end - silence_start >= min_dur:\n                silence_intervals.append((silence_start, silence_end))\n        \n        return silence_intervals\n    \n    silence_intervals = remove_silence_simple(waveform, sr)\n    print(f\"\\nüìÑ Detected silence intervals: {silence_intervals}\")\n    \n    # Should detect approximately 2-4s and 7-9s\n    assert len(silence_intervals) >= 2, \"Should detect at least 2 silence periods\"\n    print(f\"‚úÖ Detected {len(silence_intervals)} silence periods\")\n    \n    print(\"\\n‚úÖ Test 12 PASSED - Silence removal works\")\nexcept Exception as e:\n    print(f\"\\n‚ùå Test 12 FAILED: {e}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "print(\"=\" * 60)\nprint(\"Test 13: Silero VAD (Voice Activity Detection)\")\nprint(\"=\" * 60)\n\ntry:\n    # Load Silero VAD\n    print(\"Loading Silero VAD model...\")\n    vad_model, utils = torch.hub.load(\n        repo_or_dir=\"snakers4/silero-vad\",\n        model=\"silero_vad\",\n        force_reload=False,\n        onnx=False,\n    )\n    get_speech_timestamps, save_audio, read_audio, VADIterator, collect_chunks = utils\n    print(\"‚úÖ Silero VAD loaded\")\n    \n    # Load a short segment of test audio\n    waveform, orig_sr = torchaudio.load(TEST_AUDIO)\n    waveform = torchaudio.functional.resample(waveform, orig_sr, 16000)\n    waveform = waveform.mean(0)  # Mono\n    \n    # Take first 30 seconds for faster testing\n    waveform = waveform[:16000 * 30]\n    \n    print(f\"\\nüìÑ Test audio: {waveform.shape[0]/16000:.2f}s\")\n    \n    # Get speech timestamps\n    speech_timestamps = get_speech_timestamps(\n        waveform,\n        vad_model,\n        threshold=0.4,\n        min_silence_duration_ms=500,\n        sampling_rate=16000,\n    )\n    \n    print(f\"üìÑ Detected {len(speech_timestamps)} speech segments:\")\n    for i, ts in enumerate(speech_timestamps[:5]):\n        start = ts['start'] / 16000\n        end = ts['end'] / 16000\n        print(f\"   [{i}] {start:.2f}s - {end:.2f}s (duration: {end-start:.2f}s)\")\n    if len(speech_timestamps) > 5:\n        print(f\"   ... and {len(speech_timestamps) - 5} more\")\n    \n    # Collect speech chunks\n    speech_waveform = collect_chunks(speech_timestamps, waveform)\n    \n    original_dur = waveform.shape[0] / 16000\n    speech_dur = speech_waveform.shape[0] / 16000\n    print(f\"\\nüìÑ VAD result: {original_dur:.2f}s -> {speech_dur:.2f}s ({100*speech_dur/original_dur:.1f}%)\")\n    \n    assert len(speech_timestamps) > 0, \"Should detect some speech\"\n    assert speech_dur < original_dur, \"Speech duration should be less than original\"\n    print(\"\\n‚úÖ Test 13 PASSED - Silero VAD works\")\nexcept Exception as e:\n    print(f\"\\n‚ùå Test 13 FAILED: {e}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "print(\"=\" * 60)\nprint(\"Test 14: Demucs Vocal Extraction (Optional - Slow)\")\nprint(\"=\" * 60)\n\nif not DEMUCS_AVAILABLE:\n    print(\"‚ö†Ô∏è Demucs not installed, skipping test\")\n    print(\"   Install with: pip install demucs\")\nelse:\n    try:\n        from demucs.pretrained import get_model_from_args\n        from demucs.apply import apply_model\n        \n        # Load Demucs model\n        print(\"Loading Demucs model (htdemucs)...\")\n        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        demucs_model = get_model_from_args(\n            type(\"args\", (object,), dict(name=\"htdemucs\", repo=None))\n        )\n        demucs_model = demucs_model.to(device).eval()\n        print(f\"‚úÖ Demucs loaded on {device}\")\n        print(f\"   Sources: {demucs_model.sources}\")  # ['drums', 'bass', 'other', 'vocals']\n        \n        # Load a short segment (10 seconds) - Demucs is slow\n        waveform, orig_sr = torchaudio.load(TEST_AUDIO)\n        waveform = waveform[:, :orig_sr * 10]  # First 10 seconds\n        \n        print(f\"\\nüìÑ Test audio: {waveform.shape[1]/orig_sr:.2f}s at {orig_sr}Hz\")\n        \n        # Convert audio using torchaudio (demucs.audio API changed in newer versions)\n        # Demucs expects stereo audio at model.samplerate\n        if waveform.shape[0] == 1:\n            waveform_stereo = waveform.repeat(2, 1)\n        else:\n            waveform_stereo = waveform\n        \n        # Resample to model's sample rate\n        if orig_sr != demucs_model.samplerate:\n            wav = torchaudio.functional.resample(waveform_stereo, orig_sr, demucs_model.samplerate)\n        else:\n            wav = waveform_stereo\n        \n        # Add batch dimension [B, C, T]\n        if wav.dim() == 2:\n            wav = wav.unsqueeze(0)\n        wav = wav.to(device)\n        \n        print(\"Applying Demucs source separation...\")\n        with torch.no_grad():\n            result = apply_model(demucs_model, wav, device=device, split=True, overlap=0.25)\n        \n        # Extract vocals\n        vocals_idx = demucs_model.sources.index(\"vocals\")\n        vocals = result[0, vocals_idx].mean(0).cpu()  # Average channels to mono\n        vocals = torchaudio.functional.resample(vocals, demucs_model.samplerate, orig_sr)\n        \n        print(f\"\\nüìÑ Extracted vocals: {vocals.shape[0]/orig_sr:.2f}s\")\n        print(f\"   Original peak: {waveform.abs().max().item():.4f}\")\n        print(f\"   Vocals peak: {vocals.abs().max().item():.4f}\")\n        \n        # Listen to comparison\n        import IPython.display as ipd\n        print(\"\\nüîä Original audio (first 10s):\")\n        ipd.display(ipd.Audio(waveform.mean(0).cpu().numpy(), rate=orig_sr))\n        \n        print(\"üîä Extracted vocals:\")\n        ipd.display(ipd.Audio(vocals.cpu().numpy(), rate=orig_sr))\n        \n        print(\"\\n‚úÖ Test 14 PASSED - Demucs vocal extraction works\")\n    except Exception as e:\n        print(f\"\\n‚ùå Test 14 FAILED: {e}\")\n        import traceback\n        traceback.print_exc()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Test 15-17: Additional Denoising Libraries\n\nThese tests verify additional denoising options:\n- **noisereduce**: Lightweight spectral gating (CPU-friendly) - **Recommended, works everywhere**\n- **DeepFilterNet**: Deep learning noise suppression (48kHz full-band) - requires Rust compiler\n- **Resemble Enhance**: AI speech denoising - requires torch==2.1.1\n\n**For Colab, just use noisereduce:**\n```\npip install noisereduce\n```\n\nThe other options have complex build requirements. noisereduce is lightweight, effective, and works on CPU.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Check availability of additional denoising libraries\nprint(\"Checking additional denoising libraries...\")\n\nNOISEREDUCE_AVAILABLE = False\nDEEPFILTERNET_AVAILABLE = False\nRESEMBLE_ENHANCE_AVAILABLE = False\n\ntry:\n    import noisereduce as nr\n    NOISEREDUCE_AVAILABLE = True\n    print(\"‚úÖ noisereduce available\")\nexcept ImportError:\n    print(\"‚ùå noisereduce not available (pip install noisereduce)\")\n\ntry:\n    from df.enhance import enhance, init_df, load_audio, save_audio\n    DEEPFILTERNET_AVAILABLE = True\n    print(\"‚úÖ deepfilternet available\")\nexcept ImportError:\n    print(\"‚ùå deepfilternet not available (pip install deepfilternet)\")\n\ntry:\n    from resemble_enhance.enhancer.inference import denoise, enhance as resemble_enhance_fn\n    RESEMBLE_ENHANCE_AVAILABLE = True\n    print(\"‚úÖ resemble-enhance available\")\nexcept ImportError:\n    print(\"‚ùå resemble-enhance not available (pip install resemble-enhance)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "print(\"=\" * 60)\nprint(\"Test 15: noisereduce - Spectral Gating Noise Reduction\")\nprint(\"=\" * 60)\n\nif not NOISEREDUCE_AVAILABLE:\n    print(\"‚ö†Ô∏è noisereduce not installed, skipping test\")\n    print(\"   Install with: pip install noisereduce\")\nelse:\n    try:\n        import noisereduce as nr\n        import numpy as np\n        \n        # Load test audio\n        waveform, orig_sr = torchaudio.load(TEST_AUDIO)\n        waveform = waveform.mean(0)  # Mono\n        waveform = waveform[:orig_sr * 10]  # First 10 seconds\n        \n        print(f\"üìÑ Test audio: {waveform.shape[0]/orig_sr:.2f}s at {orig_sr}Hz\")\n        \n        # Apply noisereduce\n        audio_np = waveform.numpy()\n        \n        print(\"Applying noisereduce (stationary=False)...\")\n        reduced = nr.reduce_noise(\n            y=audio_np,\n            sr=orig_sr,\n            stationary=False,\n            prop_decrease=1.0,\n            n_fft=512,\n        )\n        \n        reduced_tensor = torch.from_numpy(reduced).float()\n        \n        print(f\"\\nüìÑ Result:\")\n        print(f\"   Original peak: {waveform.abs().max().item():.4f}\")\n        print(f\"   Denoised peak: {reduced_tensor.abs().max().item():.4f}\")\n        \n        # Listen to comparison\n        import IPython.display as ipd\n        print(\"\\nüîä Original audio (first 10s):\")\n        ipd.display(ipd.Audio(audio_np, rate=orig_sr))\n        \n        print(\"üîä noisereduce denoised:\")\n        ipd.display(ipd.Audio(reduced, rate=orig_sr))\n        \n        assert reduced.shape == audio_np.shape, \"Output shape should match input\"\n        print(\"\\n‚úÖ Test 15 PASSED - noisereduce works\")\n    except Exception as e:\n        print(f\"\\n‚ùå Test 15 FAILED: {e}\")\n        import traceback\n        traceback.print_exc()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "print(\"=\" * 60)\nprint(\"Test 16: DeepFilterNet - Deep Learning Noise Suppression\")\nprint(\"=\" * 60)\n\nif not DEEPFILTERNET_AVAILABLE:\n    print(\"‚ö†Ô∏è DeepFilterNet not installed, skipping test\")\n    print(\"   Install with: pip install deepfilternet\")\nelse:\n    try:\n        from df.enhance import enhance, init_df, load_audio, save_audio\n        \n        # Initialize DeepFilterNet\n        print(\"Initializing DeepFilterNet...\")\n        model, df_state, _ = init_df()\n        print(f\"‚úÖ DeepFilterNet initialized (sample_rate: {df_state.sr()}Hz)\")\n        \n        # Load test audio at DeepFilterNet's native sample rate (48kHz)\n        waveform, orig_sr = torchaudio.load(TEST_AUDIO)\n        waveform = waveform.mean(0)  # Mono\n        \n        # Resample to 48kHz if needed\n        if orig_sr != df_state.sr():\n            waveform = torchaudio.functional.resample(waveform, orig_sr, df_state.sr())\n        \n        # Take first 10 seconds\n        waveform = waveform[:df_state.sr() * 10]\n        \n        print(f\"üìÑ Test audio: {waveform.shape[0]/df_state.sr():.2f}s at {df_state.sr()}Hz\")\n        \n        # Apply DeepFilterNet\n        print(\"Applying DeepFilterNet...\")\n        enhanced = enhance(model, df_state, waveform.numpy())\n        enhanced_tensor = torch.from_numpy(enhanced).float()\n        \n        # Resample back if needed\n        if orig_sr != df_state.sr():\n            enhanced_tensor = torchaudio.functional.resample(\n                enhanced_tensor, df_state.sr(), orig_sr\n            )\n            waveform = torchaudio.functional.resample(waveform, df_state.sr(), orig_sr)\n        \n        print(f\"\\nüìÑ Result:\")\n        print(f\"   Original peak: {waveform.abs().max().item():.4f}\")\n        print(f\"   Enhanced peak: {enhanced_tensor.abs().max().item():.4f}\")\n        \n        # Listen to comparison\n        import IPython.display as ipd\n        print(\"\\nüîä Original audio (first 10s):\")\n        ipd.display(ipd.Audio(waveform.numpy(), rate=orig_sr))\n        \n        print(\"üîä DeepFilterNet enhanced:\")\n        ipd.display(ipd.Audio(enhanced_tensor.numpy(), rate=orig_sr))\n        \n        print(\"\\n‚úÖ Test 16 PASSED - DeepFilterNet works\")\n    except Exception as e:\n        print(f\"\\n‚ùå Test 16 FAILED: {e}\")\n        import traceback\n        traceback.print_exc()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "print(\"=\" * 60)\nprint(\"Test 17: Resemble Enhance - AI Speech Enhancement\")\nprint(\"=\" * 60)\n\nif not RESEMBLE_ENHANCE_AVAILABLE:\n    print(\"‚ö†Ô∏è Resemble Enhance not installed, skipping test\")\n    print(\"   Install with: pip install resemble-enhance\")\n    print(\"   Note: Heavy model (~1GB), best with GPU\")\nelse:\n    try:\n        from resemble_enhance.enhancer.inference import denoise, enhance as resemble_enhance_fn\n        \n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        print(f\"Using device: {device}\")\n        \n        # Load test audio\n        waveform, orig_sr = torchaudio.load(TEST_AUDIO)\n        waveform = waveform.mean(0)  # Mono\n        \n        # Resemble Enhance works at 44.1kHz\n        if orig_sr != 44100:\n            waveform = torchaudio.functional.resample(waveform, orig_sr, 44100)\n        \n        # Take first 10 seconds (heavy model, be patient)\n        waveform = waveform[:44100 * 10]\n        \n        print(f\"üìÑ Test audio: {waveform.shape[0]/44100:.2f}s at 44100Hz\")\n        \n        # Apply denoise only (faster than full enhance)\n        print(\"Applying Resemble Enhance (denoise only)...\")\n        enhanced, out_sr = denoise(waveform, 44100, device)\n        \n        # Resample back if needed\n        if out_sr != orig_sr:\n            enhanced = torchaudio.functional.resample(enhanced, out_sr, orig_sr)\n            waveform_out = torchaudio.functional.resample(waveform, 44100, orig_sr)\n        else:\n            waveform_out = waveform\n        \n        print(f\"\\nüìÑ Result:\")\n        print(f\"   Original peak: {waveform.abs().max().item():.4f}\")\n        print(f\"   Enhanced peak: {enhanced.abs().max().item():.4f}\")\n        \n        # Listen to comparison\n        import IPython.display as ipd\n        print(\"\\nüîä Original audio (first 10s):\")\n        ipd.display(ipd.Audio(waveform_out.cpu().numpy(), rate=orig_sr))\n        \n        print(\"üîä Resemble Enhance denoised:\")\n        ipd.display(ipd.Audio(enhanced.cpu().numpy(), rate=orig_sr))\n        \n        print(\"\\n‚úÖ Test 17 PASSED - Resemble Enhance works\")\n    except Exception as e:\n        print(f\"\\n‚ùå Test 17 FAILED: {e}\")\n        import traceback\n        traceback.print_exc()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Test 18: Real-World Noisy Audio Enhancement Comparison\n\nThis test uses the **NASA Apollo 11 moon landing audio** (1969) - a challenging real-world noisy recording.\nWe compare all denoising methods side-by-side so you can hear the before/after difference.\n\nSource: [NASA Apollo 11 Archive](https://history.nasa.gov/alsj/a11/video11.html#Step)\n\nTranscript: *\"I'm at the foot of the ladder... That's one small step for man, one giant leap for mankind.\"*",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Download NASA Apollo 11 Moon Landing Audio (1969) - Very noisy historical recording\nprint(\"=\" * 60)\nprint(\"Test 18: NASA Apollo 11 Audio - Enhancement Comparison\")\nprint(\"=\" * 60)\nprint(\"\"\"\nThis is Neil Armstrong's famous \"One small step for man\" recording from 1969.\nThe audio quality is poor (recorded on the Moon!) - perfect for testing denoising.\n\nTranscript: \"I'm at the foot of the ladder. The LM footpads are only depressed \nin the surface about 1 or 2 inches... That's one small step for man, \none giant leap for mankind.\"\n\"\"\")\n\nimport subprocess\nimport os\n\n# Download the NASA video and extract audio\nNOISY_AUDIO = \"apollo11_audio.wav\"\n\nif not os.path.exists(NOISY_AUDIO):\n    print(\"Downloading NASA Apollo 11 video...\")\n    !wget -q https://www.nasa.gov/wp-content/uploads/static/history/alsj/a11/a11.v1092338.mov -O apollo11.mov\n    print(\"Extracting audio...\")\n    !ffmpeg -loglevel warning -y -i apollo11.mov -vn -acodec pcm_s16le -ar 16000 -ac 1 {NOISY_AUDIO}\n    !rm apollo11.mov\n    print(f\"‚úÖ Audio extracted: {NOISY_AUDIO}\")\nelse:\n    print(f\"‚úÖ Using cached: {NOISY_AUDIO}\")\n\n!ls -lh {NOISY_AUDIO}",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Load the Apollo 11 audio and compare all enhancement methods\nimport IPython.display as ipd\n\nprint(\"Loading Apollo 11 audio...\")\napollo_waveform, apollo_sr = torchaudio.load(NOISY_AUDIO)\napollo_waveform = apollo_waveform.squeeze(0)  # Mono\nduration = apollo_waveform.shape[0] / apollo_sr\n\nprint(f\"üìÑ Apollo 11 Audio: {duration:.2f}s at {apollo_sr}Hz\")\nprint(f\"   This is VERY noisy 1969 audio from the Moon!\")\n\n# Store all enhanced versions for comparison\nenhanced_versions = {}\n\n# Original\nprint(\"\\n\" + \"=\"*60)\nprint(\"üîä ORIGINAL (Noisy Apollo 11 Recording)\")\nprint(\"=\"*60)\nipd.display(ipd.Audio(apollo_waveform.numpy(), rate=apollo_sr))\nenhanced_versions[\"original\"] = apollo_waveform\n\n# 1. noisereduce\nif NOISEREDUCE_AVAILABLE:\n    print(\"\\n\" + \"=\"*60)\n    print(\"üîä noisereduce (Spectral Gating)\")\n    print(\"=\"*60)\n    try:\n        import noisereduce as nr\n        nr_result = nr.reduce_noise(\n            y=apollo_waveform.numpy(),\n            sr=apollo_sr,\n            stationary=False,\n            prop_decrease=1.0,\n        )\n        enhanced_versions[\"noisereduce\"] = torch.from_numpy(nr_result).float()\n        ipd.display(ipd.Audio(nr_result, rate=apollo_sr))\n        print(\"‚úÖ noisereduce applied\")\n    except Exception as e:\n        print(f\"‚ùå noisereduce failed: {e}\")\nelse:\n    print(\"\\n‚ö†Ô∏è noisereduce not available\")\n\n# 2. DeepFilterNet\nif DEEPFILTERNET_AVAILABLE:\n    print(\"\\n\" + \"=\"*60)\n    print(\"üîä DeepFilterNet (Deep Learning 48kHz)\")\n    print(\"=\"*60)\n    try:\n        from df.enhance import enhance as df_enhance, init_df\n        \n        # Initialize if not already done\n        if 'df_model' not in dir():\n            df_model, df_state, _ = init_df()\n        \n        # Resample to 48kHz for DeepFilterNet\n        apollo_48k = torchaudio.functional.resample(apollo_waveform, apollo_sr, df_state.sr())\n        df_result = df_enhance(df_model, df_state, apollo_48k.numpy())\n        df_result_16k = torchaudio.functional.resample(\n            torch.from_numpy(df_result).float(), df_state.sr(), apollo_sr\n        )\n        enhanced_versions[\"deepfilternet\"] = df_result_16k\n        ipd.display(ipd.Audio(df_result_16k.numpy(), rate=apollo_sr))\n        print(\"‚úÖ DeepFilterNet applied\")\n    except Exception as e:\n        print(f\"‚ùå DeepFilterNet failed: {e}\")\nelse:\n    print(\"\\n‚ö†Ô∏è DeepFilterNet not available\")\n\n# 3. Resemble Enhance (if available)\nif RESEMBLE_ENHANCE_AVAILABLE:\n    print(\"\\n\" + \"=\"*60)\n    print(\"üîä Resemble Enhance (AI Speech Enhancement)\")\n    print(\"=\"*60)\n    try:\n        from resemble_enhance.enhancer.inference import denoise as resemble_denoise\n        \n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        \n        # Resample to 44.1kHz for Resemble\n        apollo_44k = torchaudio.functional.resample(apollo_waveform, apollo_sr, 44100)\n        resemble_result, out_sr = resemble_denoise(apollo_44k, 44100, device)\n        resemble_result_16k = torchaudio.functional.resample(resemble_result, out_sr, apollo_sr)\n        enhanced_versions[\"resemble\"] = resemble_result_16k.cpu()\n        ipd.display(ipd.Audio(resemble_result_16k.cpu().numpy(), rate=apollo_sr))\n        print(\"‚úÖ Resemble Enhance applied\")\n    except Exception as e:\n        print(f\"‚ùå Resemble Enhance failed: {e}\")\nelse:\n    print(\"\\n‚ö†Ô∏è Resemble Enhance not available\")\n\n# 4. Demucs (vocal extraction - different use case but interesting)\nif DEMUCS_AVAILABLE:\n    print(\"\\n\" + \"=\"*60)\n    print(\"üîä Demucs (Vocal Extraction)\")\n    print(\"=\"*60)\n    try:\n        from demucs.pretrained import get_model_from_args\n        from demucs.apply import apply_model\n        \n        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        \n        # Use model from Test 14 if available, otherwise load it\n        if 'demucs_model' not in dir():\n            demucs_model = get_model_from_args(\n                type(\"args\", (object,), dict(name=\"htdemucs\", repo=None))\n            ).to(device).eval()\n        \n        # Prepare for Demucs using torchaudio (demucs.audio API changed)\n        apollo_stereo = apollo_waveform.unsqueeze(0).repeat(2, 1)  # Make stereo [2, T]\n        \n        # Resample to model's sample rate\n        if apollo_sr != demucs_model.samplerate:\n            wav = torchaudio.functional.resample(apollo_stereo, apollo_sr, demucs_model.samplerate)\n        else:\n            wav = apollo_stereo\n        \n        # Add batch dimension [B, C, T]\n        if wav.dim() == 2:\n            wav = wav.unsqueeze(0)\n        wav = wav.to(device)\n        \n        with torch.no_grad():\n            sources = apply_model(demucs_model, wav, device=device, split=True, overlap=0.25)\n        \n        vocals_idx = demucs_model.sources.index(\"vocals\")\n        vocals = sources[0, vocals_idx].mean(0).cpu()\n        vocals = torchaudio.functional.resample(vocals, demucs_model.samplerate, apollo_sr)\n        enhanced_versions[\"demucs_vocals\"] = vocals\n        ipd.display(ipd.Audio(vocals.numpy(), rate=apollo_sr))\n        print(\"‚úÖ Demucs vocal extraction applied\")\n    except Exception as e:\n        print(f\"‚ùå Demucs failed: {e}\")\n        import traceback\n        traceback.print_exc()\nelse:\n    print(\"\\n‚ö†Ô∏è Demucs not available\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"COMPARISON COMPLETE!\")\nprint(\"=\"*60)\nprint(f\"\\nEnhanced versions available: {list(enhanced_versions.keys())}\")\nprint(\"\\nListen to each version above and compare the noise reduction quality!\")\nprint(\"The Apollo 11 audio is a challenging test case due to extreme noise.\")\nprint(\"\\n‚úÖ Test 18 PASSED - Enhancement comparison complete\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "print(\"=\" * 60)\nprint(\"TEST SUMMARY\")\nprint(\"=\" * 60)\nprint(\"\\nAudio Frontend module tests complete.\")\nprint(\"\\nCore Features (Tests 1-10):\")\nprint(\"  ‚úÖ Test 1-3: Load, resample, mono conversion\")\nprint(\"  ‚úÖ Test 4-5: Segmentation with overlap\")\nprint(\"  ‚úÖ Test 6-7: Batching and frame offsets\")\nprint(\"  ‚úÖ Test 8-9: Convenience functions, normalization\")\nprint(\"  ‚úÖ Test 10: Audio playback verification\")\nprint(\"\\nEnhancement Features (Tests 11-14, requires: pip install demucs pyloudnorm):\")\nprint(\"  ‚úÖ Test 11: TimeMappingManager (timestamp recovery)\")\nprint(\"  ‚úÖ Test 12: Silence removal (energy-based)\")\nprint(\"  ‚úÖ Test 13: Silero VAD (Voice Activity Detection)\")\nprint(\"  ‚úÖ Test 14: Demucs vocal extraction\")\nprint(\"\\nAdditional Denoising (Tests 15-17, install as needed):\")\nprint(\"  ‚úÖ Test 15: noisereduce (pip install noisereduce)\")\nprint(\"  ‚úÖ Test 16: DeepFilterNet (pip install deepfilternet)\")\nprint(\"  ‚úÖ Test 17: Resemble Enhance (pip install resemble-enhance)\")\nprint(\"\\nReal-World Comparison (Test 18):\")\nprint(\"  ‚úÖ Test 18: NASA Apollo 11 audio - all methods compared\")\nprint(\"\\n\" + \"=\" * 60)\nprint(\"\"\"\nAUDIO FRONTEND COMPLETE!\n\nFeatures:\n‚îú‚îÄ‚îÄ Loading: torchaudio, soundfile (fallback)\n‚îú‚îÄ‚îÄ Preprocessing: resample, mono, normalize\n‚îú‚îÄ‚îÄ Segmentation: overlap for divide-and-conquer alignment\n‚îú‚îÄ‚îÄ Batching: GPU-ready tensor batching\n‚îî‚îÄ‚îÄ Enhancement (optional)\n    ‚îú‚îÄ‚îÄ Demucs: Vocal extraction from music/noise\n    ‚îú‚îÄ‚îÄ Silero VAD: Voice Activity Detection\n    ‚îú‚îÄ‚îÄ noisereduce: Spectral gating (CPU-friendly)\n    ‚îú‚îÄ‚îÄ DeepFilterNet: Deep learning 48kHz denoising\n    ‚îú‚îÄ‚îÄ Resemble Enhance: AI speech enhancement (GPU)\n    ‚îî‚îÄ‚îÄ TimeMappingManager: Timestamp recovery\n\nUsage:\n    from audio_frontend import AudioEnhancement\n    \n    # Denoise with noisereduce (lightweight)\n    enhancer = AudioEnhancement()\n    result = enhancer.enhance(\"noisy.mp3\", denoise_method=\"noisereduce\")\n    \n    # Denoise with DeepFilterNet (48kHz quality)\n    result = enhancer.enhance(\"noisy.mp3\", denoise_method=\"deepfilternet\")\n    \n    # Denoise with Resemble Enhance (best quality, GPU)\n    result = enhancer.enhance(\"noisy.mp3\", denoise_method=\"resemble\")\n    \n    # Vocal extraction with Demucs\n    result = enhancer.enhance(\"audio_with_music.mp3\", extract_vocals=True)\n\"\"\")\nprint(\"=\" * 60)",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}