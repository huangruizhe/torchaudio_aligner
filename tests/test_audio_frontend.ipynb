{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Frontend Module Tests\n",
    "\n",
    "This notebook tests the `audio_frontend` module of the TorchAudio Long-Form Aligner.\n",
    "\n",
    "Each test cell will display:\n",
    "- ✅ if the test passes\n",
    "- ❌ if the test fails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check PyTorch and TorchAudio versions\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"TorchAudio: {torchaudio.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install torchcodec if using torchaudio >= 2.8\n",
    "import torchaudio\n",
    "version_parts = torchaudio.__version__.split('.')\n",
    "major, minor = int(version_parts[0]), int(version_parts[1].split('+')[0])\n",
    "if (major, minor) >= (2, 8):\n",
    "    print(\"TorchAudio >= 2.8 detected, installing torchcodec...\")\n",
    "    !pip install -q torchcodec\n",
    "else:\n",
    "    print(f\"TorchAudio {torchaudio.__version__} - torchcodec not required\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Audio Frontend Module\n",
    "\n",
    "Copy of `torchaudio_aligner/src/audio_frontend.py` for Colab testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Audio Frontend Module for TorchAudio Long-Form Aligner\n",
    "\"\"\"\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List, Callable, Tuple, Union\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AudioSegment:\n",
    "    \"\"\"Represents a segment of audio with metadata.\"\"\"\n",
    "    waveform: torch.Tensor\n",
    "    sample_rate: int\n",
    "    offset_samples: int\n",
    "    length_samples: int\n",
    "    segment_index: int\n",
    "\n",
    "    @property\n",
    "    def offset_seconds(self) -> float:\n",
    "        return self.offset_samples / self.sample_rate\n",
    "\n",
    "    @property\n",
    "    def duration_seconds(self) -> float:\n",
    "        return self.length_samples / self.sample_rate\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SegmentationResult:\n",
    "    \"\"\"Result of audio segmentation containing all segments and metadata.\"\"\"\n",
    "    segments: List[AudioSegment]\n",
    "    original_duration_samples: int\n",
    "    original_duration_seconds: float\n",
    "    sample_rate: int\n",
    "    segment_size_samples: int\n",
    "    overlap_samples: int\n",
    "    num_segments: int\n",
    "\n",
    "    def get_waveforms_batched(self) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        max_len = max(seg.waveform.shape[-1] for seg in self.segments)\n",
    "        batch_size = len(self.segments)\n",
    "\n",
    "        if self.segments[0].waveform.dim() == 1:\n",
    "            waveforms = torch.zeros(batch_size, max_len)\n",
    "        else:\n",
    "            num_channels = self.segments[0].waveform.shape[0]\n",
    "            waveforms = torch.zeros(batch_size, num_channels, max_len)\n",
    "\n",
    "        lengths = torch.zeros(batch_size, dtype=torch.long)\n",
    "\n",
    "        for i, seg in enumerate(self.segments):\n",
    "            length = seg.waveform.shape[-1]\n",
    "            if seg.waveform.dim() == 1:\n",
    "                waveforms[i, :length] = seg.waveform\n",
    "            else:\n",
    "                waveforms[i, :, :length] = seg.waveform\n",
    "            lengths[i] = length\n",
    "\n",
    "        return waveforms, lengths\n",
    "\n",
    "    def get_offsets_in_frames(self, frame_duration_seconds: float) -> torch.Tensor:\n",
    "        offsets = torch.tensor([seg.offset_samples for seg in self.segments])\n",
    "        return (offsets / self.sample_rate / frame_duration_seconds).long()\n",
    "\n",
    "\n",
    "class AudioFrontend:\n",
    "    def __init__(\n",
    "        self,\n",
    "        target_sample_rate: int = 16000,\n",
    "        mono: bool = True,\n",
    "        normalize: bool = False,\n",
    "        normalize_db: float = -3.0,\n",
    "        preprocessors: Optional[List[Callable[[torch.Tensor, int], torch.Tensor]]] = None,\n",
    "    ):\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.mono = mono\n",
    "        self.normalize = normalize\n",
    "        self.normalize_db = normalize_db\n",
    "        self.preprocessors = preprocessors or []\n",
    "\n",
    "    def load(self, audio_path: Union[str, Path]) -> Tuple[torch.Tensor, int]:\n",
    "        audio_path = Path(audio_path)\n",
    "        if not audio_path.exists():\n",
    "            raise FileNotFoundError(f\"Audio file not found: {audio_path}\")\n",
    "\n",
    "        logger.info(f\"Loading audio from: {audio_path}\")\n",
    "        waveform, sample_rate = torchaudio.load(str(audio_path))\n",
    "        logger.info(f\"Loaded: shape={waveform.shape}, sr={sample_rate}, duration={waveform.shape[1]/sample_rate:.2f}s\")\n",
    "        return waveform, sample_rate\n",
    "\n",
    "    def resample(self, waveform: torch.Tensor, orig_sample_rate: int, target_sample_rate: Optional[int] = None) -> torch.Tensor:\n",
    "        target_sr = target_sample_rate or self.target_sample_rate\n",
    "        if orig_sample_rate == target_sr:\n",
    "            return waveform\n",
    "        logger.info(f\"Resampling from {orig_sample_rate} Hz to {target_sr} Hz\")\n",
    "        return torchaudio.functional.resample(waveform, orig_sample_rate, target_sr)\n",
    "\n",
    "    def to_mono(self, waveform: torch.Tensor) -> torch.Tensor:\n",
    "        if waveform.shape[0] == 1:\n",
    "            return waveform\n",
    "        logger.info(f\"Converting {waveform.shape[0]} channels to mono\")\n",
    "        return waveform.mean(dim=0, keepdim=True)\n",
    "\n",
    "    def apply_normalization(self, waveform: torch.Tensor) -> torch.Tensor:\n",
    "        peak = waveform.abs().max()\n",
    "        if peak > 0:\n",
    "            target_peak = 10 ** (self.normalize_db / 20)\n",
    "            waveform = waveform * (target_peak / peak)\n",
    "            logger.info(f\"Normalized audio to {self.normalize_db} dB peak\")\n",
    "        return waveform\n",
    "\n",
    "    def preprocess(self, waveform: torch.Tensor, sample_rate: int) -> torch.Tensor:\n",
    "        if self.mono:\n",
    "            waveform = self.to_mono(waveform)\n",
    "        if self.normalize:\n",
    "            waveform = self.apply_normalization(waveform)\n",
    "        for preprocessor in self.preprocessors:\n",
    "            waveform = preprocessor(waveform, sample_rate)\n",
    "        return waveform\n",
    "\n",
    "    def segment(\n",
    "        self,\n",
    "        waveform: torch.Tensor,\n",
    "        sample_rate: int,\n",
    "        segment_size: float = 15.0,\n",
    "        overlap: float = 2.0,\n",
    "        min_segment_size: float = 0.2,\n",
    "        extra_samples: int = 128,\n",
    "    ) -> SegmentationResult:\n",
    "        if waveform.dim() == 1:\n",
    "            waveform = waveform.unsqueeze(0)\n",
    "\n",
    "        num_channels, total_samples = waveform.shape\n",
    "        segment_size_samples = int(sample_rate * segment_size) + extra_samples\n",
    "        overlap_samples = int(sample_rate * overlap) + extra_samples\n",
    "        min_segment_samples = int(sample_rate * min_segment_size)\n",
    "        step_size = segment_size_samples - overlap_samples\n",
    "\n",
    "        logger.info(f\"Segmenting: total={total_samples}, seg_size={segment_size_samples}, overlap={overlap_samples}, step={step_size}\")\n",
    "\n",
    "        segments = []\n",
    "        segment_idx = 0\n",
    "        offset = 0\n",
    "\n",
    "        while offset < total_samples:\n",
    "            end = min(offset + segment_size_samples, total_samples)\n",
    "            segment_length = end - offset\n",
    "\n",
    "            if segment_length < min_segment_samples:\n",
    "                break\n",
    "\n",
    "            segment_waveform = waveform[:, offset:end]\n",
    "            if num_channels == 1:\n",
    "                segment_waveform = segment_waveform.squeeze(0)\n",
    "\n",
    "            segment = AudioSegment(\n",
    "                waveform=segment_waveform,\n",
    "                sample_rate=sample_rate,\n",
    "                offset_samples=offset,\n",
    "                length_samples=segment_length,\n",
    "                segment_index=segment_idx,\n",
    "            )\n",
    "            segments.append(segment)\n",
    "            segment_idx += 1\n",
    "            offset += step_size\n",
    "\n",
    "            if end >= total_samples:\n",
    "                break\n",
    "\n",
    "        logger.info(f\"Created {len(segments)} segments\")\n",
    "\n",
    "        return SegmentationResult(\n",
    "            segments=segments,\n",
    "            original_duration_samples=total_samples,\n",
    "            original_duration_seconds=total_samples / sample_rate,\n",
    "            sample_rate=sample_rate,\n",
    "            segment_size_samples=segment_size_samples,\n",
    "            overlap_samples=overlap_samples,\n",
    "            num_segments=len(segments),\n",
    "        )\n",
    "\n",
    "    def process(\n",
    "        self,\n",
    "        audio_path: Union[str, Path],\n",
    "        segment_size: float = 15.0,\n",
    "        overlap: float = 2.0,\n",
    "        min_segment_size: float = 0.2,\n",
    "        extra_samples: int = 128,\n",
    "    ) -> SegmentationResult:\n",
    "        waveform, orig_sample_rate = self.load(audio_path)\n",
    "        waveform = self.resample(waveform, orig_sample_rate)\n",
    "        waveform = self.preprocess(waveform, self.target_sample_rate)\n",
    "        return self.segment(\n",
    "            waveform, self.target_sample_rate,\n",
    "            segment_size=segment_size, overlap=overlap,\n",
    "            min_segment_size=min_segment_size, extra_samples=extra_samples,\n",
    "        )\n",
    "\n",
    "    def process_waveform(\n",
    "        self,\n",
    "        waveform: torch.Tensor,\n",
    "        sample_rate: int,\n",
    "        segment_size: float = 15.0,\n",
    "        overlap: float = 2.0,\n",
    "        min_segment_size: float = 0.2,\n",
    "        extra_samples: int = 128,\n",
    "    ) -> SegmentationResult:\n",
    "        waveform = self.resample(waveform, sample_rate)\n",
    "        waveform = self.preprocess(waveform, self.target_sample_rate)\n",
    "        return self.segment(\n",
    "            waveform, self.target_sample_rate,\n",
    "            segment_size=segment_size, overlap=overlap,\n",
    "            min_segment_size=min_segment_size, extra_samples=extra_samples,\n",
    "        )\n",
    "\n",
    "\n",
    "def segment_audio(\n",
    "    audio_path: Union[str, Path],\n",
    "    target_sample_rate: int = 16000,\n",
    "    segment_size: float = 15.0,\n",
    "    overlap: float = 2.0,\n",
    "    mono: bool = True,\n",
    "    normalize: bool = False,\n",
    ") -> SegmentationResult:\n",
    "    frontend = AudioFrontend(\n",
    "        target_sample_rate=target_sample_rate,\n",
    "        mono=mono,\n",
    "        normalize=normalize,\n",
    "    )\n",
    "    return frontend.process(audio_path, segment_size=segment_size, overlap=overlap)\n",
    "\n",
    "\n",
    "print(\"✅ Audio Frontend module loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Test Audio\n",
    "\n",
    "We'll use Meta's Q1 2025 earnings call as test audio (same as in Tutorial.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download test audio (Meta Q1 2025 Earnings Call - ~1 hour)\n",
    "!wget -q https://static.seekingalpha.com/cdn/s3/transcripts_audio/4780182.mp3 -O test_audio.mp3\n",
    "!ls -lh test_audio.mp3\n",
    "\n",
    "TEST_AUDIO = \"test_audio.mp3\"\n",
    "print(\"✅ Test audio downloaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: Load Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Test 1: AudioFrontend.load()\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    frontend = AudioFrontend(target_sample_rate=16000)\n",
    "    waveform, sample_rate = frontend.load(TEST_AUDIO)\n",
    "\n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"  Waveform shape: {waveform.shape}\")\n",
    "    print(f\"  Sample rate: {sample_rate} Hz\")\n",
    "    print(f\"  Duration: {waveform.shape[1] / sample_rate:.2f} seconds\")\n",
    "    print(f\"  Duration: {waveform.shape[1] / sample_rate / 60:.2f} minutes\")\n",
    "\n",
    "    assert waveform.dim() == 2, \"Waveform should be 2D\"\n",
    "    assert sample_rate > 0, \"Sample rate should be positive\"\n",
    "    print(\"\\n✅ Test 1 PASSED\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Test 1 FAILED: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: Resample Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Test 2: AudioFrontend.resample()\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    print(f\"\\nOriginal sample rate: {sample_rate} Hz\")\n",
    "    print(f\"Original samples: {waveform.shape[1]}\")\n",
    "\n",
    "    resampled = frontend.resample(waveform, sample_rate, 16000)\n",
    "\n",
    "    expected_samples = int(waveform.shape[1] * 16000 / sample_rate)\n",
    "    print(f\"Resampled samples: {resampled.shape[1]}\")\n",
    "    print(f\"Expected samples (approx): {expected_samples}\")\n",
    "\n",
    "    assert abs(resampled.shape[1] - expected_samples) < 100, \"Resampled length mismatch\"\n",
    "    print(\"\\n✅ Test 2 PASSED\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Test 2 FAILED: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3: Convert to Mono"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Test 3: AudioFrontend.to_mono()\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    print(f\"\\nOriginal channels: {waveform.shape[0]}\")\n",
    "\n",
    "    mono = frontend.to_mono(waveform)\n",
    "\n",
    "    print(f\"Mono channels: {mono.shape[0]}\")\n",
    "    assert mono.shape[0] == 1, \"Should have 1 channel\"\n",
    "    print(\"\\n✅ Test 3 PASSED\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Test 3 FAILED: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 4: Segment Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Test 4: AudioFrontend.segment()\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    frontend = AudioFrontend(target_sample_rate=16000, mono=True)\n",
    "    waveform, orig_sr = frontend.load(TEST_AUDIO)\n",
    "    waveform = frontend.resample(waveform, orig_sr)\n",
    "    waveform = frontend.to_mono(waveform)\n",
    "\n",
    "    result = frontend.segment(\n",
    "        waveform,\n",
    "        sample_rate=16000,\n",
    "        segment_size=15.0,\n",
    "        overlap=2.0,\n",
    "        min_segment_size=0.2,\n",
    "    )\n",
    "\n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"  Original duration: {result.original_duration_seconds:.2f} seconds ({result.original_duration_seconds/60:.2f} min)\")\n",
    "    print(f\"  Number of segments: {result.num_segments}\")\n",
    "    print(f\"  Segment size: {result.segment_size_samples} samples ({result.segment_size_samples/16000:.2f}s)\")\n",
    "    print(f\"  Overlap: {result.overlap_samples} samples ({result.overlap_samples/16000:.2f}s)\")\n",
    "\n",
    "    print(f\"\\nFirst 3 segments:\")\n",
    "    for i, seg in enumerate(result.segments[:3]):\n",
    "        print(f\"  Segment {i}: offset={seg.offset_seconds:.2f}s, duration={seg.duration_seconds:.2f}s, shape={seg.waveform.shape}\")\n",
    "\n",
    "    print(f\"\\nLast segment:\")\n",
    "    last_seg = result.segments[-1]\n",
    "    print(f\"  Segment {last_seg.segment_index}: offset={last_seg.offset_seconds:.2f}s, duration={last_seg.duration_seconds:.2f}s\")\n",
    "\n",
    "    assert result.num_segments > 0, \"Should have at least one segment\"\n",
    "    assert all(seg.sample_rate == 16000 for seg in result.segments), \"All segments should have correct sample rate\"\n",
    "    print(\"\\n✅ Test 4 PASSED\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Test 4 FAILED: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 5: Full Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Test 5: AudioFrontend.process() - Full Pipeline\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    frontend = AudioFrontend(\n",
    "        target_sample_rate=16000,\n",
    "        mono=True,\n",
    "        normalize=False,\n",
    "    )\n",
    "\n",
    "    result = frontend.process(\n",
    "        TEST_AUDIO,\n",
    "        segment_size=15.0,\n",
    "        overlap=2.0,\n",
    "    )\n",
    "\n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"  Original duration: {result.original_duration_seconds:.2f} seconds\")\n",
    "    print(f\"  Number of segments: {result.num_segments}\")\n",
    "\n",
    "    assert isinstance(result, SegmentationResult)\n",
    "    assert result.num_segments > 0\n",
    "    print(\"\\n✅ Test 5 PASSED\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Test 5 FAILED: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 6: Batching for GPU Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Test 6: SegmentationResult.get_waveforms_batched()\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    waveforms, lengths = result.get_waveforms_batched()\n",
    "\n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"  Batched waveforms shape: {waveforms.shape}\")\n",
    "    print(f\"  Lengths shape: {lengths.shape}\")\n",
    "    print(f\"  First 5 lengths: {lengths[:5].tolist()}\")\n",
    "    print(f\"  Last 5 lengths: {lengths[-5:].tolist()}\")\n",
    "\n",
    "    assert waveforms.shape[0] == result.num_segments, \"Batch size mismatch\"\n",
    "    assert lengths.shape[0] == result.num_segments, \"Lengths mismatch\"\n",
    "    assert waveforms.dim() == 2, \"Should be 2D for mono\"\n",
    "    print(\"\\n✅ Test 6 PASSED\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Test 6 FAILED: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 7: Frame Offset Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Test 7: SegmentationResult.get_offsets_in_frames()\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    # MMS model has 20ms frame duration\n",
    "    frame_duration = 0.02\n",
    "    offsets = result.get_offsets_in_frames(frame_duration)\n",
    "\n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"  Frame duration: {frame_duration}s (20ms)\")\n",
    "    print(f\"  Frame offsets shape: {offsets.shape}\")\n",
    "    print(f\"  First 5 offsets (frames): {offsets[:5].tolist()}\")\n",
    "\n",
    "    # Verify monotonically increasing\n",
    "    is_monotonic = all(offsets[i] < offsets[i+1] for i in range(len(offsets)-1))\n",
    "    print(f\"  Monotonically increasing: {is_monotonic}\")\n",
    "\n",
    "    assert is_monotonic, \"Offsets should be monotonically increasing\"\n",
    "    print(\"\\n✅ Test 7 PASSED\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Test 7 FAILED: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 8: Convenience Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Test 8: segment_audio() convenience function\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    result = segment_audio(\n",
    "        TEST_AUDIO,\n",
    "        target_sample_rate=16000,\n",
    "        segment_size=15.0,\n",
    "        overlap=2.0,\n",
    "    )\n",
    "\n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"  Duration: {result.original_duration_seconds:.2f}s\")\n",
    "    print(f\"  Segments: {result.num_segments}\")\n",
    "\n",
    "    assert isinstance(result, SegmentationResult)\n",
    "    print(\"\\n✅ Test 8 PASSED\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Test 8 FAILED: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 9: Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Test 9: Audio Normalization\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    frontend_norm = AudioFrontend(target_sample_rate=16000, mono=True, normalize=True, normalize_db=-3.0)\n",
    "\n",
    "    waveform, sr = frontend_norm.load(TEST_AUDIO)\n",
    "    waveform = frontend_norm.resample(waveform, sr)\n",
    "    waveform = frontend_norm.to_mono(waveform)\n",
    "\n",
    "    original_peak = waveform.abs().max().item()\n",
    "    print(f\"\\nOriginal peak: {original_peak:.4f}\")\n",
    "\n",
    "    normalized = frontend_norm.apply_normalization(waveform.clone())\n",
    "    normalized_peak = normalized.abs().max().item()\n",
    "    print(f\"Normalized peak: {normalized_peak:.4f}\")\n",
    "\n",
    "    expected_peak = 10 ** (-3.0 / 20)  # -3 dB\n",
    "    print(f\"Expected peak (-3dB): {expected_peak:.4f}\")\n",
    "\n",
    "    assert abs(normalized_peak - expected_peak) < 0.01, \"Normalized peak mismatch\"\n",
    "    print(\"\\n✅ Test 9 PASSED\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Test 9 FAILED: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 10: Listen to a Segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Test 10: Listen to a Segment (Visual/Audio Check)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    import IPython.display as ipd\n",
    "\n",
    "    result = segment_audio(TEST_AUDIO, segment_size=15.0, overlap=2.0)\n",
    "\n",
    "    # Play first segment\n",
    "    seg = result.segments[0]\n",
    "    print(f\"\\nPlaying Segment 0:\")\n",
    "    print(f\"  Offset: {seg.offset_seconds:.2f}s\")\n",
    "    print(f\"  Duration: {seg.duration_seconds:.2f}s\")\n",
    "    ipd.display(ipd.Audio(seg.waveform.numpy(), rate=seg.sample_rate))\n",
    "\n",
    "    # Play a middle segment\n",
    "    mid_idx = result.num_segments // 2\n",
    "    seg = result.segments[mid_idx]\n",
    "    print(f\"\\nPlaying Segment {mid_idx} (middle):\")\n",
    "    print(f\"  Offset: {seg.offset_seconds:.2f}s\")\n",
    "    print(f\"  Duration: {seg.duration_seconds:.2f}s\")\n",
    "    ipd.display(ipd.Audio(seg.waveform.numpy(), rate=seg.sample_rate))\n",
    "    \n",
    "    print(\"\\n✅ Test 10 PASSED (verify audio plays correctly)\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Test 10 FAILED: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\" * 60)\nprint(\"TEST SUMMARY\")\nprint(\"=\" * 60)\nprint(\"\\nAudio Frontend module tests complete.\")\nprint(\"\\nKey features verified:\")\nprint(\"  ✅ Load audio from various formats\")\nprint(\"  ✅ Resample to target sample rate\")\nprint(\"  ✅ Convert to mono\")\nprint(\"  ✅ Uniform segmentation with overlap\")\nprint(\"  ✅ Batch waveforms for GPU inference\")\nprint(\"  ✅ Calculate frame offsets for acoustic models\")\nprint(\"  ✅ Audio normalization\")\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Now you can clear all outputs and save the file.\")\nprint(\"=\" * 60)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}