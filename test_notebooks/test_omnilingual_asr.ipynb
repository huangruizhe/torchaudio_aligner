{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Notebook: OmniLingual ASR (OmniASR) Backend\n",
    "\n",
    "This notebook tests and debugs the OmniASR backend for torchaudio_aligner.\n",
    "\n",
    "**Goals:**\n",
    "1. Understand OmniASR's internal structure (model, tokenizer, vocab)\n",
    "2. Properly extract vocabulary from the tokenizer\n",
    "3. Properly extract CTC emissions/posteriors\n",
    "4. Test different approaches to get emissions\n",
    "\n",
    "**Reference:**\n",
    "- https://github.com/facebookresearch/omnilingual-asr\n",
    "- Models built on fairseq2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "# WARNING: omnilingual-asr may have conflicting dependencies\n",
    "!pip install -q omnilingual-asr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Explore OmniASR Pipeline Structure\n",
    "\n",
    "First, let's understand what's inside the ASRInferencePipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading omniASR_CTC_300M...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.21G/1.21G [00:29<00:00, 44.9MB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2928e95208454673a4ec593b759dfad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline loaded!\n"
     ]
    }
   ],
   "source": [
    "from omnilingual_asr.models.inference.pipeline import ASRInferencePipeline\n",
    "\n",
    "# Load a smaller model for testing\n",
    "MODEL_NAME = \"omniASR_CTC_300M\"  # Smallest CTC model\n",
    "# MODEL_NAME = \"omniASR_CTC_1B\"  # 1B parameter model\n",
    "# MODEL_NAME = \"omniASR_CTC_1B_v2\"  # v2 uses different tokenizer\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "pipeline = ASRInferencePipeline(model_card=MODEL_NAME, device=device)\n",
    "print(\"Pipeline loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Pipeline Attributes:\n",
      "============================================================\n",
      "  beam_search_generator: NoneType\n",
      "  device: device\n",
      "  dtype: dtype\n",
      "  tokenizer: RawSentencePieceTokenizer\n",
      "\n",
      "============================================================\n",
      "Pipeline Private Attributes:\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Inspect pipeline attributes\n",
    "print(\"=\" * 60)\n",
    "print(\"Pipeline Attributes:\")\n",
    "print(\"=\" * 60)\n",
    "for attr in dir(pipeline):\n",
    "    if not attr.startswith('_'):\n",
    "        val = getattr(pipeline, attr, None)\n",
    "        if not callable(val):\n",
    "            print(f\"  {attr}: {type(val).__name__}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Pipeline Private Attributes:\")\n",
    "print(\"=\" * 60)\n",
    "for attr in dir(pipeline):\n",
    "    if attr.startswith('_') and not attr.startswith('__'):\n",
    "        val = getattr(pipeline, attr, None)\n",
    "        if not callable(val):\n",
    "            print(f\"  {attr}: {type(val).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Accessing Model:\n",
      "============================================================\n",
      "Found model via 'model' attribute\n",
      "\n",
      "Model type: Wav2Vec2AsrModel\n",
      "Model class: fairseq2.models.wav2vec2.asr.model.Wav2Vec2AsrModel\n"
     ]
    }
   ],
   "source": [
    "# Try to access the model\n",
    "print(\"=\" * 60)\n",
    "print(\"Accessing Model:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "model = None\n",
    "if hasattr(pipeline, 'model'):\n",
    "    model = pipeline.model\n",
    "    print(f\"Found model via 'model' attribute\")\n",
    "elif hasattr(pipeline, '_model'):\n",
    "    model = pipeline._model\n",
    "    print(f\"Found model via '_model' attribute\")\n",
    "else:\n",
    "    # Search for torch.nn.Module in attributes\n",
    "    for attr in dir(pipeline):\n",
    "        val = getattr(pipeline, attr, None)\n",
    "        if isinstance(val, torch.nn.Module):\n",
    "            model = val\n",
    "            print(f\"Found model via '{attr}' attribute\")\n",
    "            break\n",
    "\n",
    "if model is not None:\n",
    "    print(f\"\\nModel type: {type(model).__name__}\")\n",
    "    print(f\"Model class: {model.__class__.__module__}.{model.__class__.__name__}\")\n",
    "else:\n",
    "    print(\"Could not find model!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Model Attributes (non-callable):\n",
      "============================================================\n",
      "  T_destination: TypeVar\n",
      "  call_super_init: bool\n",
      "  dump_patches: bool\n",
      "  final_dropout: NoneType\n",
      "  masker: NoneType\n",
      "  model_dim: int\n",
      "  training: bool\n",
      "\n",
      "============================================================\n",
      "Model Sub-modules:\n",
      "============================================================\n",
      "  encoder_frontend: Wav2Vec2Frontend\n",
      "  encoder: StandardTransformerEncoder\n",
      "  final_proj: Linear\n"
     ]
    }
   ],
   "source": [
    "# Inspect model structure\n",
    "if model is not None:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Model Attributes (non-callable):\")\n",
    "    print(\"=\" * 60)\n",
    "    for attr in dir(model):\n",
    "        if not attr.startswith('_'):\n",
    "            val = getattr(model, attr, None)\n",
    "            if not callable(val) and not isinstance(val, torch.nn.Module):\n",
    "                print(f\"  {attr}: {type(val).__name__}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Model Sub-modules:\")\n",
    "    print(\"=\" * 60)\n",
    "    for name, module in model.named_children():\n",
    "        print(f\"  {name}: {type(module).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "All Named Modules (looking for CTC/projection layers):\n",
      "============================================================\n",
      "  encoder_frontend.model_dim_proj: Linear\n",
      "  encoder.layers.0.self_attn.q_proj: Linear\n",
      "  encoder.layers.0.self_attn.k_proj: Linear\n",
      "  encoder.layers.0.self_attn.v_proj: Linear\n",
      "  encoder.layers.0.self_attn.output_proj: Linear\n",
      "  encoder.layers.0.ffn.inner_proj: Linear\n",
      "  encoder.layers.0.ffn.output_proj: Linear\n",
      "  encoder.layers.1.self_attn.q_proj: Linear\n",
      "  encoder.layers.1.self_attn.k_proj: Linear\n",
      "  encoder.layers.1.self_attn.v_proj: Linear\n",
      "  encoder.layers.1.self_attn.output_proj: Linear\n",
      "  encoder.layers.1.ffn.inner_proj: Linear\n",
      "  encoder.layers.1.ffn.output_proj: Linear\n",
      "  encoder.layers.2.self_attn.q_proj: Linear\n",
      "  encoder.layers.2.self_attn.k_proj: Linear\n",
      "  encoder.layers.2.self_attn.v_proj: Linear\n",
      "  encoder.layers.2.self_attn.output_proj: Linear\n",
      "  encoder.layers.2.ffn.inner_proj: Linear\n",
      "  encoder.layers.2.ffn.output_proj: Linear\n",
      "  encoder.layers.3.self_attn.q_proj: Linear\n",
      "  encoder.layers.3.self_attn.k_proj: Linear\n",
      "  encoder.layers.3.self_attn.v_proj: Linear\n",
      "  encoder.layers.3.self_attn.output_proj: Linear\n",
      "  encoder.layers.3.ffn.inner_proj: Linear\n",
      "  encoder.layers.3.ffn.output_proj: Linear\n",
      "  encoder.layers.4.self_attn.q_proj: Linear\n",
      "  encoder.layers.4.self_attn.k_proj: Linear\n",
      "  encoder.layers.4.self_attn.v_proj: Linear\n",
      "  encoder.layers.4.self_attn.output_proj: Linear\n",
      "  encoder.layers.4.ffn.inner_proj: Linear\n",
      "  encoder.layers.4.ffn.output_proj: Linear\n",
      "  encoder.layers.5.self_attn.q_proj: Linear\n",
      "  encoder.layers.5.self_attn.k_proj: Linear\n",
      "  encoder.layers.5.self_attn.v_proj: Linear\n",
      "  encoder.layers.5.self_attn.output_proj: Linear\n",
      "  encoder.layers.5.ffn.inner_proj: Linear\n",
      "  encoder.layers.5.ffn.output_proj: Linear\n",
      "  encoder.layers.6.self_attn.q_proj: Linear\n",
      "  encoder.layers.6.self_attn.k_proj: Linear\n",
      "  encoder.layers.6.self_attn.v_proj: Linear\n",
      "  encoder.layers.6.self_attn.output_proj: Linear\n",
      "  encoder.layers.6.ffn.inner_proj: Linear\n",
      "  encoder.layers.6.ffn.output_proj: Linear\n",
      "  encoder.layers.7.self_attn.q_proj: Linear\n",
      "  encoder.layers.7.self_attn.k_proj: Linear\n",
      "  encoder.layers.7.self_attn.v_proj: Linear\n",
      "  encoder.layers.7.self_attn.output_proj: Linear\n",
      "  encoder.layers.7.ffn.inner_proj: Linear\n",
      "  encoder.layers.7.ffn.output_proj: Linear\n",
      "  encoder.layers.8.self_attn.q_proj: Linear\n",
      "  encoder.layers.8.self_attn.k_proj: Linear\n",
      "  encoder.layers.8.self_attn.v_proj: Linear\n",
      "  encoder.layers.8.self_attn.output_proj: Linear\n",
      "  encoder.layers.8.ffn.inner_proj: Linear\n",
      "  encoder.layers.8.ffn.output_proj: Linear\n",
      "  encoder.layers.9.self_attn.q_proj: Linear\n",
      "  encoder.layers.9.self_attn.k_proj: Linear\n",
      "  encoder.layers.9.self_attn.v_proj: Linear\n",
      "  encoder.layers.9.self_attn.output_proj: Linear\n",
      "  encoder.layers.9.ffn.inner_proj: Linear\n",
      "  encoder.layers.9.ffn.output_proj: Linear\n",
      "  encoder.layers.10.self_attn.q_proj: Linear\n",
      "  encoder.layers.10.self_attn.k_proj: Linear\n",
      "  encoder.layers.10.self_attn.v_proj: Linear\n",
      "  encoder.layers.10.self_attn.output_proj: Linear\n",
      "  encoder.layers.10.ffn.inner_proj: Linear\n",
      "  encoder.layers.10.ffn.output_proj: Linear\n",
      "  encoder.layers.11.self_attn.q_proj: Linear\n",
      "  encoder.layers.11.self_attn.k_proj: Linear\n",
      "  encoder.layers.11.self_attn.v_proj: Linear\n",
      "  encoder.layers.11.self_attn.output_proj: Linear\n",
      "  encoder.layers.11.ffn.inner_proj: Linear\n",
      "  encoder.layers.11.ffn.output_proj: Linear\n",
      "  encoder.layers.12.self_attn.q_proj: Linear\n",
      "  encoder.layers.12.self_attn.k_proj: Linear\n",
      "  encoder.layers.12.self_attn.v_proj: Linear\n",
      "  encoder.layers.12.self_attn.output_proj: Linear\n",
      "  encoder.layers.12.ffn.inner_proj: Linear\n",
      "  encoder.layers.12.ffn.output_proj: Linear\n",
      "  encoder.layers.13.self_attn.q_proj: Linear\n",
      "  encoder.layers.13.self_attn.k_proj: Linear\n",
      "  encoder.layers.13.self_attn.v_proj: Linear\n",
      "  encoder.layers.13.self_attn.output_proj: Linear\n",
      "  encoder.layers.13.ffn.inner_proj: Linear\n",
      "  encoder.layers.13.ffn.output_proj: Linear\n",
      "  encoder.layers.14.self_attn.q_proj: Linear\n",
      "  encoder.layers.14.self_attn.k_proj: Linear\n",
      "  encoder.layers.14.self_attn.v_proj: Linear\n",
      "  encoder.layers.14.self_attn.output_proj: Linear\n",
      "  encoder.layers.14.ffn.inner_proj: Linear\n",
      "  encoder.layers.14.ffn.output_proj: Linear\n",
      "  encoder.layers.15.self_attn.q_proj: Linear\n",
      "  encoder.layers.15.self_attn.k_proj: Linear\n",
      "  encoder.layers.15.self_attn.v_proj: Linear\n",
      "  encoder.layers.15.self_attn.output_proj: Linear\n",
      "  encoder.layers.15.ffn.inner_proj: Linear\n",
      "  encoder.layers.15.ffn.output_proj: Linear\n",
      "  encoder.layers.16.self_attn.q_proj: Linear\n",
      "  encoder.layers.16.self_attn.k_proj: Linear\n",
      "  encoder.layers.16.self_attn.v_proj: Linear\n",
      "  encoder.layers.16.self_attn.output_proj: Linear\n",
      "  encoder.layers.16.ffn.inner_proj: Linear\n",
      "  encoder.layers.16.ffn.output_proj: Linear\n",
      "  encoder.layers.17.self_attn.q_proj: Linear\n",
      "  encoder.layers.17.self_attn.k_proj: Linear\n",
      "  encoder.layers.17.self_attn.v_proj: Linear\n",
      "  encoder.layers.17.self_attn.output_proj: Linear\n",
      "  encoder.layers.17.ffn.inner_proj: Linear\n",
      "  encoder.layers.17.ffn.output_proj: Linear\n",
      "  encoder.layers.18.self_attn.q_proj: Linear\n",
      "  encoder.layers.18.self_attn.k_proj: Linear\n",
      "  encoder.layers.18.self_attn.v_proj: Linear\n",
      "  encoder.layers.18.self_attn.output_proj: Linear\n",
      "  encoder.layers.18.ffn.inner_proj: Linear\n",
      "  encoder.layers.18.ffn.output_proj: Linear\n",
      "  encoder.layers.19.self_attn.q_proj: Linear\n",
      "  encoder.layers.19.self_attn.k_proj: Linear\n",
      "  encoder.layers.19.self_attn.v_proj: Linear\n",
      "  encoder.layers.19.self_attn.output_proj: Linear\n",
      "  encoder.layers.19.ffn.inner_proj: Linear\n",
      "  encoder.layers.19.ffn.output_proj: Linear\n",
      "  encoder.layers.20.self_attn.q_proj: Linear\n",
      "  encoder.layers.20.self_attn.k_proj: Linear\n",
      "  encoder.layers.20.self_attn.v_proj: Linear\n",
      "  encoder.layers.20.self_attn.output_proj: Linear\n",
      "  encoder.layers.20.ffn.inner_proj: Linear\n",
      "  encoder.layers.20.ffn.output_proj: Linear\n",
      "  encoder.layers.21.self_attn.q_proj: Linear\n",
      "  encoder.layers.21.self_attn.k_proj: Linear\n",
      "  encoder.layers.21.self_attn.v_proj: Linear\n",
      "  encoder.layers.21.self_attn.output_proj: Linear\n",
      "  encoder.layers.21.ffn.inner_proj: Linear\n",
      "  encoder.layers.21.ffn.output_proj: Linear\n",
      "  encoder.layers.22.self_attn.q_proj: Linear\n",
      "  encoder.layers.22.self_attn.k_proj: Linear\n",
      "  encoder.layers.22.self_attn.v_proj: Linear\n",
      "  encoder.layers.22.self_attn.output_proj: Linear\n",
      "  encoder.layers.22.ffn.inner_proj: Linear\n",
      "  encoder.layers.22.ffn.output_proj: Linear\n",
      "  encoder.layers.23.self_attn.q_proj: Linear\n",
      "  encoder.layers.23.self_attn.k_proj: Linear\n",
      "  encoder.layers.23.self_attn.v_proj: Linear\n",
      "  encoder.layers.23.self_attn.output_proj: Linear\n",
      "  encoder.layers.23.ffn.inner_proj: Linear\n",
      "  encoder.layers.23.ffn.output_proj: Linear\n",
      "  final_proj: Linear\n"
     ]
    }
   ],
   "source": [
    "# Deep inspection of model modules\n",
    "if model is not None:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"All Named Modules (looking for CTC/projection layers):\")\n",
    "    print(\"=\" * 60)\n",
    "    for name, module in model.named_modules():\n",
    "        if any(s in name.lower() for s in ['ctc', 'proj', 'output', 'lm_head', 'linear', 'final']):\n",
    "            print(f\"  {name}: {type(module).__name__}\")\n",
    "            if isinstance(module, torch.nn.Linear):\n",
    "                print(f\"       in_features={module.in_features}, out_features={module.out_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Explore Tokenizer and Vocabulary\n",
    "\n",
    "The tokenizer determines the output vocabulary for CTC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Accessing Tokenizer:\n",
      "============================================================\n",
      "Found tokenizer via pipeline.tokenizer\n",
      "\n",
      "Tokenizer type: RawSentencePieceTokenizer\n",
      "Tokenizer class: fairseq2.data.tokenizers.sentencepiece.RawSentencePieceTokenizer\n"
     ]
    }
   ],
   "source": [
    "# Try to access tokenizer from pipeline\n",
    "print(\"=\" * 60)\n",
    "print(\"Accessing Tokenizer:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "tokenizer = None\n",
    "tokenizer_source = None\n",
    "\n",
    "# Check pipeline attributes\n",
    "for attr in ['tokenizer', '_tokenizer', 'text_tokenizer', '_text_tokenizer']:\n",
    "    if hasattr(pipeline, attr):\n",
    "        tokenizer = getattr(pipeline, attr)\n",
    "        tokenizer_source = f\"pipeline.{attr}\"\n",
    "        print(f\"Found tokenizer via {tokenizer_source}\")\n",
    "        break\n",
    "\n",
    "# Check model attributes\n",
    "if tokenizer is None and model is not None:\n",
    "    for attr in ['tokenizer', '_tokenizer', 'decoder']:\n",
    "        if hasattr(model, attr):\n",
    "            val = getattr(model, attr)\n",
    "            if hasattr(val, 'tokenizer'):\n",
    "                tokenizer = val.tokenizer\n",
    "                tokenizer_source = f\"model.{attr}.tokenizer\"\n",
    "            else:\n",
    "                tokenizer = val\n",
    "                tokenizer_source = f\"model.{attr}\"\n",
    "            print(f\"Found tokenizer via {tokenizer_source}\")\n",
    "            break\n",
    "\n",
    "if tokenizer is not None:\n",
    "    print(f\"\\nTokenizer type: {type(tokenizer).__name__}\")\n",
    "    print(f\"Tokenizer class: {tokenizer.__class__.__module__}.{tokenizer.__class__.__name__}\")\n",
    "else:\n",
    "    print(\"Could not find tokenizer directly!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Tokenizer Attributes:\n",
      "============================================================\n",
      "  vocab_info: VocabularyInfo = VocabularyInfo(size=9812, unk_idx=3, bos_idx=0, eos_idx=2, pad_idx=1, boh_idx=None, eoh_idx=None)\n",
      "\n",
      "Tokenizer Methods:\n",
      "  create_decoder()\n",
      "  create_encoder()\n",
      "  create_raw_encoder()\n"
     ]
    }
   ],
   "source": [
    "# Inspect tokenizer attributes\n",
    "if tokenizer is not None:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Tokenizer Attributes:\")\n",
    "    print(\"=\" * 60)\n",
    "    for attr in dir(tokenizer):\n",
    "        if not attr.startswith('_'):\n",
    "            val = getattr(tokenizer, attr, None)\n",
    "            if not callable(val):\n",
    "                try:\n",
    "                    print(f\"  {attr}: {type(val).__name__} = {str(val)[:100]}\")\n",
    "                except:\n",
    "                    print(f\"  {attr}: {type(val).__name__}\")\n",
    "    \n",
    "    print(\"\\nTokenizer Methods:\")\n",
    "    for attr in dir(tokenizer):\n",
    "        if not attr.startswith('_'):\n",
    "            val = getattr(tokenizer, attr, None)\n",
    "            if callable(val):\n",
    "                print(f\"  {attr}()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Extracting Vocabulary:\n",
      "============================================================\n",
      "Method 4 failed: 'VocabularyInfo' object is not callable\n"
     ]
    }
   ],
   "source": [
    "# Try to get vocabulary from tokenizer\n",
    "print(\"=\" * 60)\n",
    "print(\"Extracting Vocabulary:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "vocab = None\n",
    "vocab_size = None\n",
    "\n",
    "if tokenizer is not None:\n",
    "    # Method 1: get_vocab()\n",
    "    if hasattr(tokenizer, 'get_vocab'):\n",
    "        try:\n",
    "            vocab = tokenizer.get_vocab()\n",
    "            print(f\"Method 1 (get_vocab): vocab size = {len(vocab)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Method 1 failed: {e}\")\n",
    "    \n",
    "    # Method 2: vocab attribute\n",
    "    if hasattr(tokenizer, 'vocab'):\n",
    "        try:\n",
    "            v = tokenizer.vocab\n",
    "            print(f\"Method 2 (vocab attr): type = {type(v).__name__}\")\n",
    "            if isinstance(v, dict):\n",
    "                vocab = v\n",
    "                print(f\"  vocab size = {len(vocab)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Method 2 failed: {e}\")\n",
    "    \n",
    "    # Method 3: vocab_size attribute\n",
    "    if hasattr(tokenizer, 'vocab_size'):\n",
    "        try:\n",
    "            vocab_size = tokenizer.vocab_size\n",
    "            print(f\"Method 3 (vocab_size): {vocab_size}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Method 3 failed: {e}\")\n",
    "    \n",
    "    # Method 4: vocab_info()\n",
    "    if hasattr(tokenizer, 'vocab_info'):\n",
    "        try:\n",
    "            vi = tokenizer.vocab_info()\n",
    "            print(f\"Method 4 (vocab_info): {vi}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Method 4 failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Loading Tokenizer via Fairseq2 Asset Store:\n",
      "============================================================\n",
      "fairseq2.assets not available: cannot import name 'asset_store' from 'fairseq2.assets' (/usr/local/lib/python3.12/dist-packages/fairseq2/assets/__init__.py)\n"
     ]
    }
   ],
   "source": [
    "# Alternative: Load tokenizer via fairseq2 asset store\n",
    "print(\"=\" * 60)\n",
    "print(\"Loading Tokenizer via Fairseq2 Asset Store:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    from fairseq2.assets import asset_store, download_manager\n",
    "    \n",
    "    # List available cards\n",
    "    print(\"\\nSearching for tokenizer cards...\")\n",
    "    \n",
    "    # Try to find OmniASR tokenizer\n",
    "    tokenizer_names = [\n",
    "        \"omniASR_tokenizer_written_v2\",\n",
    "        \"omniASR_tokenizer_v1\",\n",
    "        \"omniASR_tokenizer\",\n",
    "    ]\n",
    "    \n",
    "    for name in tokenizer_names:\n",
    "        try:\n",
    "            card = asset_store.retrieve_card(name)\n",
    "            print(f\"\\nFound card: {name}\")\n",
    "            print(f\"  Card: {card}\")\n",
    "            \n",
    "            # Try to download\n",
    "            if hasattr(card, 'uri'):\n",
    "                path = download_manager.download_tokenizer(card.uri)\n",
    "                print(f\"  Downloaded to: {path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  {name}: {e}\")\n",
    "            \n",
    "except ImportError as e:\n",
    "    print(f\"fairseq2.assets not available: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Getting Vocab Size from Model Output Layer:\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Alternative: Get vocab from model output dimension\n",
    "print(\"=\" * 60)\n",
    "print(\"Getting Vocab Size from Model Output Layer:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if model is not None:\n",
    "    # Find the final linear layer (CTC projection)\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            if any(s in name.lower() for s in ['ctc', 'proj', 'output', 'final', 'lm']):\n",
    "                print(f\"  {name}: in={module.in_features}, out={module.out_features}\")\n",
    "                if vocab_size is None:\n",
    "                    vocab_size = module.out_features\n",
    "                    print(f\"  -> Using {vocab_size} as vocab size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Test Emission Extraction\n",
    "\n",
    "Now let's test different approaches to get CTC emissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Creating Test Input:\n",
      "============================================================\n",
      "Waveform shape: torch.Size([1, 80000])\n",
      "Lengths: tensor([80000], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Create test waveform\n",
    "print(\"=\" * 60)\n",
    "print(\"Creating Test Input:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Generate random audio (10 seconds at 16kHz)\n",
    "batch_size = 1\n",
    "duration_sec = 5\n",
    "sample_rate = 16000\n",
    "waveform = torch.randn(batch_size, duration_sec * sample_rate).to(device)\n",
    "lengths = torch.tensor([waveform.shape[1]] * batch_size).to(device)\n",
    "\n",
    "print(f\"Waveform shape: {waveform.shape}\")\n",
    "print(f\"Lengths: {lengths}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Option 1: Direct Model Forward\n",
      "============================================================\n",
      "Error: 'Tensor' object has no attribute 'packed'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipython-input-1564891814.py\", line 11, in <cell line: 0>\n",
      "    out = model(waveform, lengths)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/fairseq2/models/wav2vec2/asr/model.py\", line 125, in forward\n",
      "    seqs, seqs_layout, _ = self.encoder_frontend.extract_features(seqs, seqs_layout)\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/fairseq2/models/wav2vec2/frontend.py\", line 162, in extract_features\n",
      "    seqs, seqs_layout = self.feature_extractor(seqs, seqs_layout)\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/fairseq2/models/wav2vec2/feature_extractor.py\", line 139, in forward\n",
      "    if seqs_layout.packed:\n",
      "       ^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'Tensor' object has no attribute 'packed'\n"
     ]
    }
   ],
   "source": [
    "# Option 1: Direct model forward\n",
    "print(\"=\" * 60)\n",
    "print(\"Option 1: Direct Model Forward\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if model is not None:\n",
    "    model.eval()\n",
    "    \n",
    "    try:\n",
    "        with torch.inference_mode():\n",
    "            out = model(waveform, lengths)\n",
    "        \n",
    "        print(f\"Output type: {type(out).__name__}\")\n",
    "        \n",
    "        # Try to extract logits\n",
    "        if isinstance(out, torch.Tensor):\n",
    "            logits = out\n",
    "            print(f\"Output is tensor: {logits.shape}\")\n",
    "        elif isinstance(out, dict):\n",
    "            print(f\"Output keys: {out.keys()}\")\n",
    "            for key in ['logits', 'ctc_logits', 'emissions', 'encoder_out']:\n",
    "                if key in out:\n",
    "                    logits = out[key]\n",
    "                    print(f\"Found '{key}': {logits.shape if isinstance(logits, torch.Tensor) else type(logits)}\")\n",
    "                    break\n",
    "        else:\n",
    "            print(f\"Output attributes: {[a for a in dir(out) if not a.startswith('_')]}\")\n",
    "            for attr in ['logits', 'ctc_logits', 'emissions', 'output']:\n",
    "                if hasattr(out, attr):\n",
    "                    val = getattr(out, attr)\n",
    "                    print(f\"Found '{attr}': {val.shape if isinstance(val, torch.Tensor) else type(val)}\")\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Option 2: Encoder + CTC Projection\n",
      "============================================================\n",
      "Found encoder: model.encoder\n",
      "Found CTC projection: model.final_proj\n",
      "Error: StandardTransformerEncoder.forward() missing 1 required positional argument: 'seqs_layout'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipython-input-3988646838.py\", line 29, in <cell line: 0>\n",
      "    encoder_out = encoder(waveform)\n",
      "                  ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: StandardTransformerEncoder.forward() missing 1 required positional argument: 'seqs_layout'\n"
     ]
    }
   ],
   "source": [
    "# Option 2: Encoder + CTC projection\n",
    "print(\"=\" * 60)\n",
    "print(\"Option 2: Encoder + CTC Projection\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if model is not None:\n",
    "    model.eval()\n",
    "    \n",
    "    # Find encoder\n",
    "    encoder = None\n",
    "    for attr in ['encoder', 'wav2vec2', 'feature_extractor']:\n",
    "        if hasattr(model, attr):\n",
    "            encoder = getattr(model, attr)\n",
    "            print(f\"Found encoder: model.{attr}\")\n",
    "            break\n",
    "    \n",
    "    # Find CTC projection\n",
    "    ctc_proj = None\n",
    "    for attr in ['ctc_proj', 'output_projection', 'ctc', 'ctc_decoder', 'final_proj']:\n",
    "        if hasattr(model, attr):\n",
    "            ctc_proj = getattr(model, attr)\n",
    "            print(f\"Found CTC projection: model.{attr}\")\n",
    "            break\n",
    "    \n",
    "    if encoder is not None and ctc_proj is not None:\n",
    "        try:\n",
    "            with torch.inference_mode():\n",
    "                # Encode\n",
    "                encoder_out = encoder(waveform)\n",
    "                print(f\"Encoder output type: {type(encoder_out).__name__}\")\n",
    "                \n",
    "                # Get features\n",
    "                if isinstance(encoder_out, torch.Tensor):\n",
    "                    features = encoder_out\n",
    "                elif hasattr(encoder_out, 'output'):\n",
    "                    features = encoder_out.output\n",
    "                elif hasattr(encoder_out, 'last_hidden_state'):\n",
    "                    features = encoder_out.last_hidden_state\n",
    "                else:\n",
    "                    features = encoder_out\n",
    "                    \n",
    "                print(f\"Features shape: {features.shape if isinstance(features, torch.Tensor) else type(features)}\")\n",
    "                \n",
    "                # Apply CTC projection\n",
    "                logits = ctc_proj(features)\n",
    "                print(f\"Logits shape: {logits.shape}\")\n",
    "                \n",
    "                # Log softmax\n",
    "                emissions = F.log_softmax(logits.float(), dim=-1)\n",
    "                print(f\"Emissions shape: {emissions.shape}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    else:\n",
    "        print(\"Could not find encoder and/or CTC projection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Option 3: Hook-based Extraction\n",
      "============================================================\n",
      "Could not find a projection layer to hook\n"
     ]
    }
   ],
   "source": [
    "# Option 3: Hook into the model\n",
    "print(\"=\" * 60)\n",
    "print(\"Option 3: Hook-based Extraction\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if model is not None:\n",
    "    model.eval()\n",
    "    \n",
    "    captured = {}\n",
    "    \n",
    "    def hook_fn(module, inp, out):\n",
    "        captured[\"logits\"] = out\n",
    "    \n",
    "    # Find a likely projection layer\n",
    "    proj = None\n",
    "    proj_name = None\n",
    "    for name, m in model.named_modules():\n",
    "        if isinstance(m, torch.nn.Linear):\n",
    "            if any(s in name.lower() for s in [\"ctc\", \"proj\", \"output\", \"lm_head\", \"final\"]):\n",
    "                proj = m\n",
    "                proj_name = name\n",
    "                break\n",
    "    \n",
    "    if proj is not None:\n",
    "        print(f\"Hooking: {proj_name}\")\n",
    "        h = proj.register_forward_hook(hook_fn)\n",
    "        \n",
    "        try:\n",
    "            with torch.inference_mode():\n",
    "                _ = model(waveform, lengths)\n",
    "            \n",
    "            h.remove()\n",
    "            \n",
    "            if \"logits\" in captured:\n",
    "                logits = captured[\"logits\"]\n",
    "                print(f\"Captured logits shape: {logits.shape}\")\n",
    "                \n",
    "                emissions = F.log_softmax(logits.float(), dim=-1)\n",
    "                print(f\"Emissions shape: {emissions.shape}\")\n",
    "            else:\n",
    "                print(\"Hook did not capture logits\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            h.remove()\n",
    "            print(f\"Error: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    else:\n",
    "        print(\"Could not find a projection layer to hook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Option 4: extract_features() Method\n",
      "============================================================\n",
      "Model does not have extract_features() method\n"
     ]
    }
   ],
   "source": [
    "# Option 4: Use extract_features if available\n",
    "print(\"=\" * 60)\n",
    "print(\"Option 4: extract_features() Method\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if model is not None:\n",
    "    model.eval()\n",
    "    \n",
    "    if hasattr(model, 'extract_features'):\n",
    "        try:\n",
    "            with torch.inference_mode():\n",
    "                encoder_out, padding_mask = model.extract_features(waveform, padding_mask=None)\n",
    "            \n",
    "            print(f\"Encoder output shape: {encoder_out.shape}\")\n",
    "            if padding_mask is not None:\n",
    "                print(f\"Padding mask shape: {padding_mask.shape}\")\n",
    "            \n",
    "            # Find CTC projection\n",
    "            ctc_proj = None\n",
    "            for attr in ['ctc_proj', 'output_projection', 'proj', 'final_proj']:\n",
    "                if hasattr(model, attr):\n",
    "                    ctc_proj = getattr(model, attr)\n",
    "                    print(f\"Found CTC projection: model.{attr}\")\n",
    "                    break\n",
    "            \n",
    "            if ctc_proj is not None:\n",
    "                logits = ctc_proj(encoder_out)\n",
    "                print(f\"Logits shape: {logits.shape}\")\n",
    "                \n",
    "                emissions = F.log_softmax(logits.float(), dim=-1)\n",
    "                print(f\"Emissions shape: {emissions.shape}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    else:\n",
    "        print(\"Model does not have extract_features() method\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Option 5: Full Forward with Output Parsing\n",
      "============================================================\n",
      "Could not call model forward\n"
     ]
    }
   ],
   "source": [
    "# Option 5: Full forward with output parsing\n",
    "print(\"=\" * 60)\n",
    "print(\"Option 5: Full Forward with Output Parsing\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if model is not None:\n",
    "    model.eval()\n",
    "    \n",
    "    try:\n",
    "        with torch.inference_mode():\n",
    "            # Try different forward signatures\n",
    "            out = None\n",
    "            \n",
    "            # Try 1: (source, padding_mask)\n",
    "            try:\n",
    "                out = model(source=waveform, padding_mask=None)\n",
    "                print(\"Forward signature: (source=, padding_mask=)\")\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Try 2: (waveform, lengths)\n",
    "            if out is None:\n",
    "                try:\n",
    "                    out = model(waveform, lengths)\n",
    "                    print(\"Forward signature: (waveform, lengths)\")\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            # Try 3: just waveform\n",
    "            if out is None:\n",
    "                try:\n",
    "                    out = model(waveform)\n",
    "                    print(\"Forward signature: (waveform)\")\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            if out is not None:\n",
    "                print(f\"\\nOutput type: {type(out).__name__}\")\n",
    "                \n",
    "                # Parse output\n",
    "                if isinstance(out, torch.Tensor):\n",
    "                    print(f\"Direct tensor output: {out.shape}\")\n",
    "                    logits = out\n",
    "                elif isinstance(out, dict):\n",
    "                    print(f\"Dict output keys: {out.keys()}\")\n",
    "                    for k, v in out.items():\n",
    "                        if isinstance(v, torch.Tensor):\n",
    "                            print(f\"  {k}: {v.shape}\")\n",
    "                        elif isinstance(v, dict):\n",
    "                            print(f\"  {k}: dict with keys {v.keys()}\")\n",
    "                else:\n",
    "                    # Named tuple or custom object\n",
    "                    print(f\"Object attributes:\")\n",
    "                    for attr in dir(out):\n",
    "                        if not attr.startswith('_'):\n",
    "                            val = getattr(out, attr, None)\n",
    "                            if isinstance(val, torch.Tensor):\n",
    "                                print(f\"  {attr}: {val.shape}\")\n",
    "                            elif not callable(val):\n",
    "                                print(f\"  {attr}: {type(val).__name__}\")\n",
    "            else:\n",
    "                print(\"Could not call model forward\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Test Pipeline Audio Loading\n",
    "\n",
    "Check if the pipeline has built-in audio loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Pipeline Audio Loading Methods:\n",
      "============================================================\n",
      "  _build_audio_wavform_pipeline: method (callable)\n",
      "  _process_context_audio: method (callable)\n",
      "  audio_decoder: AudioDecoder (callable)\n",
      "  collater_audio: Collater (callable)\n"
     ]
    }
   ],
   "source": [
    "# Check pipeline for audio loading methods\n",
    "print(\"=\" * 60)\n",
    "print(\"Pipeline Audio Loading Methods:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for attr in dir(pipeline):\n",
    "    if 'audio' in attr.lower() or 'load' in attr.lower() or 'wave' in attr.lower():\n",
    "        val = getattr(pipeline, attr, None)\n",
    "        print(f\"  {attr}: {type(val).__name__} {'(callable)' if callable(val) else ''}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Test Transcription (to verify model works):\n",
      "============================================================\n",
      "Error: 'ASRInferencePipeline' object is not callable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:337: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.save_with_torchcodec` under the hood. Some parameters like format, encoding, bits_per_sample, buffer_size, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's encoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.encoders.AudioEncoder\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:247: UserWarning: torio.io._streaming_media_encoder.StreamingMediaEncoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
      "  s = torchaudio.io.StreamWriter(uri, format=muxer, buffer_size=buffer_size)\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipython-input-3959037900.py\", line 22, in <cell line: 0>\n",
      "    result = pipeline(temp_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: 'ASRInferencePipeline' object is not callable\n"
     ]
    }
   ],
   "source": [
    "# Test transcription to verify model works\n",
    "print(\"=\" * 60)\n",
    "print(\"Test Transcription (to verify model works):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Generate some test audio or use a real file\n",
    "try:\n",
    "    # If you have a test audio file, use it:\n",
    "    # result = pipeline(\"/path/to/audio.wav\")\n",
    "    \n",
    "    # Or test with generated audio (will produce gibberish)\n",
    "    import torchaudio\n",
    "    import tempfile\n",
    "    \n",
    "    # Create a simple test audio\n",
    "    test_waveform = torch.randn(1, 16000 * 3)  # 3 seconds\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as f:\n",
    "        torchaudio.save(f.name, test_waveform, 16000)\n",
    "        temp_path = f.name\n",
    "    \n",
    "    # Try to transcribe\n",
    "    result = pipeline(temp_path)\n",
    "    print(f\"Transcription result: {result}\")\n",
    "    print(f\"Result type: {type(result).__name__}\")\n",
    "    \n",
    "    import os\n",
    "    os.unlink(temp_path)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Summary and Recommendations\n",
    "\n",
    "Based on the exploration above, summarize what we learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SUMMARY\n",
      "============================================================\n",
      "\n",
      "1. Model Name: omniASR_CTC_300M\n",
      "2. Model Type: Wav2Vec2AsrModel\n",
      "3. Tokenizer Type: RawSentencePieceTokenizer\n",
      "4. Vocab Size: Unknown\n",
      "\n",
      "============================================================\n",
      "RECOMMENDATIONS FOR BACKEND IMPLEMENTATION:\n",
      "============================================================\n",
      "\n",
      "Based on the exploration, update omniasr_backend.py to:\n",
      "\n",
      "1. TOKENIZER/VOCAB:\n",
      "   - [Fill in based on what worked above]\n",
      "\n",
      "2. EMISSION EXTRACTION:\n",
      "   - [Fill in based on what worked above]\n",
      "\n",
      "3. AUDIO LOADING:\n",
      "   - [Fill in based on what worked above]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n1. Model Name: {MODEL_NAME}\")\n",
    "print(f\"2. Model Type: {type(model).__name__ if model else 'Not found'}\")\n",
    "print(f\"3. Tokenizer Type: {type(tokenizer).__name__ if tokenizer else 'Not found'}\")\n",
    "print(f\"4. Vocab Size: {vocab_size if vocab_size else 'Unknown'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RECOMMENDATIONS FOR BACKEND IMPLEMENTATION:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\"\"\n",
    "Based on the exploration, update omniasr_backend.py to:\n",
    "\n",
    "1. TOKENIZER/VOCAB:\n",
    "   - [Fill in based on what worked above]\n",
    "\n",
    "2. EMISSION EXTRACTION:\n",
    "   - [Fill in based on what worked above]\n",
    "\n",
    "3. AUDIO LOADING:\n",
    "   - [Fill in based on what worked above]\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Vocabulary Info ===\n",
      "vocab_info type: <class 'fairseq2.data.tokenizers.vocab_info.VocabularyInfo'>\n",
      "vocab_info: VocabularyInfo(size=9812, unk_idx=3, bos_idx=0, eos_idx=2, pad_idx=1, boh_idx=None, eoh_idx=None)\n",
      "size: 9812\n",
      "unk_idx: 3\n",
      "bos_idx: 0\n",
      "eos_idx: 2\n",
      "pad_idx: 1\n",
      "\n",
      "Encoder type: <class 'fairseq2.data.tokenizers.sentencepiece.SentencePieceEncoder'>\n",
      "Encoder attrs: ['encode_as_tokens', 'prefix_indices', 'suffix_indices']\n",
      "\n",
      "Decoder type: <class 'fairseq2.data.tokenizers.sentencepiece.SentencePieceDecoder'>\n",
      "Decoder attrs: ['decode_from_tokens']\n",
      "ID 0: error - The input tensor must be one dimensional, but has 2 dimension(s) instead.\n",
      "ID 1: error - The input tensor must be one dimensional, but has 2 dimension(s) instead.\n",
      "ID 2: error - The input tensor must be one dimensional, but has 2 dimension(s) instead.\n",
      "ID 3: error - The input tensor must be one dimensional, but has 2 dimension(s) instead.\n",
      "ID 4: error - The input tensor must be one dimensional, but has 2 dimension(s) instead.\n",
      "ID 5: error - The input tensor must be one dimensional, but has 2 dimension(s) instead.\n",
      "ID 10: error - The input tensor must be one dimensional, but has 2 dimension(s) instead.\n",
      "ID 100: error - The input tensor must be one dimensional, but has 2 dimension(s) instead.\n",
      "ID 1000: error - The input tensor must be one dimensional, but has 2 dimension(s) instead.\n"
     ]
    }
   ],
   "source": [
    "# Get Vocabulary from Tokenizer\n",
    "\n",
    "# The tokenizer has vocab_info attribute (not a method!)\n",
    "print(\"=== Vocabulary Info ===\")\n",
    "vocab_info = pipeline.tokenizer.vocab_info\n",
    "print(f\"vocab_info type: {type(vocab_info)}\")\n",
    "print(f\"vocab_info: {vocab_info}\")\n",
    "print(f\"size: {vocab_info.size}\")\n",
    "print(f\"unk_idx: {vocab_info.unk_idx}\")\n",
    "print(f\"bos_idx: {vocab_info.bos_idx}\")\n",
    "print(f\"eos_idx: {vocab_info.eos_idx}\")\n",
    "print(f\"pad_idx: {vocab_info.pad_idx}\")\n",
    "\n",
    "# Try to get actual tokens via encoder\n",
    "encoder = pipeline.tokenizer.create_encoder()\n",
    "print(f\"\\nEncoder type: {type(encoder)}\")\n",
    "print(f\"Encoder attrs: {[a for a in dir(encoder) if not a.startswith('_')]}\")\n",
    "\n",
    "# Try to decode some token IDs to see what they are\n",
    "decoder = pipeline.tokenizer.create_decoder()\n",
    "print(f\"\\nDecoder type: {type(decoder)}\")\n",
    "print(f\"Decoder attrs: {[a for a in dir(decoder) if not a.startswith('_')]}\")\n",
    "\n",
    "# Test decode some IDs\n",
    "for i in [0, 1, 2, 3, 4, 5, 10, 100, 1000]:\n",
    "    try:\n",
    "        # Different ways to decode\n",
    "        if hasattr(decoder, '__call__'):\n",
    "            text = decoder(torch.tensor([[i]]))\n",
    "            print(f\"ID {i}: {text}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ID {i}: error - {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Pipeline _apply_model ===\n",
      "_apply_model_wav2vec2asr signature: (batch: 'Seq2SeqBatch') -> 'List[str]'\n",
      "_build_audio_wavform_pipeline signature: (inp_list: 'AudioInput') -> 'DataPipelineBuilder'\n",
      "=== Using Pipeline Collaters ===\n",
      "audio_decoder type: <class 'fairseq2n.bindings.data.audio.AudioDecoder'>\n",
      "audio_decoder attrs: []\n",
      "\n",
      "collater_audio type: <class 'fairseq2n.bindings.data.data_pipeline.Collater'>\n",
      "\n",
      "file_mapper type: <class 'fairseq2n.bindings.data.data_pipeline.FileMapper'>\n"
     ]
    }
   ],
   "source": [
    "#### Use Pipeline's Internal Methods\n",
    "\n",
    "# The pipeline has _apply_model methods - let's check those\n",
    "print(\"=== Pipeline _apply_model ===\")\n",
    "import inspect\n",
    "\n",
    "# Check _apply_model_wav2vec2asr signature\n",
    "sig = inspect.signature(pipeline._apply_model_wav2vec2asr)\n",
    "print(f\"_apply_model_wav2vec2asr signature: {sig}\")\n",
    "\n",
    "# Check _build_audio_wavform_pipeline\n",
    "sig = inspect.signature(pipeline._build_audio_wavform_pipeline)\n",
    "print(f\"_build_audio_wavform_pipeline signature: {sig}\")\n",
    "\n",
    "#### Use the collater to prepare proper input\n",
    "\n",
    "# The pipeline has collaters that prepare data in the right format\n",
    "print(\"=== Using Pipeline Collaters ===\")\n",
    "\n",
    "# First load audio properly\n",
    "import torchaudio\n",
    "import tempfile\n",
    "\n",
    "# Create test audio file\n",
    "test_waveform = torch.randn(1, 16000 * 3)  # 3 seconds\n",
    "with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as f:\n",
    "    torchaudio.save(f.name, test_waveform, 16000)\n",
    "    temp_path = f.name\n",
    "\n",
    "# Check audio_decoder\n",
    "print(f\"audio_decoder type: {type(pipeline.audio_decoder)}\")\n",
    "print(f\"audio_decoder attrs: {[a for a in dir(pipeline.audio_decoder) if not a.startswith('_')]}\")\n",
    "\n",
    "# Check collater_audio\n",
    "print(f\"\\ncollater_audio type: {type(pipeline.collater_audio)}\")\n",
    "\n",
    "# Try file_mapper\n",
    "print(f\"\\nfile_mapper type: {type(pipeline.file_mapper)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== transcribe method source ===\n",
      "    @torch.inference_mode()\n",
      "    def transcribe(\n",
      "        self,\n",
      "        inp: AudioInput,\n",
      "        *,\n",
      "        lang: List[str | None] | List[str] | List[None] | None = None,\n",
      "        batch_size: int = 2,\n",
      "    ) -> List[str]:\n",
      "        \"\"\"\n",
      "        Transcribes `AudioInput` into text by preprocessing (decoding, resample to 16kHz, converting to mono, normalizing)\n",
      "        each input sample and performing inference with `self.model`.\n",
      "\n",
      "        Works for both CTC and LLM model variants by optionally allowing a language conditioning token to help with LLM generation.\n",
      "        It is ignored when performing inference with CTC. See `omnilingual_asr/models/wav2vec2_llama/lang_ids.py` for supported languages.\n",
      "\n",
      "        Args:\n",
      "            `inp`: Audio input in different forms.\n",
      "                - `List[ Path | str ]`: Audio file paths\n",
      "                - `List[ bytes ]`: Raw audio data\n",
      "                - `List[ np.ndarray ]`: Audio data as uint8 numpy array\n",
      "                - `List[ dict[str, Any] ]`: Pre-decoded audio with 'waveform' and 'sample_rate' keys\n",
      "            `lang`: Language code for the input audios (e.g., 'eng_Latn', ...) (default: None)\n",
      "                - List [ str | None ]`: Any combination of missing and available language ids.\n",
      "            `batch_size`: Number of audio samples to process in each batch.\n",
      "\n",
      "        Returns:\n",
      "            `List[str]`: Transcribed texts.\n",
      "        \"\"\"\n",
      "        if len(inp) == 0:\n",
      "            return []\n",
      "\n",
      "        # fmt: off\n",
      "        is_ctc_model = isinstance(self.model, Wav2Vec2AsrModel)\n",
      "        is_llm_model = isinstance(self.model, Wav2Vec2LlamaModel)\n",
      "        is_llm_zs_model = is_llm_model and self.model.model_type == ModelType.ZERO_SHOT\n",
      "\n",
      "        if is_ctc_model and lang:\n",
      "            log.info(f\"Found {lang=} with a CTC model. Ignoring.\")\n",
      "        if is_llm_model and not lang:\n",
      "            log.info(\"Using an LLM model without a `lang` code can lead to degraded trascription quality.\")\n",
      "        if is_llm_zs_model:\n",
      "            raise NotImplementedError(\"omniASR_LLM_7B_ZS \n"
     ]
    }
   ],
   "source": [
    "#### Trace through transcribe to understand flow\n",
    "\n",
    "# Look at transcribe source to understand input format\n",
    "import inspect\n",
    "print(\"=== transcribe method source ===\")\n",
    "try:\n",
    "    source = inspect.getsource(pipeline.transcribe)\n",
    "    print(source[:2000])  # First 2000 chars\n",
    "except:\n",
    "    print(\"Could not get source\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Understanding SequenceLayout ===\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'SequenceBatch' from 'fairseq2.data' (/usr/local/lib/python3.12/dist-packages/fairseq2/data/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-888758606.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Check what SequenceLayout looks like\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=== Understanding SequenceLayout ===\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfairseq2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequenceBatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Check if we can create proper input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'SequenceBatch' from 'fairseq2.data' (/usr/local/lib/python3.12/dist-packages/fairseq2/data/__init__.py)",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#### Get SequenceLayout from fairseq2\n",
    "\n",
    "# Check what SequenceLayout looks like\n",
    "print(\"=== Understanding SequenceLayout ===\")\n",
    "from fairseq2.data import SequenceBatch\n",
    "\n",
    "# Check if we can create proper input\n",
    "print(f\"SequenceBatch attrs: {[a for a in dir(SequenceBatch) if not a.startswith('_')]}\")\n",
    "\n",
    "# Try to find how pipeline creates batches\n",
    "print(f\"\\n_create_batch_simple signature: {inspect.signature(pipeline._create_batch_simple)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Create proper batch ===\n",
      "Error: cannot import name 'SequenceBatch' from 'fairseq2.data' (/usr/local/lib/python3.12/dist-packages/fairseq2/data/__init__.py)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipython-input-4276800488.py\", line 7, in <cell line: 0>\n",
      "    from fairseq2.data import SequenceBatch\n",
      "ImportError: cannot import name 'SequenceBatch' from 'fairseq2.data' (/usr/local/lib/python3.12/dist-packages/fairseq2/data/__init__.py)\n"
     ]
    }
   ],
   "source": [
    "#### Direct approach - intercept at the right level\n",
    "\n",
    "# The cleanest approach: use fairseq2's batch utilities\n",
    "print(\"=== Create proper batch ===\")\n",
    "\n",
    "try:\n",
    "    from fairseq2.data import SequenceBatch\n",
    "    from fairseq2.nn.padding import PaddingMask\n",
    "\n",
    "    # Load audio the same way pipeline does\n",
    "    # Check what audio_decoder returns\n",
    "    audio_data = pipeline.audio_decoder(temp_path)\n",
    "    print(f\"audio_decoder output type: {type(audio_data)}\")\n",
    "    print(f\"audio_decoder output: {audio_data}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Hook during transcription ===\n",
      "Hook fired! Input shape: torch.Size([1, 169, 1024])\n",
      "Output shape: torch.Size([1, 169, 9812])\n",
      "\n",
      "Transcription: ['i had that curiosity beside me at this moment']\n",
      "\n",
      "Captured logits shape: torch.Size([1, 169, 9812])\n",
      "Emissions shape: torch.Size([1, 169, 9812])\n",
      "Vocab size: 9812\n",
      "Tokenizer vocab size: 9812\n"
     ]
    }
   ],
   "source": [
    "#### Hook into final_proj during actual transcription\n",
    "\n",
    "# Hook during actual transcription call\n",
    "print(\"=== Hook during transcription ===\")\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "audio_path = \"/content/torchaudio_aligner/examples/Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav\"\n",
    "\n",
    "captured = {}\n",
    "\n",
    "def hook_fn(module, inp, out):\n",
    "    captured[\"input\"] = inp\n",
    "    captured[\"output\"] = out\n",
    "    print(f\"Hook fired! Input shape: {inp[0].shape if isinstance(inp[0], torch.Tensor) else type(inp[0])}\")\n",
    "    print(f\"Output shape: {out.shape}\")\n",
    "\n",
    "# Register hook on final_proj\n",
    "h = pipeline.model.final_proj.register_forward_hook(hook_fn)\n",
    "\n",
    "try:\n",
    "    # Run transcription - NOTE: pass as a LIST\n",
    "    result = pipeline.transcribe([audio_path])\n",
    "    print(f\"\\nTranscription: {result}\")\n",
    "finally:\n",
    "    h.remove()\n",
    "\n",
    "# Now we have the logits!\n",
    "if \"output\" in captured:\n",
    "    logits = captured[\"output\"]\n",
    "    print(f\"\\nCaptured logits shape: {logits.shape}\")\n",
    "    emissions = F.log_softmax(logits.float(), dim=-1)\n",
    "    print(f\"Emissions shape: {emissions.shape}\")\n",
    "    print(f\"Vocab size: {emissions.shape[-1]}\")\n",
    "\n",
    "    # Verify it matches tokenizer vocab\n",
    "    print(f\"Tokenizer vocab size: {pipeline.tokenizer.vocab_info.size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Extracting Vocabulary Tokens ===\n",
      "Vocab size: 9812\n",
      "Special tokens: unk=3, bos=0, eos=2, pad=1\n",
      "\n",
      "Decoder type: <class 'fairseq2.data.tokenizers.sentencepiece.SentencePieceDecoder'>\n",
      "\n",
      "=== Decoding sample token IDs ===\n",
      "  ID 0: ERROR - The input tensor must be one dimensional, but has 2 dimension(s) instead.\n",
      "  ID 1: ERROR - The input tensor must be one dimensional, but has 2 dimension(s) instead.\n",
      "  ID 2: ERROR - The input tensor must be one dimensional, but has 2 dimension(s) instead.\n",
      "  ID 3: ERROR - The input tensor must be one dimensional, but has 2 dimension(s) instead.\n",
      "  ID 4: ERROR - The input tensor must be one dimensional, but has 2 dimension(s) instead.\n",
      "  ID 5: ERROR - The input tensor must be one dimensional, but has 2 dimension(s) instead.\n",
      "  ID 6: ERROR - The input tensor must be one dimensional, but has 2 dimension(s) instead.\n",
      "  ID 7: ERROR - The input tensor must be one dimensional, but has 2 dimension(s) instead.\n",
      "  ID 8: ERROR - The input tensor must be one dimensional, but has 2 dimension(s) instead.\n",
      "  ID 9: ERROR - The input tensor must be one dimensional, but has 2 dimension(s) instead.\n",
      "  ID 10: ERROR - The input tensor must be one dimensional, but has 2 dimension(s) instead.\n",
      "  ID 11: ERROR - The input tensor must be one dimensional, but has 2 dimension(s) instead.\n",
      "  ID 12: ERROR - The input tensor must be one dimensional, but has 2 dimension(s) instead.\n",
      "  ID 13: ERROR - The input tensor must be one dimensional, but has 2 dimension(s) instead.\n",
      "  ID 14: ERROR - The input tensor must be one dimensional, but has 2 dimension(s) instead.\n",
      "  ID 15: ERROR - The input tensor must be one dimensional, but has 2 dimension(s) instead.\n",
      "  ID 16: ERROR - The input tensor must be one dimensional, but has 2 dimension(s) instead.\n",
      "  ID 17: ERROR - The input tensor must be one dimensional, but has 2 dimension(s) instead.\n",
      "  ID 18: ERROR - The input tensor must be one dimensional, but has 2 dimension(s) instead.\n",
      "  ID 19: ERROR - The input tensor must be one dimensional, but has 2 dimension(s) instead.\n",
      "\n",
      "Total labels extracted: 9812\n",
      "Sample labels [0:10]: ['<0>', '<1>', '<2>', '<3>', '<4>', '<5>', '<6>', '<7>', '<8>', '<9>']\n",
      "Sample labels [100:110]: ['<100>', '<101>', '<102>', '<103>', '<104>', '<105>', '<106>', '<107>', '<108>', '<109>']\n",
      "=== Testing Encoder ===\n",
      "Encoder type: <class 'fairseq2.data.tokenizers.sentencepiece.SentencePieceEncoder'>\n",
      "Encoder attrs: ['encode_as_tokens', 'prefix_indices', 'suffix_indices']\n",
      "'hello' -> tensor([ 113, 9346, 1875, 1875, 8749])\n",
      "'i had that curiosity' -> tensor([4328,    4,  113, 9499, 1133,    4, 2226,  113, 9499, 2226,    4, 5943,\n",
      "        4685, 6712, 4328, 8749, 7076, 4328, 2226, 8131])\n",
      "'test' -> tensor([2226, 9346, 7076, 2226])\n",
      "=== Raw SentencePiece Access ===\n",
      "Tokenizer attrs: ['create_decoder', 'create_encoder', 'create_raw_encoder', 'vocab_info']\n",
      "=== Underlying SentencePiece ===\n",
      "  _model: <class 'fairseq2.data.tokenizers.sentencepiece.SentencePieceModel'>\n",
      "=== Building Complete Vocabulary ===\n",
      "Successfully decoded: 0/9812\n",
      "\n",
      "First 50 labels: ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "\n",
      "Label at index 0 (likely blank): ''\n",
      "Label at index 1 (pad): ''\n",
      "Label at index 2 (eos): ''\n",
      "Label at index 3 (unk): ''\n"
     ]
    }
   ],
   "source": [
    "# Get vocabulary tokens from tokenizer\n",
    "print(\"=== Extracting Vocabulary Tokens ===\")\n",
    "\n",
    "vocab_info = pipeline.tokenizer.vocab_info\n",
    "vocab_size = vocab_info.size\n",
    "print(f\"Vocab size: {vocab_size}\")\n",
    "print(f\"Special tokens: unk={vocab_info.unk_idx}, bos={vocab_info.bos_idx}, eos={vocab_info.eos_idx}, pad={vocab_info.pad_idx}\")\n",
    "\n",
    "# Create decoder to convert IDs to tokens\n",
    "decoder = pipeline.tokenizer.create_decoder()\n",
    "print(f\"\\nDecoder type: {type(decoder)}\")\n",
    "\n",
    "# Try to decode individual token IDs\n",
    "print(\"\\n=== Decoding sample token IDs ===\")\n",
    "labels = []\n",
    "for i in range(vocab_size):\n",
    "    try:\n",
    "        # Decoder expects batch of sequences\n",
    "        token_tensor = torch.tensor([[i]])\n",
    "        decoded = decoder(token_tensor)\n",
    "        if isinstance(decoded, list):\n",
    "            token = decoded[0] if decoded else \"\"\n",
    "        else:\n",
    "            token = str(decoded)\n",
    "        labels.append(token)\n",
    "\n",
    "        # Print first 20 and some samples\n",
    "        if i < 20 or i in [100, 500, 1000, 5000, 9000]:\n",
    "            print(f\"  ID {i}: '{token}'\")\n",
    "    except Exception as e:\n",
    "        labels.append(f\"<{i}>\")\n",
    "        if i < 20:\n",
    "            print(f\"  ID {i}: ERROR - {e}\")\n",
    "\n",
    "print(f\"\\nTotal labels extracted: {len(labels)}\")\n",
    "print(f\"Sample labels [0:10]: {labels[0:10]}\")\n",
    "print(f\"Sample labels [100:110]: {labels[100:110]}\")\n",
    "\n",
    "# Alternative: Try raw encoder to understand token structure\n",
    "print(\"=== Testing Encoder ===\")\n",
    "\n",
    "encoder = pipeline.tokenizer.create_encoder()\n",
    "print(f\"Encoder type: {type(encoder)}\")\n",
    "print(f\"Encoder attrs: {[a for a in dir(encoder) if not a.startswith('_')]}\")\n",
    "\n",
    "# Encode some text\n",
    "test_texts = [\"hello\", \"i had that curiosity\", \"test\"]\n",
    "for text in test_texts:\n",
    "    try:\n",
    "        encoded = encoder(text)\n",
    "        print(f\"'{text}' -> {encoded}\")\n",
    "    except Exception as e:\n",
    "        print(f\"'{text}' -> ERROR: {e}\")\n",
    "\n",
    "# Check if there's a way to get the raw sentencepiece model\n",
    "print(\"=== Raw SentencePiece Access ===\")\n",
    "\n",
    "tokenizer = pipeline.tokenizer\n",
    "print(f\"Tokenizer attrs: {[a for a in dir(tokenizer) if not a.startswith('_')]}\")\n",
    "\n",
    "# Check for model or vocab access\n",
    "for attr in ['model', 'sp_model', 'spm', 'vocab', 'get_vocab', 'id_to_token', 'token_to_id']:\n",
    "    if hasattr(tokenizer, attr):\n",
    "        val = getattr(tokenizer, attr)\n",
    "        print(f\"  {attr}: {type(val)}\")\n",
    "        if callable(val):\n",
    "            try:\n",
    "                result = val()\n",
    "                print(f\"    -> {type(result)}\")\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "# Try to access underlying sentencepiece processor\n",
    "print(\"=== Underlying SentencePiece ===\")\n",
    "\n",
    "# RawSentencePieceTokenizer might have _processor or similar\n",
    "for attr in dir(tokenizer):\n",
    "    if 'piece' in attr.lower() or 'sp' in attr.lower() or 'processor' in attr.lower() or 'model' in attr.lower():\n",
    "        print(f\"  {attr}: {type(getattr(tokenizer, attr, None))}\")\n",
    "\n",
    "# Build complete vocab using the decoder we know works\n",
    "print(\"=== Building Complete Vocabulary ===\")\n",
    "\n",
    "vocab_size = pipeline.tokenizer.vocab_info.size\n",
    "decoder = pipeline.tokenizer.create_decoder()\n",
    "\n",
    "# Decode all tokens\n",
    "all_labels = []\n",
    "failed = 0\n",
    "\n",
    "for i in range(vocab_size):\n",
    "    try:\n",
    "        token_tensor = torch.tensor([[i]])\n",
    "        decoded = decoder(token_tensor)\n",
    "        if isinstance(decoded, list):\n",
    "            token = decoded[0] if decoded else \"\"\n",
    "        else:\n",
    "            token = str(decoded)\n",
    "        all_labels.append(token)\n",
    "    except:\n",
    "        all_labels.append(\"\")\n",
    "        failed += 1\n",
    "\n",
    "print(f\"Successfully decoded: {vocab_size - failed}/{vocab_size}\")\n",
    "print(f\"\\nFirst 50 labels: {all_labels[:50]}\")\n",
    "\n",
    "# Find blank token (usually index 0 for CTC)\n",
    "print(f\"\\nLabel at index 0 (likely blank): '{all_labels[0]}'\")\n",
    "print(f\"Label at index 1 (pad): '{all_labels[1]}'\")\n",
    "print(f\"Label at index 2 (eos): '{all_labels[2]}'\")\n",
    "print(f\"Label at index 3 (unk): '{all_labels[3]}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Decoding with 1D tensor ===\n",
      "  ID 0: ''\n",
      "  ID 1: ''\n",
      "  ID 2: ''\n",
      "  ID 3: ' ⁇ '\n",
      "  ID 4: ' '\n",
      "  ID 5: 'ዘ'\n",
      "  ID 10: '祭'\n",
      "  ID 100: 'ු'\n",
      "  ID 500: '仗'\n",
      "  ID 1000: '깨'\n",
      "=== Access SentencePieceModel ===\n",
      "sp_model type: <class 'fairseq2.data.tokenizers.sentencepiece.SentencePieceModel'>\n",
      "sp_model attrs: ['bos_idx', 'eos_idx', 'index_to_token', 'pad_idx', 'token_to_index', 'unk_idx', 'vocabulary_size']\n",
      "  Found method: index_to_token\n",
      "    sp_model.index_to_token(0) = '<s>'\n",
      "=== Using encode_as_tokens ===\n",
      "'hello' -> tokens: ['h', 'e', 'l', 'l', 'o'], ids: [113, 9346, 1875, 1875, 8749]\n",
      "'a' -> tokens: ['a'], ids: [9499]\n",
      "'i' -> tokens: ['i'], ids: [4328]\n",
      "' ' -> tokens: [], ids: []\n",
      "'test' -> tokens: ['t', 'e', 's', 't'], ids: [2226, 9346, 7076, 2226]\n",
      "=== Decode a sequence ===\n",
      "Decoded 'hello' tokens: 'hello'\n",
      "Decoded partial sequence: 'i ha'\n",
      "=== SentencePieceModel methods ===\n",
      "  bos_idx = int\n",
      "  eos_idx = int\n",
      "  index_to_token()\n",
      "  pad_idx = int\n",
      "  token_to_index()\n",
      "  unk_idx = int\n",
      "  vocabulary_size = int\n",
      "=== Using pipeline.token_decoder ===\n",
      "token_decoder type: <class 'fairseq2.data.tokenizers.sentencepiece.SentencePieceDecoder'>\n",
      "token_decoder attrs: ['decode_from_tokens']\n",
      "Decoded: hello\n"
     ]
    }
   ],
   "source": [
    "# Fix: Decoder expects 1D tensor\n",
    "print(\"=== Decoding with 1D tensor ===\")\n",
    "\n",
    "decoder = pipeline.tokenizer.create_decoder()\n",
    "\n",
    "# Test with 1D tensor\n",
    "for i in [0, 1, 2, 3, 4, 5, 10, 100, 500, 1000]:\n",
    "    try:\n",
    "        token_tensor = torch.tensor([i])  # 1D, not 2D\n",
    "        decoded = decoder(token_tensor)\n",
    "        print(f\"  ID {i}: '{decoded}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ID {i}: ERROR - {e}\")\n",
    "\n",
    "# Access the underlying SentencePiece model directly\n",
    "print(\"=== Access SentencePieceModel ===\")\n",
    "\n",
    "sp_model = pipeline.tokenizer._model\n",
    "print(f\"sp_model type: {type(sp_model)}\")\n",
    "print(f\"sp_model attrs: {[a for a in dir(sp_model) if not a.startswith('_')]}\")\n",
    "\n",
    "# Try common sentencepiece methods\n",
    "for method in ['id_to_piece', 'IdToPiece', 'decode_from_ids', 'index_to_token', 'get_piece']:\n",
    "    if hasattr(sp_model, method):\n",
    "        print(f\"  Found method: {method}\")\n",
    "        try:\n",
    "            result = getattr(sp_model, method)(0)\n",
    "            print(f\"    sp_model.{method}(0) = '{result}'\")\n",
    "        except Exception as e:\n",
    "            print(f\"    Error: {e}\")\n",
    "\n",
    "# Try encode_as_tokens to understand the token format\n",
    "print(\"=== Using encode_as_tokens ===\")\n",
    "\n",
    "encoder = pipeline.tokenizer.create_encoder()\n",
    "\n",
    "test_texts = [\"hello\", \"a\", \"i\", \" \", \"test\"]\n",
    "for text in test_texts:\n",
    "    try:\n",
    "        # encode_as_tokens might return actual token strings\n",
    "        tokens = encoder.encode_as_tokens(text)\n",
    "        ids = encoder(text)\n",
    "        print(f\"'{text}' -> tokens: {tokens}, ids: {ids.tolist()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"'{text}' -> ERROR: {e}\")\n",
    "\n",
    "# Try to decode a sequence (what we got from transcription)\n",
    "print(\"=== Decode a sequence ===\")\n",
    "\n",
    "decoder = pipeline.tokenizer.create_decoder()\n",
    "\n",
    "# The encoder output for \"hello\" was [113, 9346, 1875, 1875, 8749]\n",
    "test_sequence = torch.tensor([113, 9346, 1875, 1875, 8749])\n",
    "try:\n",
    "    decoded = decoder(test_sequence)\n",
    "    print(f\"Decoded 'hello' tokens: '{decoded}'\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Try other sequences\n",
    "test_sequence2 = torch.tensor([4328, 4, 113, 9499])  # Part of \"i had that curiosity\"\n",
    "try:\n",
    "    decoded = decoder(test_sequence2)\n",
    "    print(f\"Decoded partial sequence: '{decoded}'\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Check if SentencePieceModel has index_to_token or similar\n",
    "print(\"=== SentencePieceModel methods ===\")\n",
    "\n",
    "sp_model = pipeline.tokenizer._model\n",
    "\n",
    "# List all methods\n",
    "for attr in sorted(dir(sp_model)):\n",
    "    if not attr.startswith('_'):\n",
    "        val = getattr(sp_model, attr)\n",
    "        if callable(val):\n",
    "            print(f\"  {attr}()\")\n",
    "        else:\n",
    "            print(f\"  {attr} = {type(val).__name__}\")\n",
    "\n",
    "# Try to use the model's token_decoder (from pipeline attributes)\n",
    "print(\"=== Using pipeline.token_decoder ===\")\n",
    "\n",
    "token_decoder = pipeline.token_decoder\n",
    "print(f\"token_decoder type: {type(token_decoder)}\")\n",
    "print(f\"token_decoder attrs: {[a for a in dir(token_decoder) if not a.startswith('_')]}\")\n",
    "\n",
    "# Try to decode\n",
    "try:\n",
    "    test_ids = torch.tensor([113, 9346, 1875, 1875, 8749])\n",
    "    result = token_decoder(test_ids)\n",
    "    print(f\"Decoded: {result}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Building Complete Vocabulary ===\n",
      "Vocab size: 9812\n",
      "Special indices: bos=0, eos=2, pad=1, unk=3\n",
      "\n",
      "First 30 tokens:\n",
      "  0: '<s>'\n",
      "  1: '<pad>'\n",
      "  2: '</s>'\n",
      "  3: '<unk>'\n",
      "  4: ' '\n",
      "  5: 'ዘ'\n",
      "  6: 'љ'\n",
      "  7: '耷'\n",
      "  8: '氰'\n",
      "  9: '賬'\n",
      "  10: '祭'\n",
      "  11: '繳'\n",
      "  12: '渭'\n",
      "  13: '穗'\n",
      "  14: '捡'\n",
      "  15: '栩'\n",
      "  16: '勸'\n",
      "  17: '诚'\n",
      "  18: '戮'\n",
      "  19: '尉'\n",
      "  20: '們'\n",
      "  21: '邮'\n",
      "  22: '佈'\n",
      "  23: 'ޯ'\n",
      "  24: '刻'\n",
      "  25: '겔'\n",
      "  26: '纪'\n",
      "  27: '빠'\n",
      "  28: '吠'\n",
      "  29: '螺'\n",
      "\n",
      "Sample tokens:\n",
      "  100: 'ු'\n",
      "  500: '仗'\n",
      "  1000: '깨'\n",
      "  5000: '抑'\n",
      "  9000: '닐'\n",
      "  9811: '곽'\n",
      "\n",
      "Common characters:\n",
      "  'a' -> 9499\n",
      "  'b' -> 7565\n",
      "  'c' -> 5943\n",
      "  'h' -> 113\n",
      "  'e' -> 9346\n",
      "  'l' -> 1875\n",
      "  'o' -> 8749\n",
      "  ' ' -> 4\n",
      "=== Identifying CTC Blank Token ===\n",
      "  Index 0: '<s>'\n",
      "  Index 1: '<pad>'\n",
      "  Index 2: '</s>'\n",
      "  Index 3: '<unk>'\n",
      "  Index 4: ' '\n",
      "\n",
      "=== Check emissions for blank patterns ===\n",
      "Argmax shape: torch.Size([169])\n",
      "Argmax first 50: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4328, 0, 0, 4, 4, 113, 0, 9499, 0, 1133, 0, 0, 4, 4, 2226, 113, 9499, 0, 0]\n",
      "\n",
      "Most common IDs (likely blank is most frequent):\n",
      "  ID 0 ('<s>'): 115 times\n",
      "  ID 4 (' '): 17 times\n",
      "  ID 2226 ('t'): 6 times\n",
      "  ID 4328 ('i'): 5 times\n",
      "  ID 9346 ('e'): 4 times\n",
      "  ID 113 ('h'): 3 times\n",
      "  ID 9499 ('a'): 3 times\n",
      "  ID 7076 ('s'): 3 times\n",
      "  ID 910 ('m'): 3 times\n",
      "  ID 1133 ('d'): 2 times\n",
      "=== VocabInfo Summary for Backend ===\n",
      "VocabInfo:\n",
      "  vocab_size: 9812\n",
      "  blank_id: 0\n",
      "  blank_token: <s>\n",
      "  unk_id: 3\n",
      "  unk_token: <unk>\n",
      "  bos_id: 0\n",
      "  eos_id: 2\n",
      "  pad_id: 1\n",
      "\n",
      "Labels list: 9812 tokens\n",
      "  First 10: ['<s>', '<pad>', '</s>', '<unk>', ' ', 'ዘ', 'љ', '耷', '氰', '賬']\n"
     ]
    }
   ],
   "source": [
    "# Build complete vocabulary using sp_model.index_to_token\n",
    "print(\"=== Building Complete Vocabulary ===\")\n",
    "\n",
    "sp_model = pipeline.tokenizer._model\n",
    "vocab_size = sp_model.vocabulary_size\n",
    "\n",
    "print(f\"Vocab size: {vocab_size}\")\n",
    "print(f\"Special indices: bos={sp_model.bos_idx}, eos={sp_model.eos_idx}, pad={sp_model.pad_idx}, unk={sp_model.unk_idx}\")\n",
    "\n",
    "# Build labels list\n",
    "labels = []\n",
    "for i in range(vocab_size):\n",
    "    token = sp_model.index_to_token(i)\n",
    "    labels.append(token)\n",
    "\n",
    "print(f\"\\nFirst 30 tokens:\")\n",
    "for i in range(30):\n",
    "    print(f\"  {i}: '{labels[i]}'\")\n",
    "\n",
    "print(f\"\\nSample tokens:\")\n",
    "for i in [100, 500, 1000, 5000, 9000, 9811]:\n",
    "    print(f\"  {i}: '{labels[i]}'\")\n",
    "\n",
    "# Check common characters\n",
    "print(f\"\\nCommon characters:\")\n",
    "for char in ['a', 'b', 'c', 'h', 'e', 'l', 'o', ' ']:\n",
    "    try:\n",
    "        idx = sp_model.token_to_index(char)\n",
    "        print(f\"  '{char}' -> {idx}\")\n",
    "    except:\n",
    "        print(f\"  '{char}' -> NOT FOUND\")\n",
    "\n",
    "# Identify blank token for CTC\n",
    "print(\"=== Identifying CTC Blank Token ===\")\n",
    "\n",
    "# In CTC, blank is often:\n",
    "# - Index 0\n",
    "# - A special <blank> or <ctc> token\n",
    "# - The pad token\n",
    "\n",
    "# Check what's at common blank positions\n",
    "for i in [0, 1, 2, 3, 4]:\n",
    "    print(f\"  Index {i}: '{labels[i]}'\")\n",
    "\n",
    "# For OmniASR CTC, blank is likely index 0 (<s>/bos) or a dedicated blank\n",
    "# Let's check by looking at emission argmax patterns\n",
    "print(\"\\n=== Check emissions for blank patterns ===\")\n",
    "\n",
    "# Get emissions from our earlier capture\n",
    "if \"output\" in captured:\n",
    "    emissions = F.log_softmax(captured[\"output\"].float(), dim=-1)\n",
    "\n",
    "    # Argmax prediction\n",
    "    argmax_ids = emissions.argmax(dim=-1)[0]  # [T]\n",
    "    print(f\"Argmax shape: {argmax_ids.shape}\")\n",
    "    print(f\"Argmax first 50: {argmax_ids[:50].tolist()}\")\n",
    "\n",
    "    # Count most common IDs (blank should be most frequent)\n",
    "    from collections import Counter\n",
    "    counts = Counter(argmax_ids.tolist())\n",
    "    print(f\"\\nMost common IDs (likely blank is most frequent):\")\n",
    "    for idx, count in counts.most_common(10):\n",
    "        print(f\"  ID {idx} ('{labels[idx]}'): {count} times\")\n",
    "\n",
    "# Summary: Build VocabInfo for our backend\n",
    "print(\"=== VocabInfo Summary for Backend ===\")\n",
    "\n",
    "sp_model = pipeline.tokenizer._model\n",
    "\n",
    "# Determine blank_id - most common in CTC output is usually blank\n",
    "# Based on emission analysis above, or use bos_idx (0) as common choice\n",
    "\n",
    "# For OmniASR, blank is typically index 0\n",
    "blank_id = 0  # Adjust based on emission analysis\n",
    "\n",
    "vocab_info_dict = {\n",
    "    \"vocab_size\": sp_model.vocabulary_size,\n",
    "    \"blank_id\": blank_id,\n",
    "    \"blank_token\": sp_model.index_to_token(blank_id),\n",
    "    \"unk_id\": sp_model.unk_idx,\n",
    "    \"unk_token\": sp_model.index_to_token(sp_model.unk_idx),\n",
    "    \"bos_id\": sp_model.bos_idx,\n",
    "    \"eos_id\": sp_model.eos_idx,\n",
    "    \"pad_id\": sp_model.pad_idx,\n",
    "}\n",
    "\n",
    "print(f\"VocabInfo:\")\n",
    "for k, v in vocab_info_dict.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "print(f\"\\nLabels list: {len(labels)} tokens\")\n",
    "print(f\"  First 10: {labels[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Exploring OmniASR Preprocessing ===\n",
      "_build_audio_wavform_pipeline signature:\n",
      "(inp_list: 'AudioInput') -> 'DataPipelineBuilder'\n",
      "\n",
      "audio_decoder type: <class 'fairseq2n.bindings.data.audio.AudioDecoder'>\n",
      "audio_decoder attrs: []\n",
      "=== Check collater_audio ===\n",
      "collater_audio type: <class 'fairseq2n.bindings.data.data_pipeline.Collater'>\n",
      "collater_audio attrs: []\n",
      "\n",
      "full_collater type: <class 'fairseq2n.bindings.data.data_pipeline.Collater'>\n",
      "=== _apply_model_wav2vec2asr source ===\n",
      "    def _apply_model_wav2vec2asr(self, batch: Seq2SeqBatch) -> List[str]:\n",
      "        batch_layout = BatchLayout(\n",
      "            batch.source_seqs.shape,\n",
      "            seq_lens=batch.source_seq_lens,\n",
      "            device=batch.source_seqs.device,\n",
      "        )\n",
      "\n",
      "        logits, bl_out = self.model(batch.source_seqs, batch_layout)\n",
      "        pred_ids = torch.argmax(logits, dim=-1)\n",
      "        transcriptions = []\n",
      "\n",
      "        for i in range(pred_ids.shape[0]):\n",
      "            # Create a mask for where consecutive elements differ (CTC decoding)\n",
      "            # First element is always True, then compare with previous elements\n",
      "            seq = pred_ids[i][: bl_out.seq_lens[i]]\n",
      "            mask = torch.ones(seq.shape[0], dtype=torch.bool, device=seq.device)\n",
      "            mask[1:] = seq[1:] != seq[:-1]\n",
      "\n",
      "            # Use the mask to select non-duplicate tokens\n",
      "            decoded_ids = seq[mask]\n",
      "            transcriptions.append(self.token_decoder(decoded_ids))\n",
      "        return transcriptions\n",
      "\n",
      "=== Finding SequenceBatch ===\n",
      "FAILED: from fairseq2.nn.padding import PaddingMask -> No module named 'fairseq2.nn.padding'\n",
      "SUCCESS: from fairseq2.data import Collater\n",
      "FAILED: from fairseq2.nn import SequenceBatch -> cannot import name 'SequenceBatch' from 'fairseq2.nn' (/usr/local/lib/python3.12/dist-packages/fairseq2/nn/__init__.py)\n",
      "FAILED: from fairseq2.models.sequence import SequenceBatch -> No module named 'fairseq2.models.sequence'\n",
      "=== Model forward signature ===\n",
      "Model type: <class 'fairseq2.models.wav2vec2.asr.model.Wav2Vec2AsrModel'>\n",
      "forward signature: (seqs: 'Tensor', seqs_layout: 'BatchLayout', targets: 'Tensor | None' = None, targets_layout: 'BatchLayout | None' = None, *, return_logits: 'bool' = False) -> 'Tensor | tuple[Tensor, BatchLayout] | tuple[Tensor, Tensor, BatchLayout]'\n",
      "\n",
      "encoder_frontend type: <class 'fairseq2.models.wav2vec2.frontend.Wav2Vec2Frontend'>\n",
      "encoder_frontend.forward signature: (seqs: 'Tensor', seqs_layout: 'BatchLayout', *, state_bag: 'IncrementalStateBag | None' = None) -> 'tuple[Tensor, BatchLayout]'\n",
      "=== Wav2Vec2AsrModel forward source ===\n",
      "    def forward(\n",
      "        self,\n",
      "        seqs: Tensor,\n",
      "        seqs_layout: BatchLayout,\n",
      "        targets: Tensor | None = None,\n",
      "        targets_layout: BatchLayout | None = None,\n",
      "        *,\n",
      "        return_logits: bool = False,\n",
      "    ) -> Tensor | tuple[Tensor, BatchLayout] | tuple[Tensor, Tensor, BatchLayout]:\n",
      "        seqs, seqs_layout, _ = self.encoder_frontend.extract_features(seqs, seqs_layout)\n",
      "\n",
      "        seqs, _ = self.encoder_frontend.process_features(\n",
      "            seqs, seqs_layout, self.masker if self.training else None\n",
      "        )\n",
      "\n",
      "        seqs = self.encoder(seqs, seqs_layout)\n",
      "\n",
      "        if self.final_dropout is not None:\n",
      "            seqs = self.final_dropout(seqs)\n",
      "\n",
      "        logits = self.final_proj(seqs)\n",
      "\n",
      "        if targets is None:\n",
      "            return logits, seqs_layout\n",
      "\n",
      "        if targets_layout is None:\n",
      "            raise ValueError(\n",
      "                \"`targets_layout` must be specified when `targets` is specified.\"\n",
      "            )\n",
      "\n",
      "        if targets_layout.packed:\n",
      "            raise ValueError(\"`targets` must not be a packed batch.\")\n",
      "\n",
      "        # For numerical stability run in single precision.\n",
      "        # (N, S, T)\n",
      "        log_probs = log_softmax(logits, dim=-1, dtype=torch.float32)\n",
      "\n",
      "        # (N, S, T) -> (S, N, T)\n",
      "        log_probs_t = log_probs.transpose(0, 1)\n",
      "\n",
      "        # ()\n",
      "        loss = ctc_loss(\n",
      "            log_probs=log_probs_t,\n",
      "            input_lengths=seqs_layout.seq_lens_pt,\n",
      "            targets=targets,\n",
      "            target_lengths=targets_layout.seq_lens_pt,\n",
      "            reduction=\"sum\",\n",
      "            zero_infinity=True,\n",
      "        )\n",
      "\n",
      "        if return_logits:\n",
      "            return loss, logits, seqs_layout\n",
      "\n",
      "        return loss\n",
      "\n",
      "=== Creating proper input ===\n",
      "fairseq2 version: 0.6\n",
      "fairseq2.nn contents: ['AdditiveResidualConnect', 'BatchLayout', 'ColumnShardedLinear', 'DropPathResidualConnect', 'Embedding', 'IdentityProjection', 'IncrementalState', 'IncrementalStateBag', 'InterpolatedPositionEncoder', 'LayerNorm', 'LearnedPositionEncoder', 'Linear', 'PositionEncoder', 'Projection', 'RMSNorm', 'ReferenceRotaryEncoder', 'ResidualConnect', 'RotaryEncoder', 'RowShardedLinear', 'ScaledResidualConnect', 'Sharded', 'ShardedEmbedding', 'Sinusoidal2dPositionEncoder', 'Sinusoidal3dPositionEncoder', 'SinusoidalNdPositionEncoder', 'SinusoidalPositionEncoder', 'StandardEmbedding', 'StandardLayerNorm', 'TiedProjection', 'VocabShardedEmbedding', 'annotations', 'batch_layout', 'embedding', 'fsdp', 'functional', 'get_shard_dims', 'incremental_state', 'init_bert_projection', 'init_scaled_embedding', 'normalization', 'position_encoder', 'projection', 'residual', 'sharded', 'utils']\n",
      "=== Exploring fairseq2.data ===\n",
      "fairseq2.data contents: ['CollateOptionsOverride', 'Collater', 'DataPipelineBuilder', 'SequenceData', 'audio', 'create_bucket_sizes', 'data_pipeline', 'parquet', 'text', 'tokenizers']\n",
      "  CollateOptionsOverride: <class 'pybind11_builtins.pybind11_type'>\n",
      "  Collater: <class 'pybind11_builtins.pybind11_type'>\n",
      "  SequenceData: <class 'typing._TypedDictMeta'>\n"
     ]
    }
   ],
   "source": [
    "# Explore OmniASR preprocessing pipeline\n",
    "print(\"=== Exploring OmniASR Preprocessing ===\")\n",
    "\n",
    "# Check what _build_audio_wavform_pipeline does\n",
    "import inspect\n",
    "print(\"_build_audio_wavform_pipeline signature:\")\n",
    "print(inspect.signature(pipeline._build_audio_wavform_pipeline))\n",
    "\n",
    "# Check audio_decoder\n",
    "print(f\"\\naudio_decoder type: {type(pipeline.audio_decoder)}\")\n",
    "print(f\"audio_decoder attrs: {[a for a in dir(pipeline.audio_decoder) if not a.startswith('_')]}\")\n",
    "\n",
    "# Try to understand how pipeline processes audio internally\n",
    "print(\"=== Check collater_audio ===\")\n",
    "\n",
    "collater = pipeline.collater_audio\n",
    "print(f\"collater_audio type: {type(collater)}\")\n",
    "print(f\"collater_audio attrs: {[a for a in dir(collater) if not a.startswith('_')]}\")\n",
    "\n",
    "# Check full_collater\n",
    "print(f\"\\nfull_collater type: {type(pipeline.full_collater)}\")\n",
    "\n",
    "# Look at _apply_model_wav2vec2asr - this is where the magic happens\n",
    "print(\"=== _apply_model_wav2vec2asr source ===\")\n",
    "try:\n",
    "    source = inspect.getsource(pipeline._apply_model_wav2vec2asr)\n",
    "    print(source[:3000])\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Check what fairseq2 SequenceBatch looks like (different import path)\n",
    "print(\"=== Finding SequenceBatch ===\")\n",
    "\n",
    "# Try different import paths\n",
    "import_attempts = [\n",
    "    \"from fairseq2.nn.padding import PaddingMask\",\n",
    "    \"from fairseq2.data import Collater\",\n",
    "    \"from fairseq2.nn import SequenceBatch\",\n",
    "    \"from fairseq2.models.sequence import SequenceBatch\",\n",
    "]\n",
    "\n",
    "for imp in import_attempts:\n",
    "    try:\n",
    "        exec(imp)\n",
    "        print(f\"SUCCESS: {imp}\")\n",
    "    except ImportError as e:\n",
    "        print(f\"FAILED: {imp} -> {e}\")\n",
    "\n",
    "# Check model's expected input format by looking at forward signature\n",
    "print(\"=== Model forward signature ===\")\n",
    "\n",
    "model = pipeline.model\n",
    "print(f\"Model type: {type(model)}\")\n",
    "\n",
    "# Check forward method\n",
    "import inspect\n",
    "try:\n",
    "    sig = inspect.signature(model.forward)\n",
    "    print(f\"forward signature: {sig}\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Check what encoder_frontend expects\n",
    "print(f\"\\nencoder_frontend type: {type(model.encoder_frontend)}\")\n",
    "if hasattr(model.encoder_frontend, 'forward'):\n",
    "    try:\n",
    "        sig = inspect.signature(model.encoder_frontend.forward)\n",
    "        print(f\"encoder_frontend.forward signature: {sig}\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Look at fairseq2's wav2vec2 model source to understand input format\n",
    "print(\"=== Wav2Vec2AsrModel forward source ===\")\n",
    "\n",
    "from fairseq2.models.wav2vec2.asr.model import Wav2Vec2AsrModel\n",
    "try:\n",
    "    source = inspect.getsource(Wav2Vec2AsrModel.forward)\n",
    "    print(source[:2000])\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Check if there's a way to create the proper input batch\n",
    "print(\"=== Creating proper input ===\")\n",
    "\n",
    "# Look for SequenceBatch or similar in fairseq2\n",
    "import fairseq2\n",
    "print(f\"fairseq2 version: {fairseq2.__version__}\")\n",
    "\n",
    "# Check nn module\n",
    "from fairseq2 import nn as fs2_nn\n",
    "print(f\"fairseq2.nn contents: {[a for a in dir(fs2_nn) if not a.startswith('_')]}\")\n",
    "\n",
    "# Try to find how to create a SequenceBatch from raw waveform\n",
    "print(\"=== Exploring fairseq2.data ===\")\n",
    "\n",
    "from fairseq2 import data as fs2_data\n",
    "print(f\"fairseq2.data contents: {[a for a in dir(fs2_data) if not a.startswith('_')]}\")\n",
    "\n",
    "# Check for collate utilities\n",
    "for name in dir(fs2_data):\n",
    "    if 'collat' in name.lower() or 'batch' in name.lower() or 'sequence' in name.lower():\n",
    "        print(f\"  {name}: {type(getattr(fs2_data, name))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Compare model structures ===\n",
      "OmniASR structure:\n",
      "  encoder_frontend: Wav2Vec2Frontend\n",
      "  encoder: StandardTransformerEncoder\n",
      "  final_proj: Linear\n"
     ]
    }
   ],
   "source": [
    "# Explore model structure patterns\n",
    "print(\"=== Compare model structures ===\")\n",
    "\n",
    "# OmniASR (fairseq2)\n",
    "omni_model = pipeline.model\n",
    "print(\"OmniASR structure:\")\n",
    "for name, child in omni_model.named_children():\n",
    "    print(f\"  {name}: {type(child).__name__}\")\n",
    "\n",
    "# If you have HuggingFace model loaded:\n",
    "from transformers import Wav2Vec2ForCTC\n",
    "hf_model = Wav2Vec2ForCTC.from_pretrained(\"facebook/omniASR_CTC_300M\")\n",
    "print(\"\\nHuggingFace structure:\")\n",
    "for name, child in hf_model.named_children():\n",
    "    print(f\"  {name}: {type(child).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Direct Model Call with BatchLayout ===\n",
      "Model dtype: torch.bfloat16\n",
      "Waveform shape: torch.Size([1, 54400]), dtype: torch.bfloat16\n",
      "BatchLayout: BatchLayout(width=54400, seq_begin_indices=[0, 54400], seq_lens=[tensor(54400, device='cuda:0')], min_seq_len=54400, max_seq_len=54400, padded=False, packed=False)\n",
      "=== Calling model.forward() directly ===\n",
      "Logits shape: torch.Size([1, 169, 9812])\n",
      "Output layout: BatchLayout(width=169, seq_begin_indices=[0, 169], seq_lens=[169], min_seq_len=169, max_seq_len=169, padded=False, packed=False)\n",
      "Emissions shape: torch.Size([1, 169, 9812])\n",
      "Sequence length: 169\n",
      "\n",
      "Decoded text: 'i had that curiosity beside me at this moment'\n"
     ]
    }
   ],
   "source": [
    "# Direct model call with BatchLayout\n",
    "print(\"=== Direct Model Call with BatchLayout ===\")\n",
    "\n",
    "from fairseq2.nn import BatchLayout\n",
    "import torchaudio\n",
    "\n",
    "# Convert device string to torch.device\n",
    "device_obj = torch.device(device)\n",
    "\n",
    "# Check model dtype\n",
    "model = pipeline.model\n",
    "model_dtype = next(model.parameters()).dtype\n",
    "print(f\"Model dtype: {model_dtype}\")\n",
    "\n",
    "# Load test audio\n",
    "audio_path = \"/content/torchaudio_aligner/examples/Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav\"\n",
    "waveform, sr = torchaudio.load(audio_path)\n",
    "\n",
    "# Resample to 16kHz if needed\n",
    "if sr != 16000:\n",
    "    waveform = torchaudio.functional.resample(waveform, sr, 16000)\n",
    "\n",
    "# Convert to mono if stereo\n",
    "if waveform.shape[0] > 1:\n",
    "    waveform = waveform.mean(dim=0, keepdim=True)\n",
    "\n",
    "# Prepare input: (batch, samples) - MATCH MODEL DTYPE\n",
    "waveform = waveform.squeeze(0)  # Remove channel dim -> (samples,)\n",
    "waveform_batch = waveform.unsqueeze(0).to(device_obj, dtype=model_dtype)  # Match dtype!\n",
    "\n",
    "print(f\"Waveform shape: {waveform_batch.shape}, dtype: {waveform_batch.dtype}\")\n",
    "\n",
    "# Create BatchLayout\n",
    "seq_lens = torch.tensor([waveform_batch.shape[1]], device=device_obj)\n",
    "batch_layout = BatchLayout(\n",
    "    waveform_batch.shape,\n",
    "    seq_lens=seq_lens,\n",
    "    device=device_obj,\n",
    ")\n",
    "\n",
    "print(f\"BatchLayout: {batch_layout}\")\n",
    "\n",
    "# Call model directly\n",
    "print(\"=== Calling model.forward() directly ===\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    logits, output_layout = model(waveform_batch, batch_layout)\n",
    "\n",
    "print(f\"Logits shape: {logits.shape}\")\n",
    "print(f\"Output layout: {output_layout}\")\n",
    "\n",
    "# Convert to emissions (use float32 for numerical stability)\n",
    "emissions = F.log_softmax(logits.float(), dim=-1)\n",
    "print(f\"Emissions shape: {emissions.shape}\")\n",
    "\n",
    "# Verify by greedy decoding\n",
    "pred_ids = torch.argmax(logits, dim=-1)[0]\n",
    "\n",
    "# seq_lens is a list of ints, not tensors\n",
    "seq_len = output_layout.seq_lens[0]\n",
    "if hasattr(seq_len, 'item'):\n",
    "    seq_len = seq_len.item()\n",
    "print(f\"Sequence length: {seq_len}\")\n",
    "\n",
    "pred_ids = pred_ids[:seq_len]\n",
    "\n",
    "# CTC collapse\n",
    "mask = torch.ones(pred_ids.shape[0], dtype=torch.bool, device=device_obj)\n",
    "mask[1:] = pred_ids[1:] != pred_ids[:-1]\n",
    "decoded_ids = pred_ids[mask]\n",
    "\n",
    "# Remove blank (index 0)\n",
    "blank_id = 0\n",
    "decoded_ids = decoded_ids[decoded_ids != blank_id]\n",
    "\n",
    "# Decode to text\n",
    "decoder = pipeline.tokenizer.create_decoder()\n",
    "text = decoder(decoded_ids)\n",
    "print(f\"\\nDecoded text: '{text}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test Batched Inference ===\n",
      "Batch shape: torch.Size([2, 54400])\n",
      "Seq lens: [27200, 54400]\n",
      "BatchLayout: BatchLayout(width=54400, seq_begin_indices=[0, 54400, 108800], seq_lens=[tensor(27200, device='cuda:0'), tensor(54400, device='cuda:0')], min_seq_len=27200, max_seq_len=54400, padded=True, packed=False)\n",
      "\n",
      "Output logits shape: torch.Size([2, 169, 9812])\n",
      "Output seq_lens: [84, 169]\n",
      "\n",
      "Sample 0 (len=84): 'i had that curiosi'\n",
      "\n",
      "Sample 1 (len=169): 'i had that curiosity beside me at this moment'\n"
     ]
    }
   ],
   "source": [
    "# Test batched inference with different lengths\n",
    "print(\"=== Test Batched Inference ===\")\n",
    "\n",
    "# Create two waveforms of different lengths\n",
    "waveform1_len = waveform_batch.shape[1] // 2  # Half length\n",
    "waveform2_len = waveform_batch.shape[1]       # Full length\n",
    "\n",
    "waveform1 = waveform_batch[0, :waveform1_len]\n",
    "waveform2 = waveform_batch[0, :waveform2_len]\n",
    "\n",
    "# Pad to same length\n",
    "max_len = max(waveform1_len, waveform2_len)\n",
    "waveform1_padded = F.pad(waveform1, (0, max_len - waveform1_len))\n",
    "waveform2_padded = waveform2  # Already max length\n",
    "\n",
    "# Stack into batch\n",
    "batch = torch.stack([waveform1_padded, waveform2_padded])\n",
    "seq_lens = torch.tensor([waveform1_len, waveform2_len], device=device_obj)\n",
    "\n",
    "print(f\"Batch shape: {batch.shape}\")\n",
    "print(f\"Seq lens: {seq_lens.tolist()}\")\n",
    "\n",
    "# Create BatchLayout\n",
    "batch_layout = BatchLayout(\n",
    "    batch.shape,\n",
    "    seq_lens=seq_lens,\n",
    "    device=device_obj,\n",
    ")\n",
    "\n",
    "print(f\"BatchLayout: {batch_layout}\")\n",
    "\n",
    "# Forward pass\n",
    "with torch.inference_mode():\n",
    "    logits, output_layout = model(batch, batch_layout)\n",
    "\n",
    "print(f\"\\nOutput logits shape: {logits.shape}\")\n",
    "print(f\"Output seq_lens: {output_layout.seq_lens}\")\n",
    "\n",
    "# Decode both\n",
    "emissions = F.log_softmax(logits.float(), dim=-1)\n",
    "decoder = pipeline.tokenizer.create_decoder()\n",
    "blank_id = 0\n",
    "\n",
    "for i in range(2):\n",
    "    pred_ids = torch.argmax(logits[i], dim=-1)\n",
    "    seq_len = output_layout.seq_lens[i]\n",
    "    if hasattr(seq_len, 'item'):\n",
    "        seq_len = seq_len.item()\n",
    "    pred_ids = pred_ids[:seq_len]\n",
    "\n",
    "    # CTC collapse\n",
    "    mask = torch.ones(pred_ids.shape[0], dtype=torch.bool, device=device_obj)\n",
    "    mask[1:] = pred_ids[1:] != pred_ids[:-1]\n",
    "    decoded_ids = pred_ids[mask]\n",
    "    decoded_ids = decoded_ids[decoded_ids != blank_id]\n",
    "\n",
    "    text = decoder(decoded_ids)\n",
    "    print(f\"\\nSample {i} (len={output_layout.seq_lens[i]}): '{text}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# First, clone repo to get access to install_utils\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "    GITHUB_REPO = \"https://github.com/huangruizhe/torchaudio_aligner.git\"\n",
    "    BRANCH = \"dev\"\n",
    "    repo_path = '/content/torchaudio_aligner'\n",
    "    src_path = f'{repo_path}/src'\n",
    "    \n",
    "    if not os.path.exists(repo_path):\n",
    "        os.system(f'git clone -b {BRANCH} {GITHUB_REPO} {repo_path}')\n",
    "    else:\n",
    "        os.system(f'cd {repo_path} && git pull origin dev')\n",
    "    \n",
    "    if src_path not in sys.path:\n",
    "        sys.path.insert(0, src_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">parameter load: <span style=\"color: #f92672; text-decoration-color: #f92672\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">╺━━━━━━━━━</span> <span style=\"color: #008000; text-decoration-color: #008000\">  321/423</span> <span style=\"color: #800080; text-decoration-color: #800080\"> 76%</span> <span style=\"color: #008080; text-decoration-color: #008080\">0:00:01</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "parameter load: \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━\u001b[0m \u001b[32m  321/423\u001b[0m \u001b[35m 76%\u001b[0m \u001b[36m0:00:01\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 9812\n",
      "First 20 tokens: ['<s>', '<pad>', '</s>', '<unk>', ' ', 'ዘ', 'љ', '耷', '氰', '賬', '祭', '繳', '渭', '穗', '捡', '栩', '勸', '诚', '戮', '尉']\n",
      "Emissions shape: torch.Size([1, 49, 9812])\n",
      "Lengths: tensor([49], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Reload the module\n",
    "import importlib\n",
    "import labeling_utils\n",
    "importlib.reload(labeling_utils)\n",
    "\n",
    "# Test the OmniASR backend\n",
    "from labeling_utils import load_model\n",
    "\n",
    "backend = load_model(\"omniasr-300m\", device=\"cuda\")\n",
    "print(f\"Vocab size: {len(backend.get_vocab_info().labels)}\")\n",
    "print(f\"First 20 tokens: {backend.get_vocab_info().labels[:20]}\")\n",
    "\n",
    "# Test emission extraction\n",
    "import torch\n",
    "waveform = torch.randn(1, 16000).to(\"cuda\")  # 1 second of audio\n",
    "emissions, lengths = backend.get_emissions(waveform)\n",
    "print(f\"Emissions shape: {emissions.shape}\")\n",
    "print(f\"Lengths: {lengths}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
