{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Notebook: OmniLingual ASR (OmniASR) Backend\n",
    "\n",
    "This notebook tests and debugs the OmniASR backend for torchaudio_aligner.\n",
    "\n",
    "**Goals:**\n",
    "1. Understand OmniASR's internal structure (model, tokenizer, vocab)\n",
    "2. Properly extract vocabulary from the tokenizer\n",
    "3. Properly extract CTC emissions/posteriors\n",
    "4. Test different approaches to get emissions\n",
    "\n",
    "**Reference:**\n",
    "- https://github.com/facebookresearch/omnilingual-asr\n",
    "- Models built on fairseq2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "# WARNING: omnilingual-asr may have conflicting dependencies\n",
    "!pip install -q omnilingual-asr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Explore OmniASR Pipeline Structure\n",
    "\n",
    "First, let's understand what's inside the ASRInferencePipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omnilingual_asr.models.inference.pipeline import ASRInferencePipeline\n",
    "\n",
    "# Load a smaller model for testing\n",
    "MODEL_NAME = \"omniASR_CTC_300M\"  # Smallest CTC model\n",
    "# MODEL_NAME = \"omniASR_CTC_1B\"  # 1B parameter model\n",
    "# MODEL_NAME = \"omniASR_CTC_1B_v2\"  # v2 uses different tokenizer\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "pipeline = ASRInferencePipeline(model_card=MODEL_NAME, device=device)\n",
    "print(\"Pipeline loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect pipeline attributes\n",
    "print(\"=\" * 60)\n",
    "print(\"Pipeline Attributes:\")\n",
    "print(\"=\" * 60)\n",
    "for attr in dir(pipeline):\n",
    "    if not attr.startswith('_'):\n",
    "        val = getattr(pipeline, attr, None)\n",
    "        if not callable(val):\n",
    "            print(f\"  {attr}: {type(val).__name__}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Pipeline Private Attributes:\")\n",
    "print(\"=\" * 60)\n",
    "for attr in dir(pipeline):\n",
    "    if attr.startswith('_') and not attr.startswith('__'):\n",
    "        val = getattr(pipeline, attr, None)\n",
    "        if not callable(val):\n",
    "            print(f\"  {attr}: {type(val).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to access the model\n",
    "print(\"=\" * 60)\n",
    "print(\"Accessing Model:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "model = None\n",
    "if hasattr(pipeline, 'model'):\n",
    "    model = pipeline.model\n",
    "    print(f\"Found model via 'model' attribute\")\n",
    "elif hasattr(pipeline, '_model'):\n",
    "    model = pipeline._model\n",
    "    print(f\"Found model via '_model' attribute\")\n",
    "else:\n",
    "    # Search for torch.nn.Module in attributes\n",
    "    for attr in dir(pipeline):\n",
    "        val = getattr(pipeline, attr, None)\n",
    "        if isinstance(val, torch.nn.Module):\n",
    "            model = val\n",
    "            print(f\"Found model via '{attr}' attribute\")\n",
    "            break\n",
    "\n",
    "if model is not None:\n",
    "    print(f\"\\nModel type: {type(model).__name__}\")\n",
    "    print(f\"Model class: {model.__class__.__module__}.{model.__class__.__name__}\")\n",
    "else:\n",
    "    print(\"Could not find model!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect model structure\n",
    "if model is not None:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Model Attributes (non-callable):\")\n",
    "    print(\"=\" * 60)\n",
    "    for attr in dir(model):\n",
    "        if not attr.startswith('_'):\n",
    "            val = getattr(model, attr, None)\n",
    "            if not callable(val) and not isinstance(val, torch.nn.Module):\n",
    "                print(f\"  {attr}: {type(val).__name__}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Model Sub-modules:\")\n",
    "    print(\"=\" * 60)\n",
    "    for name, module in model.named_children():\n",
    "        print(f\"  {name}: {type(module).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep inspection of model modules\n",
    "if model is not None:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"All Named Modules (looking for CTC/projection layers):\")\n",
    "    print(\"=\" * 60)\n",
    "    for name, module in model.named_modules():\n",
    "        if any(s in name.lower() for s in ['ctc', 'proj', 'output', 'lm_head', 'linear', 'final']):\n",
    "            print(f\"  {name}: {type(module).__name__}\")\n",
    "            if isinstance(module, torch.nn.Linear):\n",
    "                print(f\"       in_features={module.in_features}, out_features={module.out_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Explore Tokenizer and Vocabulary\n",
    "\n",
    "The tokenizer determines the output vocabulary for CTC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to access tokenizer from pipeline\n",
    "print(\"=\" * 60)\n",
    "print(\"Accessing Tokenizer:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "tokenizer = None\n",
    "tokenizer_source = None\n",
    "\n",
    "# Check pipeline attributes\n",
    "for attr in ['tokenizer', '_tokenizer', 'text_tokenizer', '_text_tokenizer']:\n",
    "    if hasattr(pipeline, attr):\n",
    "        tokenizer = getattr(pipeline, attr)\n",
    "        tokenizer_source = f\"pipeline.{attr}\"\n",
    "        print(f\"Found tokenizer via {tokenizer_source}\")\n",
    "        break\n",
    "\n",
    "# Check model attributes\n",
    "if tokenizer is None and model is not None:\n",
    "    for attr in ['tokenizer', '_tokenizer', 'decoder']:\n",
    "        if hasattr(model, attr):\n",
    "            val = getattr(model, attr)\n",
    "            if hasattr(val, 'tokenizer'):\n",
    "                tokenizer = val.tokenizer\n",
    "                tokenizer_source = f\"model.{attr}.tokenizer\"\n",
    "            else:\n",
    "                tokenizer = val\n",
    "                tokenizer_source = f\"model.{attr}\"\n",
    "            print(f\"Found tokenizer via {tokenizer_source}\")\n",
    "            break\n",
    "\n",
    "if tokenizer is not None:\n",
    "    print(f\"\\nTokenizer type: {type(tokenizer).__name__}\")\n",
    "    print(f\"Tokenizer class: {tokenizer.__class__.__module__}.{tokenizer.__class__.__name__}\")\n",
    "else:\n",
    "    print(\"Could not find tokenizer directly!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect tokenizer attributes\n",
    "if tokenizer is not None:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Tokenizer Attributes:\")\n",
    "    print(\"=\" * 60)\n",
    "    for attr in dir(tokenizer):\n",
    "        if not attr.startswith('_'):\n",
    "            val = getattr(tokenizer, attr, None)\n",
    "            if not callable(val):\n",
    "                try:\n",
    "                    print(f\"  {attr}: {type(val).__name__} = {str(val)[:100]}\")\n",
    "                except:\n",
    "                    print(f\"  {attr}: {type(val).__name__}\")\n",
    "    \n",
    "    print(\"\\nTokenizer Methods:\")\n",
    "    for attr in dir(tokenizer):\n",
    "        if not attr.startswith('_'):\n",
    "            val = getattr(tokenizer, attr, None)\n",
    "            if callable(val):\n",
    "                print(f\"  {attr}()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to get vocabulary from tokenizer\n",
    "print(\"=\" * 60)\n",
    "print(\"Extracting Vocabulary:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "vocab = None\n",
    "vocab_size = None\n",
    "\n",
    "if tokenizer is not None:\n",
    "    # Method 1: get_vocab()\n",
    "    if hasattr(tokenizer, 'get_vocab'):\n",
    "        try:\n",
    "            vocab = tokenizer.get_vocab()\n",
    "            print(f\"Method 1 (get_vocab): vocab size = {len(vocab)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Method 1 failed: {e}\")\n",
    "    \n",
    "    # Method 2: vocab attribute\n",
    "    if hasattr(tokenizer, 'vocab'):\n",
    "        try:\n",
    "            v = tokenizer.vocab\n",
    "            print(f\"Method 2 (vocab attr): type = {type(v).__name__}\")\n",
    "            if isinstance(v, dict):\n",
    "                vocab = v\n",
    "                print(f\"  vocab size = {len(vocab)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Method 2 failed: {e}\")\n",
    "    \n",
    "    # Method 3: vocab_size attribute\n",
    "    if hasattr(tokenizer, 'vocab_size'):\n",
    "        try:\n",
    "            vocab_size = tokenizer.vocab_size\n",
    "            print(f\"Method 3 (vocab_size): {vocab_size}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Method 3 failed: {e}\")\n",
    "    \n",
    "    # Method 4: vocab_info()\n",
    "    if hasattr(tokenizer, 'vocab_info'):\n",
    "        try:\n",
    "            vi = tokenizer.vocab_info()\n",
    "            print(f\"Method 4 (vocab_info): {vi}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Method 4 failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Load tokenizer via fairseq2 asset store\n",
    "print(\"=\" * 60)\n",
    "print(\"Loading Tokenizer via Fairseq2 Asset Store:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    from fairseq2.assets import asset_store, download_manager\n",
    "    \n",
    "    # List available cards\n",
    "    print(\"\\nSearching for tokenizer cards...\")\n",
    "    \n",
    "    # Try to find OmniASR tokenizer\n",
    "    tokenizer_names = [\n",
    "        \"omniASR_tokenizer_written_v2\",\n",
    "        \"omniASR_tokenizer_v1\",\n",
    "        \"omniASR_tokenizer\",\n",
    "    ]\n",
    "    \n",
    "    for name in tokenizer_names:\n",
    "        try:\n",
    "            card = asset_store.retrieve_card(name)\n",
    "            print(f\"\\nFound card: {name}\")\n",
    "            print(f\"  Card: {card}\")\n",
    "            \n",
    "            # Try to download\n",
    "            if hasattr(card, 'uri'):\n",
    "                path = download_manager.download_tokenizer(card.uri)\n",
    "                print(f\"  Downloaded to: {path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  {name}: {e}\")\n",
    "            \n",
    "except ImportError as e:\n",
    "    print(f\"fairseq2.assets not available: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Get vocab from model output dimension\n",
    "print(\"=\" * 60)\n",
    "print(\"Getting Vocab Size from Model Output Layer:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if model is not None:\n",
    "    # Find the final linear layer (CTC projection)\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            if any(s in name.lower() for s in ['ctc', 'proj', 'output', 'final', 'lm']):\n",
    "                print(f\"  {name}: in={module.in_features}, out={module.out_features}\")\n",
    "                if vocab_size is None:\n",
    "                    vocab_size = module.out_features\n",
    "                    print(f\"  -> Using {vocab_size} as vocab size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Test Emission Extraction\n",
    "\n",
    "Now let's test different approaches to get CTC emissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test waveform\n",
    "print(\"=\" * 60)\n",
    "print(\"Creating Test Input:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Generate random audio (10 seconds at 16kHz)\n",
    "batch_size = 1\n",
    "duration_sec = 5\n",
    "sample_rate = 16000\n",
    "waveform = torch.randn(batch_size, duration_sec * sample_rate).to(device)\n",
    "lengths = torch.tensor([waveform.shape[1]] * batch_size).to(device)\n",
    "\n",
    "print(f\"Waveform shape: {waveform.shape}\")\n",
    "print(f\"Lengths: {lengths}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Direct model forward\n",
    "print(\"=\" * 60)\n",
    "print(\"Option 1: Direct Model Forward\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if model is not None:\n",
    "    model.eval()\n",
    "    \n",
    "    try:\n",
    "        with torch.inference_mode():\n",
    "            out = model(waveform, lengths)\n",
    "        \n",
    "        print(f\"Output type: {type(out).__name__}\")\n",
    "        \n",
    "        # Try to extract logits\n",
    "        if isinstance(out, torch.Tensor):\n",
    "            logits = out\n",
    "            print(f\"Output is tensor: {logits.shape}\")\n",
    "        elif isinstance(out, dict):\n",
    "            print(f\"Output keys: {out.keys()}\")\n",
    "            for key in ['logits', 'ctc_logits', 'emissions', 'encoder_out']:\n",
    "                if key in out:\n",
    "                    logits = out[key]\n",
    "                    print(f\"Found '{key}': {logits.shape if isinstance(logits, torch.Tensor) else type(logits)}\")\n",
    "                    break\n",
    "        else:\n",
    "            print(f\"Output attributes: {[a for a in dir(out) if not a.startswith('_')]}\")\n",
    "            for attr in ['logits', 'ctc_logits', 'emissions', 'output']:\n",
    "                if hasattr(out, attr):\n",
    "                    val = getattr(out, attr)\n",
    "                    print(f\"Found '{attr}': {val.shape if isinstance(val, torch.Tensor) else type(val)}\")\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Encoder + CTC projection\n",
    "print(\"=\" * 60)\n",
    "print(\"Option 2: Encoder + CTC Projection\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if model is not None:\n",
    "    model.eval()\n",
    "    \n",
    "    # Find encoder\n",
    "    encoder = None\n",
    "    for attr in ['encoder', 'wav2vec2', 'feature_extractor']:\n",
    "        if hasattr(model, attr):\n",
    "            encoder = getattr(model, attr)\n",
    "            print(f\"Found encoder: model.{attr}\")\n",
    "            break\n",
    "    \n",
    "    # Find CTC projection\n",
    "    ctc_proj = None\n",
    "    for attr in ['ctc_proj', 'output_projection', 'ctc', 'ctc_decoder', 'final_proj']:\n",
    "        if hasattr(model, attr):\n",
    "            ctc_proj = getattr(model, attr)\n",
    "            print(f\"Found CTC projection: model.{attr}\")\n",
    "            break\n",
    "    \n",
    "    if encoder is not None and ctc_proj is not None:\n",
    "        try:\n",
    "            with torch.inference_mode():\n",
    "                # Encode\n",
    "                encoder_out = encoder(waveform)\n",
    "                print(f\"Encoder output type: {type(encoder_out).__name__}\")\n",
    "                \n",
    "                # Get features\n",
    "                if isinstance(encoder_out, torch.Tensor):\n",
    "                    features = encoder_out\n",
    "                elif hasattr(encoder_out, 'output'):\n",
    "                    features = encoder_out.output\n",
    "                elif hasattr(encoder_out, 'last_hidden_state'):\n",
    "                    features = encoder_out.last_hidden_state\n",
    "                else:\n",
    "                    features = encoder_out\n",
    "                    \n",
    "                print(f\"Features shape: {features.shape if isinstance(features, torch.Tensor) else type(features)}\")\n",
    "                \n",
    "                # Apply CTC projection\n",
    "                logits = ctc_proj(features)\n",
    "                print(f\"Logits shape: {logits.shape}\")\n",
    "                \n",
    "                # Log softmax\n",
    "                emissions = F.log_softmax(logits.float(), dim=-1)\n",
    "                print(f\"Emissions shape: {emissions.shape}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    else:\n",
    "        print(\"Could not find encoder and/or CTC projection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 3: Hook into the model\n",
    "print(\"=\" * 60)\n",
    "print(\"Option 3: Hook-based Extraction\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if model is not None:\n",
    "    model.eval()\n",
    "    \n",
    "    captured = {}\n",
    "    \n",
    "    def hook_fn(module, inp, out):\n",
    "        captured[\"logits\"] = out\n",
    "    \n",
    "    # Find a likely projection layer\n",
    "    proj = None\n",
    "    proj_name = None\n",
    "    for name, m in model.named_modules():\n",
    "        if isinstance(m, torch.nn.Linear):\n",
    "            if any(s in name.lower() for s in [\"ctc\", \"proj\", \"output\", \"lm_head\", \"final\"]):\n",
    "                proj = m\n",
    "                proj_name = name\n",
    "                break\n",
    "    \n",
    "    if proj is not None:\n",
    "        print(f\"Hooking: {proj_name}\")\n",
    "        h = proj.register_forward_hook(hook_fn)\n",
    "        \n",
    "        try:\n",
    "            with torch.inference_mode():\n",
    "                _ = model(waveform, lengths)\n",
    "            \n",
    "            h.remove()\n",
    "            \n",
    "            if \"logits\" in captured:\n",
    "                logits = captured[\"logits\"]\n",
    "                print(f\"Captured logits shape: {logits.shape}\")\n",
    "                \n",
    "                emissions = F.log_softmax(logits.float(), dim=-1)\n",
    "                print(f\"Emissions shape: {emissions.shape}\")\n",
    "            else:\n",
    "                print(\"Hook did not capture logits\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            h.remove()\n",
    "            print(f\"Error: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    else:\n",
    "        print(\"Could not find a projection layer to hook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 4: Use extract_features if available\n",
    "print(\"=\" * 60)\n",
    "print(\"Option 4: extract_features() Method\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if model is not None:\n",
    "    model.eval()\n",
    "    \n",
    "    if hasattr(model, 'extract_features'):\n",
    "        try:\n",
    "            with torch.inference_mode():\n",
    "                encoder_out, padding_mask = model.extract_features(waveform, padding_mask=None)\n",
    "            \n",
    "            print(f\"Encoder output shape: {encoder_out.shape}\")\n",
    "            if padding_mask is not None:\n",
    "                print(f\"Padding mask shape: {padding_mask.shape}\")\n",
    "            \n",
    "            # Find CTC projection\n",
    "            ctc_proj = None\n",
    "            for attr in ['ctc_proj', 'output_projection', 'proj', 'final_proj']:\n",
    "                if hasattr(model, attr):\n",
    "                    ctc_proj = getattr(model, attr)\n",
    "                    print(f\"Found CTC projection: model.{attr}\")\n",
    "                    break\n",
    "            \n",
    "            if ctc_proj is not None:\n",
    "                logits = ctc_proj(encoder_out)\n",
    "                print(f\"Logits shape: {logits.shape}\")\n",
    "                \n",
    "                emissions = F.log_softmax(logits.float(), dim=-1)\n",
    "                print(f\"Emissions shape: {emissions.shape}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    else:\n",
    "        print(\"Model does not have extract_features() method\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 5: Full forward with output parsing\n",
    "print(\"=\" * 60)\n",
    "print(\"Option 5: Full Forward with Output Parsing\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if model is not None:\n",
    "    model.eval()\n",
    "    \n",
    "    try:\n",
    "        with torch.inference_mode():\n",
    "            # Try different forward signatures\n",
    "            out = None\n",
    "            \n",
    "            # Try 1: (source, padding_mask)\n",
    "            try:\n",
    "                out = model(source=waveform, padding_mask=None)\n",
    "                print(\"Forward signature: (source=, padding_mask=)\")\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Try 2: (waveform, lengths)\n",
    "            if out is None:\n",
    "                try:\n",
    "                    out = model(waveform, lengths)\n",
    "                    print(\"Forward signature: (waveform, lengths)\")\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            # Try 3: just waveform\n",
    "            if out is None:\n",
    "                try:\n",
    "                    out = model(waveform)\n",
    "                    print(\"Forward signature: (waveform)\")\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            if out is not None:\n",
    "                print(f\"\\nOutput type: {type(out).__name__}\")\n",
    "                \n",
    "                # Parse output\n",
    "                if isinstance(out, torch.Tensor):\n",
    "                    print(f\"Direct tensor output: {out.shape}\")\n",
    "                    logits = out\n",
    "                elif isinstance(out, dict):\n",
    "                    print(f\"Dict output keys: {out.keys()}\")\n",
    "                    for k, v in out.items():\n",
    "                        if isinstance(v, torch.Tensor):\n",
    "                            print(f\"  {k}: {v.shape}\")\n",
    "                        elif isinstance(v, dict):\n",
    "                            print(f\"  {k}: dict with keys {v.keys()}\")\n",
    "                else:\n",
    "                    # Named tuple or custom object\n",
    "                    print(f\"Object attributes:\")\n",
    "                    for attr in dir(out):\n",
    "                        if not attr.startswith('_'):\n",
    "                            val = getattr(out, attr, None)\n",
    "                            if isinstance(val, torch.Tensor):\n",
    "                                print(f\"  {attr}: {val.shape}\")\n",
    "                            elif not callable(val):\n",
    "                                print(f\"  {attr}: {type(val).__name__}\")\n",
    "            else:\n",
    "                print(\"Could not call model forward\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Test Pipeline Audio Loading\n",
    "\n",
    "Check if the pipeline has built-in audio loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check pipeline for audio loading methods\n",
    "print(\"=\" * 60)\n",
    "print(\"Pipeline Audio Loading Methods:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for attr in dir(pipeline):\n",
    "    if 'audio' in attr.lower() or 'load' in attr.lower() or 'wave' in attr.lower():\n",
    "        val = getattr(pipeline, attr, None)\n",
    "        print(f\"  {attr}: {type(val).__name__} {'(callable)' if callable(val) else ''}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test transcription to verify model works\n",
    "print(\"=\" * 60)\n",
    "print(\"Test Transcription (to verify model works):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Generate some test audio or use a real file\n",
    "try:\n",
    "    # If you have a test audio file, use it:\n",
    "    # result = pipeline(\"/path/to/audio.wav\")\n",
    "    \n",
    "    # Or test with generated audio (will produce gibberish)\n",
    "    import torchaudio\n",
    "    import tempfile\n",
    "    \n",
    "    # Create a simple test audio\n",
    "    test_waveform = torch.randn(1, 16000 * 3)  # 3 seconds\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as f:\n",
    "        torchaudio.save(f.name, test_waveform, 16000)\n",
    "        temp_path = f.name\n",
    "    \n",
    "    # Try to transcribe\n",
    "    result = pipeline(temp_path)\n",
    "    print(f\"Transcription result: {result}\")\n",
    "    print(f\"Result type: {type(result).__name__}\")\n",
    "    \n",
    "    import os\n",
    "    os.unlink(temp_path)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Summary and Recommendations\n",
    "\n",
    "Based on the exploration above, summarize what we learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n1. Model Name: {MODEL_NAME}\")\n",
    "print(f\"2. Model Type: {type(model).__name__ if model else 'Not found'}\")\n",
    "print(f\"3. Tokenizer Type: {type(tokenizer).__name__ if tokenizer else 'Not found'}\")\n",
    "print(f\"4. Vocab Size: {vocab_size if vocab_size else 'Unknown'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RECOMMENDATIONS FOR BACKEND IMPLEMENTATION:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\"\"\n",
    "Based on the exploration, update omniasr_backend.py to:\n",
    "\n",
    "1. TOKENIZER/VOCAB:\n",
    "   - [Fill in based on what worked above]\n",
    "\n",
    "2. EMISSION EXTRACTION:\n",
    "   - [Fill in based on what worked above]\n",
    "\n",
    "3. AUDIO LOADING:\n",
    "   - [Fill in based on what worked above]\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
