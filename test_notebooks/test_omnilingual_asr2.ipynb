{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OmniASR Backend Testing\n",
    "\n",
    "This notebook tests the OmniASR backend integration with `labeling_utils`.\n",
    "\n",
    "**OmniASR** is Facebook/Meta's Omnilingual ASR project supporting 1600+ languages.\n",
    "\n",
    "**Available models:**\n",
    "- `omniASR_CTC_300M`: 325M parameters (fastest)\n",
    "- `omniASR_CTC_1B`: 975M parameters\n",
    "- `omniASR_CTC_3B`: 3.08B parameters\n",
    "- `omniASR_CTC_7B`: 6.5B parameters\n",
    "\n",
    "**References:**\n",
    "- https://github.com/facebookresearch/omnilingual-asr\n",
    "- https://github.com/NeuralFalconYT/omnilingual-asr-colab (installation credit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation (Colab)\n",
    "\n",
    "OmniASR requires specific PyTorch/fairseq2 versions. Run this cell to install."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Installing OmniASR dependencies...\")\n",
    "    \n",
    "    # Uninstall existing PyTorch\n",
    "    !pip uninstall -y torch torchaudio torchvision -q\n",
    "    \n",
    "    # Install PyTorch 2.8.0 with CUDA 12.8 (fairseq2 requirement)\n",
    "    !pip install torch==2.8.0+cu128 torchaudio==2.8.0+cu128 torchvision==0.23.0+cu128 --index-url https://download.pytorch.org/whl/cu128 -q\n",
    "    \n",
    "    # Install fairseq2 and omnilingual-asr\n",
    "    !pip install fairseq2==0.6 -q\n",
    "    !pip install omnilingual-asr==0.1.0 -q\n",
    "    \n",
    "    # Additional dependencies\n",
    "    !pip install silero-vad>=4.0.0 onnxruntime>=1.12.0 uroman==1.3.1.1 -q\n",
    "    \n",
    "    # Reinstall fairseq2 with correct CUDA variant\n",
    "    !pip uninstall fairseq2 -y -q\n",
    "    !pip install fairseq2 --extra-index-url https://fair.pkg.atmeta.com/fairseq2/whl/pt2.8.0/cu126 -q\n",
    "    \n",
    "    # Reinstall omnilingual-asr\n",
    "    !pip install omnilingual-asr -q\n",
    "    \n",
    "    print(\"Installation complete!\")\n",
    "else:\n",
    "    print(\"Not running in Colab. See https://github.com/facebookresearch/omnilingual-asr for installation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Verify Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify OmniASR is installed\n",
    "try:\n",
    "    from omnilingual_asr.models.inference.pipeline import ASRInferencePipeline\n",
    "    print(\"OmniASR is installed correctly!\")\n",
    "except ImportError as e:\n",
    "    print(f\"OmniASR not installed: {e}\")\n",
    "    print(\"Run the installation cell above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setup Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "GITHUB_REPO = \"https://github.com/huangruizhe/torchaudio_aligner.git\"\n",
    "BRANCH = \"dev\"\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    repo_path = '/content/torchaudio_aligner'\n",
    "    src_path = f'{repo_path}/src'\n",
    "    \n",
    "    if not os.path.exists(repo_path):\n",
    "        print(f\"Cloning repository (branch: {BRANCH})...\")\n",
    "        os.system(f'git clone -b {BRANCH} {GITHUB_REPO} {repo_path}')\n",
    "    else:\n",
    "        print(f\"Updating repository (branch: {BRANCH})...\")\n",
    "        os.system(f'cd {repo_path} && git pull origin {BRANCH}')\n",
    "else:\n",
    "    # Local development\n",
    "    possible_paths = [\n",
    "        Path(\".\").absolute().parent / \"src\",\n",
    "        Path(\".\").absolute() / \"src\",\n",
    "    ]\n",
    "    src_path = None\n",
    "    for p in possible_paths:\n",
    "        if p.exists() and (p / \"labeling_utils\").exists():\n",
    "            src_path = str(p.absolute())\n",
    "            break\n",
    "    if src_path is None:\n",
    "        raise FileNotFoundError(\"src directory not found\")\n",
    "    print(f\"Running locally from: {src_path}\")\n",
    "\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "\n",
    "print(f\"Source path: {src_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import labeling_utils\n",
    "import importlib\n",
    "import labeling_utils\n",
    "importlib.reload(labeling_utils)\n",
    "\n",
    "from labeling_utils import (\n",
    "    load_model,\n",
    "    get_emissions,\n",
    "    get_emissions_batched,\n",
    "    list_backends,\n",
    "    is_backend_available,\n",
    "    list_presets,\n",
    ")\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Available backends: {list_backends()}\")\n",
    "print(f\"OmniASR available: {is_backend_available('omniasr')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load OmniASR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load OmniASR CTC 300M model (fastest for testing)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Loading OmniASR CTC 300M on {device}...\")\n",
    "backend = load_model(\"omniasr-300m\", device=device)\n",
    "\n",
    "print(f\"\\nModel loaded!\")\n",
    "print(f\"  Frame duration: {backend.frame_duration}s\")\n",
    "print(f\"  Sample rate: {backend.sample_rate}Hz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Vocabulary Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get vocabulary info\n",
    "vocab = backend.get_vocab_info()\n",
    "\n",
    "print(f\"Vocabulary size: {len(vocab.labels)}\")\n",
    "print(f\"Blank ID: {vocab.blank_id} ('{vocab.blank_token}')\")\n",
    "print(f\"UNK ID: {vocab.unk_id} ('{vocab.unk_token}')\")\n",
    "print(f\"\\nFirst 50 tokens:\")\n",
    "print(vocab.labels[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore vocabulary - find specific characters\n",
    "def find_tokens(pattern, vocab_labels, max_results=20):\n",
    "    \"\"\"Find tokens matching a pattern.\"\"\"\n",
    "    results = []\n",
    "    for i, token in enumerate(vocab_labels):\n",
    "        if pattern.lower() in token.lower():\n",
    "            results.append((i, token))\n",
    "        if len(results) >= max_results:\n",
    "            break\n",
    "    return results\n",
    "\n",
    "# Find some common characters\n",
    "print(\"Space token:\", [(i, t) for i, t in enumerate(vocab.labels) if t == ' '])\n",
    "print(\"\\nLatin letters (a-z):\")\n",
    "latin = [(i, t) for i, t in enumerate(vocab.labels) if len(t) == 1 and t.isalpha() and ord(t) < 128]\n",
    "print(latin[:26])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test Emission Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample audio\n",
    "from audio_frontend import load_audio, resample\n",
    "import urllib.request\n",
    "\n",
    "SAMPLE_AUDIO = \"Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav\"\n",
    "SAMPLE_TEXT = \"I had that curiosity beside me at this moment\"\n",
    "\n",
    "# Determine path\n",
    "if IN_COLAB:\n",
    "    sample_path = f\"/content/torchaudio_aligner/examples/{SAMPLE_AUDIO}\"\n",
    "else:\n",
    "    sample_path = str(Path(src_path).parent / \"examples\" / SAMPLE_AUDIO)\n",
    "\n",
    "if not os.path.exists(sample_path):\n",
    "    url = f\"https://raw.githubusercontent.com/huangruizhe/torchaudio_aligner/dev/examples/{SAMPLE_AUDIO}\"\n",
    "    print(f\"Downloading sample audio...\")\n",
    "    urllib.request.urlretrieve(url, SAMPLE_AUDIO)\n",
    "    sample_path = SAMPLE_AUDIO\n",
    "\n",
    "waveform, sample_rate = load_audio(sample_path)\n",
    "print(f\"Loaded: {sample_path}\")\n",
    "print(f\"  Shape: {waveform.shape}\")\n",
    "print(f\"  Sample rate: {sample_rate}Hz\")\n",
    "print(f\"  Duration: {waveform.shape[1] / sample_rate:.2f}s\")\n",
    "print(f\"  Transcript: \\\"{SAMPLE_TEXT}\\\"\")\n",
    "\n",
    "# Resample if needed\n",
    "if sample_rate != 16000:\n",
    "    waveform = resample(waveform, sample_rate, 16000)\n",
    "    sample_rate = 16000\n",
    "    print(f\"  Resampled to: {sample_rate}Hz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract emissions\n",
    "result = get_emissions(backend, waveform, sample_rate=16000)\n",
    "\n",
    "print(f\"Emissions shape: {result.emissions.shape}\")\n",
    "print(f\"Num frames: {result.num_frames}\")\n",
    "print(f\"Vocab size: {result.vocab_size}\")\n",
    "print(f\"Duration: {result.duration:.2f}s\")\n",
    "\n",
    "# Verify log probabilities sum to ~1\n",
    "probs = torch.exp(result.emissions[0])\n",
    "print(f\"\\nProb sum at frame 0: {probs.sum().item():.4f} (should be ~1.0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Greedy Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greedy decode\n",
    "decoded = backend.greedy_decode(result.emissions)\n",
    "\n",
    "print(f\"Ground truth: \\\"{SAMPLE_TEXT}\\\"\")\n",
    "print(f\"Decoded:      \\\"{decoded}\\\"\")\n",
    "\n",
    "# Word overlap\n",
    "gt_words = set(SAMPLE_TEXT.lower().split())\n",
    "decoded_words = set(decoded.lower().split())\n",
    "overlap = len(gt_words & decoded_words)\n",
    "print(f\"\\nWord overlap: {overlap}/{len(gt_words)} ({100*overlap/len(gt_words):.0f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze frame predictions\n",
    "print(\"Top predictions per frame (first 30 frames):\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "blank_count = 0\n",
    "for i in range(min(30, result.num_frames)):\n",
    "    top_idx = result.emissions[i].argmax().item()\n",
    "    top_prob = torch.exp(result.emissions[i, top_idx]).item()\n",
    "    label = vocab.id_to_label.get(top_idx, \"?\")\n",
    "    \n",
    "    if top_idx == vocab.blank_id:\n",
    "        blank_count += 1\n",
    "        display_label = \"<blank>\"\n",
    "    else:\n",
    "        display_label = repr(label)\n",
    "    \n",
    "    print(f\"Frame {i:3d}: {display_label:10s} (idx={top_idx:4d}, prob={top_prob:.3f})\")\n",
    "\n",
    "print(f\"\\nBlank frames in first 30: {blank_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Batched Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test batched inference with variable-length inputs\n",
    "waveforms = [\n",
    "    waveform.squeeze(0)[:16000],   # 1 second\n",
    "    waveform.squeeze(0)[:32000],   # 2 seconds\n",
    "    waveform.squeeze(0)[:48000],   # 3 seconds\n",
    "]\n",
    "\n",
    "print(f\"Testing batched inference with {len(waveforms)} samples:\")\n",
    "for i, w in enumerate(waveforms):\n",
    "    print(f\"  [{i}] {len(w)/16000:.1f}s ({len(w)} samples)\")\n",
    "\n",
    "# Extract emissions in batch\n",
    "batch_results = get_emissions_batched(backend, waveforms, sample_rate=16000)\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "for i, res in enumerate(batch_results):\n",
    "    decoded = backend.greedy_decode(res.emissions)\n",
    "    print(f\"  [{i}] frames={res.num_frames:3d}, decoded=\\\"{decoded}\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Direct Model Access (Advanced)\n",
    "\n",
    "The OmniASR backend uses direct model forward calls with fairseq2's BatchLayout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access underlying model components\n",
    "print(\"OmniASR Model Architecture:\")\n",
    "print(f\"  Model type: {type(backend._model).__name__}\")\n",
    "print(f\"  Model dtype: {backend._model_dtype}\")\n",
    "print(f\"  Device: {backend._device_obj}\")\n",
    "\n",
    "# Model submodules\n",
    "print(f\"\\nSubmodules:\")\n",
    "for name, module in backend._model.named_children():\n",
    "    print(f\"  {name}: {type(module).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct forward call example (what the backend does internally)\n",
    "from fairseq2.nn import BatchLayout\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Prepare input\n",
    "test_wav = waveform.squeeze(0).to(backend._device_obj, dtype=backend._model_dtype)\n",
    "test_wav = test_wav.unsqueeze(0)  # Add batch dim\n",
    "\n",
    "# Create lengths\n",
    "lengths = torch.tensor([test_wav.shape[1]], dtype=torch.long, device=backend._device_obj)\n",
    "\n",
    "# Create BatchLayout\n",
    "batch_layout = BatchLayout(\n",
    "    test_wav.shape,\n",
    "    seq_lens=lengths,\n",
    "    device=backend._device_obj,\n",
    ")\n",
    "\n",
    "# Forward pass\n",
    "backend._model.eval()\n",
    "with torch.inference_mode():\n",
    "    logits, output_layout = backend._model(test_wav, batch_layout)\n",
    "\n",
    "# Convert to log probabilities\n",
    "emissions = F.log_softmax(logits.float(), dim=-1)\n",
    "\n",
    "print(f\"Direct forward call:\")\n",
    "print(f\"  Input shape: {test_wav.shape}\")\n",
    "print(f\"  Logits shape: {logits.shape}\")\n",
    "print(f\"  Emissions shape: {emissions.shape}\")\n",
    "print(f\"  Output seq lens: {output_layout.seq_lens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Compare Model Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with different model sizes (if you have enough GPU memory)\n",
    "# Uncomment to test larger models\n",
    "\n",
    "model_sizes = [\n",
    "    (\"omniasr-300m\", \"omniASR_CTC_300M (325M params)\"),\n",
    "    # (\"omniasr-1b\", \"omniASR_CTC_1B (975M params)\"),\n",
    "    # (\"omniasr-3b\", \"omniASR_CTC_3B (3.08B params)\"),\n",
    "    # (\"omniasr-7b\", \"omniASR_CTC_7B (6.5B params)\"),\n",
    "]\n",
    "\n",
    "print(\"Model comparison:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for preset, description in model_sizes:\n",
    "    print(f\"\\nLoading {description}...\")\n",
    "    try:\n",
    "        model = load_model(preset, device=device)\n",
    "        result = get_emissions(model, waveform, sample_rate=16000)\n",
    "        decoded = model.greedy_decode(result.emissions)\n",
    "        \n",
    "        print(f\"  Vocab size: {len(model.get_vocab_info().labels)}\")\n",
    "        print(f\"  Decoded: \\\"{decoded}\\\"\")\n",
    "        \n",
    "        # Clean up to free GPU memory\n",
    "        del model\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Integration with Full Alignment Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full alignment example using OmniASR backend\n",
    "from alignment import Aligner\n",
    "\n",
    "# Create aligner with OmniASR backend\n",
    "aligner = Aligner(\n",
    "    backend=\"omniasr-300m\",\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "print(f\"Aligner created with OmniASR backend\")\n",
    "print(f\"  Backend: {aligner.backend}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align the sample audio\n",
    "result = aligner.align(\n",
    "    audio=sample_path,\n",
    "    text=SAMPLE_TEXT,\n",
    ")\n",
    "\n",
    "print(\"Alignment result:\")\n",
    "print(f\"  Words: {len(result.words)}\")\n",
    "print()\n",
    "for word in result.words:\n",
    "    print(f\"  {word.start:.3f} - {word.end:.3f}: {word.label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display alignment with audio player\n",
    "result.display_html()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The OmniASR backend provides:\n",
    "\n",
    "1. **Character-level vocabulary** with 9812 tokens supporting 1600+ languages\n",
    "2. **Direct model forward calls** using fairseq2's BatchLayout (no temp files)\n",
    "3. **Batched inference** with variable-length inputs\n",
    "4. **Multiple model sizes** from 300M to 7B parameters\n",
    "5. **Full integration** with the alignment pipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
