{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Notebook: Labeling Utils\n",
    "\n",
    "This notebook tests the `labeling_utils` module for extracting frame-wise posteriors from CTC models.\n",
    "\n",
    "**Features tested:**\n",
    "1. Model loading (HuggingFace MMS, Wav2Vec2)\n",
    "2. Emission extraction (single audio)\n",
    "3. Batched emission extraction\n",
    "4. Vocabulary information\n",
    "5. TorchAudio Pipeline Backend (MMS_FA)\n",
    "6. Integration with audio_frontend\n",
    "7. **NeMo Backend** (FastConformer hybrid RNN-T/CTC)\n",
    "8. **OmniASR Backend** (1600+ languages)\n",
    "\n",
    "**Architecture:**\n",
    "The module uses a plugin-style backend system:\n",
    "- `labeling_utils.base`: Core abstractions (CTCModelBackend, VocabInfo, BackendConfig)\n",
    "- `labeling_utils.registry`: Backend registration and discovery\n",
    "- `labeling_utils.backends/`: Individual backend implementations\n",
    "  - `huggingface.py`: HuggingFace Transformers (MMS, Wav2Vec2)\n",
    "  - `torchaudio_backend.py`: TorchAudio pipelines (MMS_FA)\n",
    "  - `nemo_backend.py`: NVIDIA NeMo (Conformer-CTC, FastConformer)\n",
    "  - `omniasr_backend.py`: Facebook OmniASR (1600+ languages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "# !pip install -q transformers torch torchaudio torchcodec soundfile\n",
    "!pip install transformers torch torchaudio torchcodec soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Setup: Clone Repository and Configure Imports\n",
    "# =============================================================================\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# ===== CONFIGURATION =====\n",
    "GITHUB_REPO = \"https://github.com/huangruizhe/torchaudio_aligner.git\"\n",
    "BRANCH = \"dev\"  # Use 'dev' for testing, 'main' for stable\n",
    "# =========================\n",
    "\n",
    "# Test result tracking\n",
    "test_results = {}\n",
    "\n",
    "def setup_imports():\n",
    "    \"\"\"Setup Python path for imports based on environment.\"\"\"\n",
    "    \n",
    "    IN_COLAB = 'google.colab' in sys.modules\n",
    "    \n",
    "    if IN_COLAB:\n",
    "        repo_path = '/content/torchaudio_aligner'\n",
    "        src_path = f'{repo_path}/src'\n",
    "        \n",
    "        if not os.path.exists(repo_path):\n",
    "            print(f\"Cloning repository (branch: {BRANCH})...\")\n",
    "            os.system(f'git clone -b {BRANCH} {GITHUB_REPO} {repo_path}')\n",
    "            print(\"Repository cloned\")\n",
    "        else:\n",
    "            print(f\"Updating repository (branch: {BRANCH})...\")\n",
    "            os.system(f'cd {repo_path} && git fetch origin && git checkout {BRANCH} && git pull origin {BRANCH}')\n",
    "            print(\"Repository updated\")\n",
    "    else:\n",
    "        possible_paths = [\n",
    "            Path(\".\").absolute().parent / \"src\",\n",
    "            Path(\".\").absolute() / \"src\",\n",
    "        ]\n",
    "        \n",
    "        src_path = None\n",
    "        for p in possible_paths:\n",
    "            if p.exists() and (p / \"labeling_utils\").exists():\n",
    "                src_path = str(p.absolute())\n",
    "                break\n",
    "        \n",
    "        if src_path is None:\n",
    "            raise FileNotFoundError(\"src directory not found\")\n",
    "        \n",
    "        print(f\"Running locally from: {src_path}\")\n",
    "    \n",
    "    if src_path not in sys.path:\n",
    "        sys.path.insert(0, src_path)\n",
    "    \n",
    "    return src_path\n",
    "\n",
    "src_path = setup_imports()\n",
    "\n",
    "# Import labeling_utils - new modular structure\n",
    "from labeling_utils import (\n",
    "    # High-level API\n",
    "    load_model,\n",
    "    get_emissions,\n",
    "    get_emissions_batched,\n",
    "    EmissionResult,\n",
    "    # Model configuration\n",
    "    ModelConfig,\n",
    "    list_presets,\n",
    "    get_model_info,\n",
    "    # Backend system\n",
    "    list_backends,\n",
    "    get_backend,\n",
    "    is_backend_available,\n",
    "    # Core classes\n",
    "    CTCModelBackend,\n",
    "    VocabInfo,\n",
    "    BackendConfig,\n",
    ")\n",
    "\n",
    "import torch\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"‚úÖ Labeling Utils imported successfully!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Available backends: {list_backends()}\")\n",
    "print(f\"Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")\n",
    "print()\n",
    "print(\"Available presets:\")\n",
    "for preset in list_presets():\n",
    "    info = get_model_info(preset)\n",
    "    print(f\"  ‚Ä¢ {preset}: {info['model_name']} ({info['backend']}, {info['languages']} languages)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: Load MMS Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Test 1: Load MMS Model (HuggingFace Backend)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    # Load MMS model for English\n",
    "    backend = load_model(\n",
    "        \"facebook/mms-1b-all\",\n",
    "        language=\"eng\",\n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    )\n",
    "    \n",
    "    print(f\"Model loaded: {backend}\")\n",
    "    print(f\"  ‚Ä¢ Is loaded: {backend.is_loaded}\")\n",
    "    print(f\"  ‚Ä¢ Frame duration: {backend.frame_duration}s\")\n",
    "    print(f\"  ‚Ä¢ Sample rate: {backend.sample_rate}Hz\")\n",
    "    \n",
    "    # Get vocab info\n",
    "    vocab = backend.get_vocab_info()\n",
    "    print(f\"\\nVocabulary:\")\n",
    "    print(f\"  ‚Ä¢ Size: {len(vocab.labels)}\")\n",
    "    print(f\"  ‚Ä¢ Blank ID: {vocab.blank_id} ('{vocab.blank_token}')\")\n",
    "    print(f\"  ‚Ä¢ UNK ID: {vocab.unk_id} ('{vocab.unk_token}')\")\n",
    "    print(f\"  ‚Ä¢ Sample labels: {vocab.labels[:10]}...\")\n",
    "    \n",
    "    test_results[\"Test 1\"] = \"‚úÖ PASSED\"\n",
    "    print(f\"\\n‚úÖ Test 1 PASSED - MMS model loaded successfully\")\n",
    "except Exception as e:\n",
    "    test_results[\"Test 1\"] = \"‚ùå FAILED\"\n",
    "    print(f\"\\n‚ùå Test 1 FAILED: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: Extract Emissions from VOiCES Sample Audio\n",
    "\n",
    "Using the Lab41 VOiCES dataset sample: *\"I had that curiosity beside me at this moment\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Test 2: Extract Emissions from VOiCES Sample Audio\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    from audio_frontend import load_audio, resample\n",
    "    import urllib.request\n",
    "    import os\n",
    "    \n",
    "    # VOiCES sample audio from repository\n",
    "    # Transcript: \"I had that curiosity beside me at this moment\"\n",
    "    SAMPLE_AUDIO = \"Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav\"\n",
    "    SAMPLE_TEXT = \"I had that curiosity beside me at this moment\"\n",
    "    \n",
    "    # Determine path based on environment\n",
    "    IN_COLAB = 'google.colab' in sys.modules\n",
    "    if IN_COLAB:\n",
    "        sample_path = f\"/content/torchaudio_aligner/examples/{SAMPLE_AUDIO}\"\n",
    "    else:\n",
    "        sample_path = str(Path(src_path).parent / \"examples\" / SAMPLE_AUDIO)\n",
    "    \n",
    "    if not os.path.exists(sample_path):\n",
    "        # Download from GitHub if not found locally\n",
    "        url = f\"https://raw.githubusercontent.com/huangruizhe/torchaudio_aligner/dev/examples/{SAMPLE_AUDIO}\"\n",
    "        print(f\"üì• Downloading sample audio...\")\n",
    "        urllib.request.urlretrieve(url, SAMPLE_AUDIO)\n",
    "        sample_path = SAMPLE_AUDIO\n",
    "        print(f\"   Downloaded: {sample_path}\")\n",
    "    \n",
    "    # Load audio using our own API\n",
    "    waveform, sample_rate = load_audio(sample_path)\n",
    "    print(f\"üéµ Loaded: {sample_path}\")\n",
    "    print(f\"   Transcript: \\\"{SAMPLE_TEXT}\\\"\")\n",
    "    print(f\"   ‚Ä¢ Waveform shape: {waveform.shape}\")\n",
    "    print(f\"   ‚Ä¢ Sample rate: {sample_rate}Hz\")\n",
    "    print(f\"   ‚Ä¢ Duration: {waveform.shape[1] / sample_rate:.2f}s\")\n",
    "    \n",
    "    # Resample if needed using our own API\n",
    "    if sample_rate != 16000:\n",
    "        waveform = resample(waveform, sample_rate, 16000)\n",
    "        sample_rate = 16000\n",
    "        print(f\"   ‚Ä¢ Resampled to: {sample_rate}Hz\")\n",
    "    \n",
    "    # Extract emissions\n",
    "    result = get_emissions(backend, waveform, sample_rate=sample_rate)\n",
    "    \n",
    "    print(f\"\\nüìä Emission result:\")\n",
    "    print(f\"   ‚Ä¢ Emissions shape: {result.emissions.shape}\")\n",
    "    print(f\"   ‚Ä¢ Num frames: {result.num_frames}\")\n",
    "    print(f\"   ‚Ä¢ Vocab size: {result.vocab_size}\")\n",
    "    print(f\"   ‚Ä¢ Duration: {result.duration:.2f}s\")\n",
    "    \n",
    "    # Verify log probabilities (should sum to ~1 after exp)\n",
    "    probs = torch.exp(result.emissions[0])\n",
    "    prob_sum = probs.sum().item()\n",
    "    print(f\"   ‚Ä¢ Prob sum at frame 0: {prob_sum:.4f} (should be ~1.0)\")\n",
    "    \n",
    "    # Show top predictions for a few frames\n",
    "    print(f\"\\nüî§ Top predictions (frames 10-15):\")\n",
    "    vocab = result.vocab_info\n",
    "    for i in range(10, min(15, result.num_frames)):\n",
    "        top_idx = result.emissions[i].argmax().item()\n",
    "        top_prob = torch.exp(result.emissions[i, top_idx]).item()\n",
    "        label = vocab.id_to_label.get(top_idx, \"?\")\n",
    "        print(f\"   Frame {i}: '{label}' (prob={top_prob:.3f})\")\n",
    "    \n",
    "    test_results[\"Test 2\"] = \"‚úÖ PASSED\"\n",
    "    print(f\"\\n‚úÖ Test 2 PASSED - Emissions extracted from VOiCES sample\")\n",
    "except Exception as e:\n",
    "    test_results[\"Test 2\"] = \"‚ùå FAILED\"\n",
    "    print(f\"\\n‚ùå Test 2 FAILED: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3: Greedy Decoding from Emissions\n",
    "\n",
    "Decode the emissions to verify the model recognizes the speech content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Test 3: Greedy Decoding from Emissions\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    # Use the backend's built-in greedy_decode method\n",
    "    # This uses the tokenizer to properly decode token IDs to text\n",
    "    decoded = backend.greedy_decode(result.emissions)\n",
    "    \n",
    "    print(f\"üìù Ground truth: \\\"{SAMPLE_TEXT}\\\"\")\n",
    "    print(f\"üîä Decoded:      \\\"{decoded}\\\"\")\n",
    "    \n",
    "    # Check if decoding roughly matches\n",
    "    gt_normalized = SAMPLE_TEXT.lower().replace(\"'\", \"\")\n",
    "    decoded_normalized = decoded.lower().replace(\"'\", \"\")\n",
    "    \n",
    "    # Simple word overlap check\n",
    "    gt_words = set(gt_normalized.split())\n",
    "    decoded_words = set(decoded_normalized.split())\n",
    "    overlap = len(gt_words & decoded_words)\n",
    "    total = len(gt_words)\n",
    "    \n",
    "    print(f\"\\nüìà Word overlap: {overlap}/{total} ({100*overlap/total:.0f}%)\")\n",
    "    \n",
    "    if overlap >= total // 2:\n",
    "        test_results[\"Test 3\"] = \"‚úÖ PASSED\"\n",
    "        print(f\"\\n‚úÖ Test 3 PASSED - Greedy decoding produces reasonable output\")\n",
    "    else:\n",
    "        test_results[\"Test 3\"] = \"‚ö†Ô∏è WARNING\"\n",
    "        print(f\"\\n‚ö†Ô∏è Test 3 WARNING - Low word overlap (model may need tuning)\")\n",
    "        \n",
    "except Exception as e:\n",
    "    test_results[\"Test 3\"] = \"‚ùå FAILED\"\n",
    "    print(f\"\\n‚ùå Test 3 FAILED: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 4: Batched Emission Extraction\n",
    "\n",
    "Test batch processing using segments from the VOiCES sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Test 4: Batched Emission Extraction\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    # Create multiple waveforms from the VOiCES sample (different segments)\n",
    "    full_wav = waveform.squeeze(0)  # Remove channel dim\n",
    "    total_samples = full_wav.shape[0]\n",
    "    \n",
    "    # Create 3 segments of different lengths\n",
    "    waveforms = [\n",
    "        full_wav[:total_samples // 3],           # First third\n",
    "        full_wav[total_samples // 4:],            # Last 3/4\n",
    "        full_wav[total_samples // 3:2*total_samples // 3],  # Middle third\n",
    "    ]\n",
    "    \n",
    "    print(f\"üì¶ Input: {len(waveforms)} waveforms from VOiCES sample\")\n",
    "    for i, w in enumerate(waveforms):\n",
    "        print(f\"   [{i}] shape={w.shape}, duration={len(w)/16000:.2f}s\")\n",
    "    \n",
    "    # Extract emissions in batch\n",
    "    results = get_emissions_batched(\n",
    "        backend,\n",
    "        waveforms,\n",
    "        sample_rate=16000,\n",
    "        batch_size=2,\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüìä Output: {len(results)} EmissionResults\")\n",
    "    for i, res in enumerate(results):\n",
    "        decoded = backend.greedy_decode(res.emissions)\n",
    "        print(f\"   [{i}] frames={res.num_frames}, duration={res.duration:.2f}s\")\n",
    "        print(f\"       decoded: \\\"{decoded[:50]}{'...' if len(decoded) > 50 else ''}\\\"\")\n",
    "    \n",
    "    assert len(results) == len(waveforms), \"Output count mismatch\"\n",
    "    \n",
    "    test_results[\"Test 4\"] = \"‚úÖ PASSED\"\n",
    "    print(f\"\\n‚úÖ Test 4 PASSED - Batched extraction works\")\n",
    "except Exception as e:\n",
    "    test_results[\"Test 4\"] = \"‚ùå FAILED\"\n",
    "    print(f\"\\n‚ùå Test 4 FAILED: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 5: Different Languages (MMS Multilingual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Test 5: Load MMS for Different Languages\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test a few languages\n",
    "languages = [\n",
    "    (\"fra\", \"French\"),\n",
    "    (\"deu\", \"German\"),\n",
    "    (\"jpn\", \"Japanese\"),\n",
    "]\n",
    "\n",
    "test5_passed = 0\n",
    "test5_total = len(languages)\n",
    "\n",
    "for lang_code, lang_name in languages:\n",
    "    print(f\"\\nüåç Loading MMS for {lang_name} ({lang_code})...\")\n",
    "    try:\n",
    "        lang_backend = load_model(\n",
    "            \"facebook/mms-1b-all\",\n",
    "            language=lang_code,\n",
    "            device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        )\n",
    "        \n",
    "        vocab = lang_backend.get_vocab_info()\n",
    "        print(f\"   ‚Ä¢ Vocab size: {len(vocab.labels)}\")\n",
    "        \n",
    "        # Quick emission test with VOiCES sample\n",
    "        test_result = get_emissions(lang_backend, waveform, sample_rate=16000)\n",
    "        print(f\"   ‚Ä¢ Emissions shape: {test_result.emissions.shape}\")\n",
    "        print(f\"   ‚úÖ {lang_name} PASSED\")\n",
    "        test5_passed += 1\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå {lang_name} FAILED: {e}\")\n",
    "\n",
    "if test5_passed == test5_total:\n",
    "    test_results[\"Test 5\"] = \"‚úÖ PASSED\"\n",
    "    print(f\"\\n‚úÖ Test 5 PASSED - All {test5_total} languages loaded successfully\")\n",
    "elif test5_passed > 0:\n",
    "    test_results[\"Test 5\"] = f\"‚ö†Ô∏è PARTIAL ({test5_passed}/{test5_total})\"\n",
    "    print(f\"\\n‚ö†Ô∏è Test 5 PARTIAL - {test5_passed}/{test5_total} languages loaded\")\n",
    "else:\n",
    "    test_results[\"Test 5\"] = \"‚ùå FAILED\"\n",
    "    print(f\"\\n‚ùå Test 5 FAILED - No languages loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 6: TorchAudio Pipeline Backend (MMS_FA)\n",
    "\n",
    "Note: This test uses the TorchAudio pipeline API which has a different interface than HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Test 6: TorchAudio Pipeline Backend (MMS_FA)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    # Check if torchaudio backend is available\n",
    "    if not is_backend_available(\"torchaudio\"):\n",
    "        test_results[\"Test 6\"] = \"‚è≠Ô∏è SKIPPED\"\n",
    "        print(\"‚è≠Ô∏è TorchAudio backend not available (missing dependencies)\")\n",
    "        print(\"   Skipping test...\")\n",
    "    else:\n",
    "        # Load MMS_FA using the preset\n",
    "        ta_backend = load_model(\"mms-fa\")  # Uses TorchAudio backend automatically\n",
    "        \n",
    "        print(f\"üîß Model loaded: {ta_backend}\")\n",
    "        print(f\"   ‚Ä¢ Is loaded: {ta_backend.is_loaded}\")\n",
    "        print(f\"   ‚Ä¢ Frame duration: {ta_backend.frame_duration}s\")\n",
    "        print(f\"   ‚Ä¢ Sample rate: {ta_backend.sample_rate}Hz\")\n",
    "        \n",
    "        # Get vocab info\n",
    "        vocab = ta_backend.get_vocab_info()\n",
    "        print(f\"\\nüìö Vocabulary:\")\n",
    "        print(f\"   ‚Ä¢ Size: {len(vocab.labels)}\")\n",
    "        print(f\"   ‚Ä¢ Labels: {vocab.labels}\") \n",
    "        print(f\"   ‚Ä¢ Blank ID: {vocab.blank_id} ('{vocab.blank_token}')\")\n",
    "        print(f\"   ‚Ä¢ UNK ID: {vocab.unk_id} ('{vocab.unk_token}')\")\n",
    "        \n",
    "        # Test with VOiCES sample\n",
    "        print(f\"\\nüéµ Testing with VOiCES sample:\")\n",
    "        print(f\"   Transcript: \\\"{SAMPLE_TEXT}\\\"\")\n",
    "        \n",
    "        ta_result = get_emissions(ta_backend, waveform, sample_rate=16000)\n",
    "        \n",
    "        print(f\"\\nüìä Emission result:\")\n",
    "        print(f\"   ‚Ä¢ Emissions shape: {ta_result.emissions.shape}\")\n",
    "        print(f\"   ‚Ä¢ Num frames: {ta_result.num_frames}\")\n",
    "        print(f\"   ‚Ä¢ Vocab size: {ta_result.vocab_size}\")\n",
    "        \n",
    "        # MMS_FA uses romanized phonemes - use backend's greedy_decode\n",
    "        ta_decoded = ta_backend.greedy_decode(ta_result.emissions)\n",
    "        print(f\"\\nüî§ Decoded (romanized): \\\"{ta_decoded}\\\"\")\n",
    "        \n",
    "        test_results[\"Test 6\"] = \"‚úÖ PASSED\"\n",
    "        print(f\"\\n‚úÖ Test 6 PASSED - TorchAudio Pipeline backend works\")\n",
    "except Exception as e:\n",
    "    test_results[\"Test 6\"] = \"‚ùå FAILED\"\n",
    "    print(f\"\\n‚ùå Test 6 FAILED: {e}\")\n",
    "    print(\"   Note: This test requires torchaudio with MMS_FA pipeline.\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 7: Integration with Audio Frontend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Test 7: Integration with Audio Frontend\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    from audio_frontend import segment_waveform\n",
    "    \n",
    "    # Use VOiCES sample - segment into smaller chunks\n",
    "    print(f\"üéµ Original audio: {waveform.shape[1]/16000:.2f}s\")\n",
    "    \n",
    "    # Segment into overlapping chunks using segment_waveform (works with tensors)\n",
    "    seg_result = segment_waveform(\n",
    "        waveform.squeeze(0),  # 1D tensor\n",
    "        sample_rate=16000, \n",
    "        segment_size=1.5,  # 1.5 second segments\n",
    "        overlap=0.3\n",
    "    )\n",
    "    print(f\"‚úÇÔ∏è Segmented into {len(seg_result.segments)} segments\")\n",
    "    \n",
    "    # Extract emissions for each segment\n",
    "    all_emissions = []\n",
    "    for i, segment in enumerate(seg_result.segments):\n",
    "        seg_emission = get_emissions(backend, segment.waveform)\n",
    "        all_emissions.append(seg_emission)\n",
    "        decoded = backend.greedy_decode(seg_emission.emissions)\n",
    "        print(f\"   Segment {i}: frames={seg_emission.num_frames}, decoded=\\\"{decoded}\\\"\")\n",
    "    \n",
    "    print(f\"\\nüìä Total emissions extracted: {len(all_emissions)}\")\n",
    "    print(f\"   Total frames: {sum(e.num_frames for e in all_emissions)}\")\n",
    "    \n",
    "    test_results[\"Test 7\"] = \"‚úÖ PASSED\"\n",
    "    print(f\"\\n‚úÖ Test 7 PASSED - Audio frontend + labeling utils integration works\")\n",
    "except ImportError:\n",
    "    test_results[\"Test 7\"] = \"‚è≠Ô∏è SKIPPED\"\n",
    "    print(\"‚è≠Ô∏è audio_frontend not available - skipping integration test\")\n",
    "    print(\"   This is expected if running labeling_utils tests only\")\n",
    "except Exception as e:\n",
    "    test_results[\"Test 7\"] = \"‚ùå FAILED\"\n",
    "    print(f\"\\n‚ùå Test 7 FAILED: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 8: NeMo Backend (FastConformer Hybrid RNN-T/CTC)\n",
    "\n",
    "Test the NeMo backend using the same model as in the tutorial: `nvidia/stt_en_fastconformer_hybrid_large_pc`\n",
    "\n",
    "This is a hybrid RNN-T/CTC model - we use the CTC head for emission extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: NeMo backend (heavy install, ~5-10 min)\n",
    "# ! pip install nemo_toolkit[asr]\n",
    "# ! pip install nemo_toolkit[all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Test 8: NeMo Backend (FastConformer Hybrid RNN-T/CTC)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    # Check if nemo backend is available\n",
    "    if not is_backend_available(\"nemo\"):\n",
    "        test_results[\"Test 8\"] = \"‚è≠Ô∏è SKIPPED\"\n",
    "        print(\"‚è≠Ô∏è NeMo backend not available (nemo_toolkit not installed)\")\n",
    "        print(\"   Install with: pip install nemo_toolkit[asr]\")\n",
    "        print(\"   Skipping test...\")\n",
    "    else:\n",
    "        # Load FastConformer Hybrid model (same as in tutorial)\n",
    "        # This is the model used in nemo_forced_aligner_tutorial.py\n",
    "        print(\"üîß Loading NeMo FastConformer Hybrid model...\")\n",
    "        print(\"   Model: nvidia/stt_en_fastconformer_hybrid_large_pc\")\n",
    "        \n",
    "        nemo_backend = load_model(\n",
    "            \"nemo-fastconformer\",  # Uses nvidia/stt_en_fastconformer_hybrid_large_pc\n",
    "            device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nüì¶ Model loaded: {nemo_backend}\")\n",
    "        print(f\"   ‚Ä¢ Is loaded: {nemo_backend.is_loaded}\")\n",
    "        print(f\"   ‚Ä¢ Frame duration: {nemo_backend.frame_duration}s\")\n",
    "        print(f\"   ‚Ä¢ Sample rate: {nemo_backend.sample_rate}Hz\")\n",
    "        \n",
    "        # Get vocab info\n",
    "        nemo_vocab = nemo_backend.get_vocab_info()\n",
    "        print(f\"\\nüìö Vocabulary (BPE):\")\n",
    "        print(f\"   ‚Ä¢ Size: {len(nemo_vocab.labels)}\")\n",
    "        print(f\"   ‚Ä¢ Blank ID: {nemo_vocab.blank_id} ('{nemo_vocab.blank_token}')\")\n",
    "        print(f\"   ‚Ä¢ Sample tokens: {nemo_vocab.labels[1:11]}...\")  # Skip blank\n",
    "        \n",
    "        # Test with VOiCES sample\n",
    "        print(f\"\\nüéµ Testing with VOiCES sample:\")\n",
    "        print(f\"   Transcript: \\\"{SAMPLE_TEXT}\\\"\")\n",
    "        \n",
    "        nemo_result = get_emissions(nemo_backend, waveform, sample_rate=16000)\n",
    "        \n",
    "        print(f\"\\nüìä Emission result:\")\n",
    "        print(f\"   ‚Ä¢ Emissions shape: {nemo_result.emissions.shape}\")\n",
    "        print(f\"   ‚Ä¢ Num frames: {nemo_result.num_frames}\")\n",
    "        print(f\"   ‚Ä¢ Vocab size: {nemo_result.vocab_size}\")\n",
    "        \n",
    "        # Greedy decode using backend's tokenizer\n",
    "        nemo_decoded = nemo_backend.greedy_decode(nemo_result.emissions)\n",
    "        \n",
    "        print(f\"\\nüî§ Greedy decoding:\")\n",
    "        print(f\"   üìù Ground truth: \\\"{SAMPLE_TEXT}\\\"\")\n",
    "        print(f\"   üîä Decoded:      \\\"{nemo_decoded}\\\"\")\n",
    "        \n",
    "        # Check word overlap\n",
    "        gt_normalized = SAMPLE_TEXT.lower().replace(\"'\", \"\")\n",
    "        decoded_normalized = nemo_decoded.lower().replace(\"'\", \"\")\n",
    "        \n",
    "        gt_words = set(gt_normalized.split())\n",
    "        decoded_words = set(decoded_normalized.split())\n",
    "        overlap = len(gt_words & decoded_words)\n",
    "        total = len(gt_words)\n",
    "        \n",
    "        print(f\"\\nüìà Word overlap: {overlap}/{total} ({100*overlap/total:.0f}%)\")\n",
    "        \n",
    "        if overlap >= total // 2:\n",
    "            test_results[\"Test 8\"] = \"‚úÖ PASSED\"\n",
    "            print(f\"\\n‚úÖ Test 8 PASSED - NeMo backend works with reasonable decoding\")\n",
    "        else:\n",
    "            test_results[\"Test 8\"] = \"‚ö†Ô∏è WARNING\"\n",
    "            print(f\"\\n‚ö†Ô∏è Test 8 WARNING - Low word overlap (but backend works)\")\n",
    "            \n",
    "except Exception as e:\n",
    "    test_results[\"Test 8\"] = \"‚ùå FAILED\"\n",
    "    print(f\"\\n‚ùå Test 8 FAILED: {e}\")\n",
    "    print(\"   Note: This test requires nemo_toolkit[asr]\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 9: OmniASR Backend (1600+ Languages)\n",
    "\n",
    "Test the OmniASR backend from Facebook/Meta's Omnilingual ASR project.\n",
    "\n",
    "Note: This requires the `omnilingual-asr` package: `pip install omnilingual-asr`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# OmniASR Installation (Colab only)\n# Credit: https://github.com/NeuralFalconYT/omnilingual-asr-colab\n#\n# WARNING: omnilingual-asr has specific PyTorch/fairseq2 requirements\n# that may conflict with other packages. Run in a fresh environment.\n\nIN_COLAB = 'google.colab' in sys.modules\n\nif IN_COLAB:\n    # Uncomment to install OmniASR:\n    # !pip uninstall -y torch torchaudio\n    # !pip install torch==2.8.0+cu128 torchaudio==2.8.0+cu128 torchvision==0.23.0+cu128 --index-url https://download.pytorch.org/whl/cu128\n    # !pip install fairseq2==0.6\n    # !pip install omnilingual-asr==0.1.0\n    # !pip install silero-vad>=4.0.0 onnxruntime>=1.12.0 uroman==1.3.1.1\n    # !pip uninstall fairseq2 -y\n    # !pip install fairseq2 --extra-index-url https://fair.pkg.atmeta.com/fairseq2/whl/pt2.8.0/cu126\n    # !pip install omnilingual-asr\n    pass\nelse:\n    print(\"OmniASR installation instructions are for Colab only.\")\n    print(\"For local installation, see: https://github.com/facebookresearch/omnilingual-asr\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\" * 60)\nprint(\"Test 9: OmniASR Backend (1600+ Languages)\")\nprint(\"=\" * 60)\n\ntry:\n    # Check if omniasr backend is available\n    if not is_backend_available(\"omniasr\"):\n        test_results[\"Test 9\"] = \"‚è≠Ô∏è SKIPPED\"\n        print(\"‚è≠Ô∏è OmniASR backend not available (omnilingual-asr not installed)\")\n        print(\"   Install with the cell above (Colab) or see:\")\n        print(\"   https://github.com/facebookresearch/omnilingual-asr\")\n        print(\"   Skipping test...\")\n    else:\n        # Load OmniASR CTC model (300M is fastest for testing)\n        print(\"üîß Loading OmniASR CTC model...\")\n        print(\"   Model: omniASR_CTC_300M (325M parameters)\")\n        \n        omni_backend = load_model(\n            \"omniasr-300m\",  # Fastest model for testing\n            device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n        )\n        \n        print(f\"\\nüì¶ Model loaded: {omni_backend}\")\n        print(f\"   ‚Ä¢ Is loaded: {omni_backend.is_loaded}\")\n        print(f\"   ‚Ä¢ Frame duration: {omni_backend.frame_duration}s\")\n        print(f\"   ‚Ä¢ Sample rate: {omni_backend.sample_rate}Hz\")\n        \n        # Get vocab info - should show full 9812 tokens\n        omni_vocab = omni_backend.get_vocab_info()\n        print(f\"\\nüìö Vocabulary (Character-level SentencePiece):\")\n        print(f\"   ‚Ä¢ Size: {len(omni_vocab.labels)}\")\n        print(f\"   ‚Ä¢ Blank ID: {omni_vocab.blank_id} ('{omni_vocab.blank_token}')\")\n        print(f\"   ‚Ä¢ First 20 tokens: {omni_vocab.labels[:20]}\")\n        \n        # Test with VOiCES sample\n        print(f\"\\nüéµ Testing with VOiCES sample:\")\n        print(f\"   Transcript: \\\"{SAMPLE_TEXT}\\\"\")\n        \n        omni_result = get_emissions(omni_backend, waveform, sample_rate=16000)\n        \n        print(f\"\\nüìä Emission result:\")\n        print(f\"   ‚Ä¢ Emissions shape: {omni_result.emissions.shape}\")\n        print(f\"   ‚Ä¢ Num frames: {omni_result.num_frames}\")\n        print(f\"   ‚Ä¢ Vocab size: {omni_result.vocab_size}\")\n        \n        # Greedy decode using backend's tokenizer\n        omni_decoded = omni_backend.greedy_decode(omni_result.emissions)\n        \n        print(f\"\\nüî§ Greedy decoding:\")\n        print(f\"   üìù Ground truth: \\\"{SAMPLE_TEXT}\\\"\")\n        print(f\"   üîä Decoded:      \\\"{omni_decoded}\\\"\")\n        \n        # Check word overlap\n        gt_normalized = SAMPLE_TEXT.lower().replace(\"'\", \"\")\n        decoded_normalized = omni_decoded.lower().replace(\"'\", \"\")\n        \n        gt_words = set(gt_normalized.split())\n        decoded_words = set(decoded_normalized.split())\n        overlap = len(gt_words & decoded_words)\n        total = len(gt_words)\n        \n        print(f\"\\nüìà Word overlap: {overlap}/{total} ({100*overlap/total:.0f}%)\")\n        \n        # Test batched inference\n        print(f\"\\nüîÑ Testing batched inference...\")\n        test_waveforms = [\n            waveform.squeeze(0)[:16000],  # 1 second\n            waveform.squeeze(0)[:32000],  # 2 seconds\n        ]\n        batch_results = get_emissions_batched(omni_backend, test_waveforms, sample_rate=16000)\n        print(f\"   ‚Ä¢ Batch size: {len(batch_results)}\")\n        for i, res in enumerate(batch_results):\n            decoded = omni_backend.greedy_decode(res.emissions)\n            print(f\"   ‚Ä¢ [{i}] frames={res.num_frames}, decoded=\\\"{decoded[:40]}...\\\"\")\n        \n        if overlap >= total // 2:\n            test_results[\"Test 9\"] = \"‚úÖ PASSED\"\n            print(f\"\\n‚úÖ Test 9 PASSED - OmniASR backend works with reasonable decoding\")\n        else:\n            test_results[\"Test 9\"] = \"‚ö†Ô∏è WARNING\"\n            print(f\"\\n‚ö†Ô∏è Test 9 WARNING - Low word overlap (but backend works)\")\n            \nexcept Exception as e:\n    test_results[\"Test 9\"] = \"‚ùå FAILED\"\n    print(f\"\\n‚ùå Test 9 FAILED: {e}\")\n    print(\"   Note: This test requires omnilingual-asr package\")\n    import traceback\n    traceback.print_exc()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Test Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"üìã TEST RESULTS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Display test results\n",
    "print(\"\\n\" + \"-\" * 40)\n",
    "for test_name, result in test_results.items():\n",
    "    print(f\"  {result}  {test_name}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Count results\n",
    "passed = sum(1 for r in test_results.values() if \"‚úÖ\" in r)\n",
    "failed = sum(1 for r in test_results.values() if \"‚ùå\" in r)\n",
    "skipped = sum(1 for r in test_results.values() if \"‚è≠Ô∏è\" in r)\n",
    "warning = sum(1 for r in test_results.values() if \"‚ö†Ô∏è\" in r)\n",
    "total = len(test_results)\n",
    "\n",
    "print(f\"\\n  Total: {total} tests\")\n",
    "print(f\"  ‚úÖ Passed:  {passed}\")\n",
    "if warning > 0:\n",
    "    print(f\"  ‚ö†Ô∏è Warning: {warning}\")\n",
    "if skipped > 0:\n",
    "    print(f\"  ‚è≠Ô∏è Skipped: {skipped}\")\n",
    "if failed > 0:\n",
    "    print(f\"  ‚ùå Failed:  {failed}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "if failed == 0:\n",
    "    print(\"üéâ All tests passed!\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è {failed} test(s) failed - please check above for details\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}